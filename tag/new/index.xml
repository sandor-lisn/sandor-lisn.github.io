<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>new | Christian Sandor</title><link>https://drsandor.net/tag/new/</link><atom:link href="https://drsandor.net/tag/new/index.xml" rel="self" type="application/rss+xml"/><description>new</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Fri, 07 Nov 2025 14:00:00 +0000</lastBuildDate><image><url>https://drsandor.net/media/icon_hu287fcf417612116bcf0d00ac1ba165d7_82622_512x512_fill_lanczos_center_3.png</url><title>new</title><link>https://drsandor.net/tag/new/</link></image><item><title>Keynote at ICXR</title><link>https://drsandor.net/project/icxr/</link><pubDate>Fri, 07 Nov 2025 14:00:00 +0000</pubDate><guid>https://drsandor.net/project/icxr/</guid><description>&lt;p>Every time I visit China, I am absolutely astonished by their pace of progress. It is simply not possible to grasp this from the outside, as lots of information is not very accessible from abroad. In this post, I would like to share some of the things I learned during my visit.&lt;/p>
&lt;p>I presented a keynote at the new and upcoming &lt;a href="https://icxr.net/2025/index.html" target="_blank" rel="noopener">International Conference on Extended Reality&lt;/a> (ICXR); even though it was only the second time that this conference took place, it was well attended. It was co-located with &lt;a href="https://chinavr2025.qdvri.com" target="_blank" rel="noopener">ChinaVR&lt;/a>, the biggest VR conference of China (running since 2001).&lt;/p>
&lt;p>It is important to understand how Chinese research is organized. The Chinese research community elects &lt;a href="https://en.wikipedia.org/wiki/Academician_of_the_Chinese_Academy_of_Sciences" target="_blank" rel="noopener">Academicians (ä¸­å›½ç§‘å­¦é™¢é™¢å£«)&lt;/a> for all important research areas. Then, the Academician has strong influence on the policy, research, and commercial developments in China. I don&amp;rsquo;t think neither the US nor Europe have any equivalent of this. 3 Academicians attended the event:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://cg.cs.tsinghua.edu.cn/shimin.htm" target="_blank" rel="noopener">Shi-min Hu&lt;/a>: Computer Graphics&lt;/li>
&lt;li>&lt;a href="https://baike.baidu.com/item/%e8%b5%b5%e6%b2%81%e5%b9%b3/7417905" target="_blank" rel="noopener">Qinping Zhao&lt;/a>: Virtual Reality&lt;/li>
&lt;li>&lt;a href="https://faculty.bjtu.edu.cn/eaie/6358.html" target="_blank" rel="noopener">Hongke Zhang&lt;/a>: Networking&lt;/li>
&lt;/ul>
&lt;p>There were also 2 more researchers of that caliber, who I guess will become Academicians sooner or later:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://baoquanchen.info" target="_blank" rel="noopener">Baoquan Chen&lt;/a>: Computer Vision&lt;/li>
&lt;li>&lt;a href="https://fve.bfa.edu.cn/info/1056/2054.htm" target="_blank" rel="noopener">Yongtian Wang&lt;/a>: Optics for AR/VR&lt;/li>
&lt;/ul>
&lt;p>Several companies showed exhibits, including &lt;strong>Goertek&lt;/strong>. To be honest, I have never heard of this company before! But, they are producing &lt;strong>80% of the world&amp;rsquo;s AR and VR glasses&lt;/strong>! Could you have guessed that based on their &lt;a href="https://en.wikipedia.org/wiki/Goertek" target="_blank" rel="noopener">Wikipedia page&lt;/a>? Unfortunately, photos were strictly prohibited at their large booth. I could clearly see that they are not simply a factory for US companies, but very innovative and contributing key technological developments, including:&lt;/p>
&lt;ul>
&lt;li>High pixel density display panels (Micro OLED)&lt;/li>
&lt;li>Miniaturized water cooling for AR/VR glasses (the water-cooled board they showcased was about 2x4 cm big; water pipes around 2mm diameter)&lt;/li>
&lt;li>Advanced pancake optics&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Hongke Zhang&lt;/strong> presented a thought-provoking keynote along the lines of: What network infrastructure do we need to support the rapidly changing network requirements in an AI world? It included many science-fiction level concepts such as:&lt;/p>
&lt;ul>
&lt;li>A longterm roadmap including 6G and beyond with mobile speeds of Terabits per second&lt;/li>
&lt;li>Mesh-networking with drone swarms&lt;/li>
&lt;li>Streaming neural content (NERFs, Gaussian Splats, etc)&lt;/li>
&lt;/ul>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="" srcset="
/project/icxr/zhang1_hu307ffe06fa0f0a670173d11c2083b3d1_607338_8a43a8be846f13d72cc33c24cba4b58f.webp 400w,
/project/icxr/zhang1_hu307ffe06fa0f0a670173d11c2083b3d1_607338_36d5485571fe887cb67e75c802aaa646.webp 760w,
/project/icxr/zhang1_hu307ffe06fa0f0a670173d11c2083b3d1_607338_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://drsandor.net/project/icxr/zhang1_hu307ffe06fa0f0a670173d11c2083b3d1_607338_8a43a8be846f13d72cc33c24cba4b58f.webp"
width="760"
height="570"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="" srcset="
/project/icxr/zhang2_hu4a70eccacf39bf605e2091d4dd40c932_633050_89802289d51d9d197134dc144cfc2eea.webp 400w,
/project/icxr/zhang2_hu4a70eccacf39bf605e2091d4dd40c932_633050_a685ee0f8e782cd067056cc30638a894.webp 760w,
/project/icxr/zhang2_hu4a70eccacf39bf605e2091d4dd40c932_633050_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://drsandor.net/project/icxr/zhang2_hu4a70eccacf39bf605e2091d4dd40c932_633050_89802289d51d9d197134dc144cfc2eea.webp"
width="760"
height="570"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="" srcset="
/project/icxr/zhang3_hue07d29e6f16d0edda3870e3fa88a1557_629518_e2d2058e8bb3f6c80b97f3304a6a2304.webp 400w,
/project/icxr/zhang3_hue07d29e6f16d0edda3870e3fa88a1557_629518_741f1728e9355540cb4f0051b7cacf1b.webp 760w,
/project/icxr/zhang3_hue07d29e6f16d0edda3870e3fa88a1557_629518_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://drsandor.net/project/icxr/zhang3_hue07d29e6f16d0edda3870e3fa88a1557_629518_e2d2058e8bb3f6c80b97f3304a6a2304.webp"
width="760"
height="570"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;p>Next up was &lt;strong>Baoquan Chen&lt;/strong>. A major thread of his talk was along the lines of (please excuse my informal language):&lt;/p>
&lt;ul>
&lt;li>AI Video generators along the lines of SORA and VEO suck, because they lack understanding of the physical world. They are essentially a dead end.&lt;/li>
&lt;li>Gaussian Splatting is very popular now, but it is by itself also a dead end, because of the inherent inflexibility and non-interactiveness.&lt;/li>
&lt;/ul>
&lt;p>I already came to similar conclusions, albeit for different reasons :-) He then presented several recent works by his group how to overcome these limitations. In short: real instead of hallucinated physics are needed!&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="" srcset="
/project/icxr/chen1_hufe082b017d9719ead8de3b0b5e59b3d3_544830_9982a9761f77d7bd7df3367926c787fd.webp 400w,
/project/icxr/chen1_hufe082b017d9719ead8de3b0b5e59b3d3_544830_e808e18baa805a907cf8a7ec56af595f.webp 760w,
/project/icxr/chen1_hufe082b017d9719ead8de3b0b5e59b3d3_544830_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://drsandor.net/project/icxr/chen1_hufe082b017d9719ead8de3b0b5e59b3d3_544830_9982a9761f77d7bd7df3367926c787fd.webp"
width="760"
height="570"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="" srcset="
/project/icxr/chen2_huadedc5b8b6f11091bcbfafbc1c1ed320_505349_540916450eb775080b984f1bce2a6c3e.webp 400w,
/project/icxr/chen2_huadedc5b8b6f11091bcbfafbc1c1ed320_505349_5ceeb53afb3fb36c5710848b674afd77.webp 760w,
/project/icxr/chen2_huadedc5b8b6f11091bcbfafbc1c1ed320_505349_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://drsandor.net/project/icxr/chen2_huadedc5b8b6f11091bcbfafbc1c1ed320_505349_540916450eb775080b984f1bce2a6c3e.webp"
width="760"
height="570"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="" srcset="
/project/icxr/chen3_hu2784ae0ef6911b4defd144ef4bc2371f_796921_a1c070b5b5ff17cf4fc092428a798286.webp 400w,
/project/icxr/chen3_hu2784ae0ef6911b4defd144ef4bc2371f_796921_511db7516ebfbf70775aad112a26ee4f.webp 760w,
/project/icxr/chen3_hu2784ae0ef6911b4defd144ef4bc2371f_796921_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://drsandor.net/project/icxr/chen3_hu2784ae0ef6911b4defd144ef4bc2371f_796921_a1c070b5b5ff17cf4fc092428a798286.webp"
width="760"
height="570"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;p>I was extremely glad that &lt;strong>Shi-Min Hu&lt;/strong> chiseled out 20 minutes out of his busy schedule to talk to me. Our chat was very intense&amp;hellip; I now understand better China&amp;rsquo;s strategy how to deal with the import restrictions of NVIDIA GPUs. I mean, at the moment, Chinese AI models are already on the same level as their US counterparts (I personally think that they are actually already better, but this is debatable). They achieved this even though their current GPUs are much weaker than the ones Meta, OpenAI etc. are using. If they get comparable compute power as the US than their AI models will run circles around their US counterparts. It will happen much sooner than the public thinks&amp;hellip; I can&amp;rsquo;t wait to switch to Chinese GPUs!&lt;/p>
&lt;p>It was a lucky accident that I sat next to &lt;strong>Yongtian Wang&lt;/strong> at the closing dinner. He shared some incredible stories about the custom optics design software he wrote back in the day (in Fortran!): GOLD = &lt;strong>G&lt;/strong>eneral &lt;strong>O&lt;/strong>ptical &lt;strong>L&lt;/strong>ens &lt;strong>D&lt;/strong>esign system :-) As their group can&amp;rsquo;t use the world-wide standard software (Code V; another US import restriction), they have been recently reviving this decade-old software.&lt;/p>
&lt;p>Finally, I should also spend some words on my keynote. My recent discussions with Notch&amp;rsquo;s Matt Swoboda (the grand wizard of realtime computer graphics) inspired many of the open questions that I discussed in my talk.&lt;/p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="" srcset="
/project/icxr/questions_hu5636f1c492ebb8d576e1c13f2147d42f_394607_f55721850aaf7cf860e43549482454be.webp 400w,
/project/icxr/questions_hu5636f1c492ebb8d576e1c13f2147d42f_394607_76bc53af824a7556e83d173b721936b1.webp 760w,
/project/icxr/questions_hu5636f1c492ebb8d576e1c13f2147d42f_394607_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://drsandor.net/project/icxr/questions_hu5636f1c492ebb8d576e1c13f2147d42f_394607_f55721850aaf7cf860e43549482454be.webp"
width="760"
height="428"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;p>To conclude, a little video from the soundcheck of my presentation. The sound system was really good!&lt;/p>
&lt;video width="1920" controls loop>
&lt;source src="soundcheck.mp4" type="video/mp4" />
&lt;/video></description></item><item><title>Short Animation with Wan Video, Flux Kontext, and DeepSeek</title><link>https://drsandor.net/project/ai-minecraft/</link><pubDate>Mon, 23 Jun 2025 14:00:00 +0000</pubDate><guid>https://drsandor.net/project/ai-minecraft/</guid><description>&lt;p>Test&lt;/p></description></item><item><title>Keynote at ADOS Paris</title><link>https://drsandor.net/ai/ados/</link><pubDate>Thu, 10 Apr 2025 14:00:00 +0000</pubDate><guid>https://drsandor.net/ai/ados/</guid><description>&lt;p>On 28 March 2025, I attended the amazing &lt;a href="https://www.lightricks.com/ados" target="_blank" rel="noopener">ADOS&lt;/a> event in Paris at &lt;a href="https://www.artifex-lab.com" target="_blank" rel="noopener">Artifex Lab&lt;/a>, a new space in Paris for bringing together AI arists and technologists. ADOS was co-organized by &lt;a href="https://banodoco.ai" target="_blank" rel="noopener">Banadoco&lt;/a> and &lt;a href="https://www.lightricks.com" target="_blank" rel="noopener">Lightricks&lt;/a>, which was an ideal fit for Artifex Lab: Banadoco is the leading online community for AI art+technology and Lightricks&amp;rsquo;s LTX is the fastest generative AI video model (and it&amp;rsquo;s Open Source on top of that!).&lt;/p>
&lt;p>Coincidentally, our team has recently been investigating LTX deeply and also has been hanging out a lot on Banadoco&amp;rsquo;s inspiring Discord server. One thing led to another and I was invited to present a keynote at ADOS. We brought most of the team, as well as a bleeding edge demo. Unfortuntately, I missed out on the second day of the event, which featured a hackathon, where our PhD student Hovhannes received a prize (see his upcoming blogpost on our &lt;a href="https://ar-ai.org" target="_blank" rel="noopener">team webpage&lt;/a>).&lt;/p>
&lt;p>
&lt;figure id="figure-team-arai-at-ados">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Team ARAI at ADOS" srcset="
/media/ados/team_hu3a541a33e063e69d3b30aeefb30179f7_534763_b783a900c056276757e7f7017585a4b8.webp 400w,
/media/ados/team_hu3a541a33e063e69d3b30aeefb30179f7_534763_4448afd20a1677241c97a762bfe02851.webp 760w,
/media/ados/team_hu3a541a33e063e69d3b30aeefb30179f7_534763_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://drsandor.net/media/ados/team_hu3a541a33e063e69d3b30aeefb30179f7_534763_b783a900c056276757e7f7017585a4b8.webp"
width="760"
height="570"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Team ARAI at ADOS
&lt;/figcaption>&lt;/figure>
&lt;figure id="figure-zeev-farbman-co-founder--ceo-of-lightricks-trying-our-demo">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Zeev Farbman, Co-Founder &amp;amp; CEO of Lightricks trying our demo" srcset="
/media/ados/demo_hua7d9fbdda16f2d02e5b664a9fb28b240_541462_3cc95a501fbfe4de2ff237a29e99ded4.webp 400w,
/media/ados/demo_hua7d9fbdda16f2d02e5b664a9fb28b240_541462_007047a2ff3dd4aaa738306758667246.webp 760w,
/media/ados/demo_hua7d9fbdda16f2d02e5b664a9fb28b240_541462_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://drsandor.net/media/ados/demo_hua7d9fbdda16f2d02e5b664a9fb28b240_541462_3cc95a501fbfe4de2ff237a29e99ded4.webp"
width="760"
height="570"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Zeev Farbman, Co-Founder &amp;amp; CEO of Lightricks trying our demo
&lt;/figcaption>&lt;/figure>&lt;/p>
&lt;p>Before reporting about the keynote and demo, I would like to highlight some of my favourite things that I saw at the event. All talks are &lt;a href="https://www.youtube.com/watch?v=gBeZSbaxMvc" target="_blank" rel="noopener">recorded on youtube&lt;/a>; I provide direct links to the talks in the following.&lt;/p>
&lt;p>I enjoyed &lt;a href="https://www.youtube.com/live/gBeZSbaxMvc?si=orYRKkprSBqWWwBo&amp;amp;t=468" target="_blank" rel="noopener">Emma Catnip&amp;rsquo;s talk&lt;/a>, particularly her work Bloom. It is very soulful and beautifully designed. I will never forget her bitter-sweet explanation: &amp;ldquo;It&amp;rsquo;s about the summer of love I that never had&amp;rdquo;.&lt;/p>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/cDUExDUYRaE" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;p>Another very interesting artist&amp;rsquo;s talk was by &lt;a href="https://udart.dk" target="_blank" rel="noopener">Vibeke Bertelsen (Udart)&lt;/a>; &lt;a href="https://www.youtube.com/live/gBeZSbaxMvc?si=r3DAzpV5a7a7C8jd&amp;amp;t=8304" target="_blank" rel="noopener">timestamp&lt;/a>. Imagine current AI tools in the hands of H.R. Giger&amp;hellip; Below is one of her award-winning short movies. Definitely most unique!&lt;/p>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/8Vodtg8WNiY" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;p>On the tech side, my favourite talk was by &lt;a href="https://github.com/yvann-ba" target="_blank" rel="noopener">Yvann Barbot&lt;/a>. In &lt;a href="https://www.youtube.com/live/gBeZSbaxMvc?si=utqO7-YM0Oq1c5Fx&amp;amp;t=8960" target="_blank" rel="noopener">his talk&lt;/a>, he showed how easy it is to generate cool audio-reactive video clips using his ComfyUI nodes. One example below by &lt;a href="https://www.youtube.com/@visualfrisson" target="_blank" rel="noopener">Visual Frisson&lt;/a>. After the event, I checked Yvann&amp;rsquo;s youtube channel. I can highly recommend it!&lt;/p>
&lt;video width="720" controls>
&lt;source src="frisson.mp4" type="video/mp4" />
&lt;/video>
&lt;p>Finally, let me get to my keynote talk and our demo.&lt;/p>
&lt;p>Due to the extreme technical difficulties involved in our demo, we were working literally till the last minute on it. Chapeau to DÃ¡vid MaruscsÃ¡k and Francesco Dettori for their very hard work near demo time! Very unfortunately, the backend of our demo stopped working 5 minutes before my talk started. As a result, we could not show the demo during my keynote (as we had originally planned), but only 2 hours later. Luckily, most attendants of ADOS were still there and could give it a try!&lt;/p>
&lt;p>We demonstrate how to use a cloud backend for highspeed generation of spatial videos based on image and text inputs. You can see a similar version of the demo below; compared to the demo we showed at ADOS, it is only using 1 local RTX 4090 GPU for inference, as opposed to 4 H100 GPUs at ADOS, so inference speed is significantly lower.&lt;/p>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/Gk2dtoOXDac" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;p>You can see my keynote on &lt;a href="https://www.youtube.com/live/gBeZSbaxMvc?si=qr5cMNz5WXu8Jt7d&amp;amp;t=7061" target="_blank" rel="noopener">youtube&lt;/a>, where I speak about my long past with AR and my more recent past with AI, and how everything converged into this demo :-)&lt;/p>
&lt;p>To conclude, I would like to sincerely thank:&lt;/p>
&lt;ul>
&lt;li>The &lt;a href="https://www.lightricks.com/ados" target="_blank" rel="noopener">ADOS&lt;/a> organizational team: &lt;a href="https://banodoco.ai" target="_blank" rel="noopener">Banadoco&lt;/a>, &lt;a href="https://www.lightricks.com" target="_blank" rel="noopener">Lightricks&lt;/a>, and &lt;a href="https://www.artifex-lab.com" target="_blank" rel="noopener">Artifex Lab&lt;/a>.&lt;/li>
&lt;li>Special thanks to Lightricks for giving us early access to their distilled LTX model, which enabled impressive speedups.&lt;/li>
&lt;li>All &lt;a href="https://ar-ai.org/people/" target="_blank" rel="noopener">ARAI team members&lt;/a> for bearing with my demands :-) Directly involved in the demo were Cyrille Leroux, Thomas Betton, DÃ¡vid MaruscsÃ¡k, Francesco Dettori, and Hovhannes Margaryan.&lt;/li>
&lt;/ul></description></item><item><title>Keynote at ADOS Paris</title><link>https://drsandor.net/project/ai3/</link><pubDate>Thu, 10 Apr 2025 14:00:00 +0000</pubDate><guid>https://drsandor.net/project/ai3/</guid><description>&lt;p>Test&lt;/p></description></item><item><title>Demo at ACM SIGGRAPH Real-Time Live!</title><link>https://drsandor.net/project/rtl/</link><pubDate>Wed, 04 Sep 2024 14:00:00 +0000</pubDate><guid>https://drsandor.net/project/rtl/</guid><description>&lt;p>A long-term dream of mine came true! At the end of July 2024, we showed a demonstration at ACM SIGGRAPH 2024 in Denver for their Real-Time Live! category.&lt;/p>
&lt;p>First of all, &lt;strong>what is ACM SIGRAPH Real-Time Live?&lt;/strong> It is a specific category at the premiere computer graphics conference &lt;a href="https://en.wikipedia.org/wiki/SIGGRAPH" target="_blank" rel="noopener">ACM SIGGRAPH&lt;/a>. While SIGGRAPH has been running every year since 1974, the Real-Time Live! category (RTL for short) has only been around for 15 years. It is very engaging, as presenters have 6 minutes to show a demonstration of real-time graphics to a large audience of computer graphics professionals (~5000 people). For me, this has always been the most fascinating event of SIGGRAPH, as it shows you the maximum of whatâ€™s possible in real-time graphics today. Also, compared to research papers, you can actually see the inventors demonstrating what they made&amp;mdash; this makes much clearer what really works and what doesnâ€™t.&lt;/p>
&lt;p>Here are some of my favorites from recent years:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://youtu.be/MAXJWEoKbxY?si=s9nHKBy8K2WWPvuN&amp;amp;t=822" target="_blank" rel="noopener">AI &amp;amp; Physics-Assisted Character Pose Authoring&lt;/a> (2022)&lt;/li>
&lt;li>&lt;a href="https://www.youtube.com/watch?v=lXZhgkNFGfM" target="_blank" rel="noopener">Bebylon&lt;/a> (2018)&lt;/li>
&lt;li>IQ livecoding with Shadertoy (I canâ€™t find the video on YouTube; guess it was before they started uploading RTL to YouTube)&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>How did our demo come about?&lt;/strong> In December 2023, I contacted Matt Swoboda about submitting something to RTL. We were playing with this idea over the last decade, but it never materialized, until this year! Matt quickly brought in a great artist, while I could secure some sponsorship and support from Canon Japan, which led to this amazing constellation:&lt;/p>
&lt;ul>
&lt;li>Best real-time rendering engine for live events: &lt;a href="https://www.notch.one" target="_blank" rel="noopener">Notch&lt;/a> (Matt is their Co-Founder &amp;amp; CTO)&lt;/li>
&lt;li>Fantastic audio-visual artist: &lt;a href="https://www.brettbolton.net" target="_blank" rel="noopener">Brett Bolton&lt;/a> (Designer of U2â€™s show in the Las Vegas sphere, VJ for the Grateful Dead, etc.)&lt;/li>
&lt;li>The most stressable AR demo crew: &lt;a href="https://ar-ai.org/author/david-maruscak/" target="_blank" rel="noopener">David Maruscsak&lt;/a> and me ðŸ˜Š&lt;/li>
&lt;li>Best AR headset: &lt;a href="https://global.canon/en/technology/mr2019.html" target="_blank" rel="noopener">Canonâ€™s X1&lt;/a> (Sorry Apple, but you need to work much harder! Even though X1 was released 4 years before your Vision Pro, it has a much better image quality, better co-axial alignment of screens and cameras, better tracking infrastructure, easier integration with custom software, much better weight, etc; let me not even get started about Metaâ€™s headsetsâ€¦)&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>What is the concept of our demo?&lt;/strong> You can watch a concise summary of our
concept in the submission video below that we sent to SIGGRAPH. The main feedback that we received from the organizers was that we should expand our demo to have 2 viewers instead of 1 in order to highlight the potential to have visuals interact with multiple viewers in 3D. Challenge accepted!&lt;/p>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/uCb42SsFLk8" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;br>
&lt;p>&lt;strong>What could the audience see?&lt;/strong> Unfortunately, there were several issues with the YouTube stream of the event, so it was much harder for the remote audience to understand what was going on. Let me start by explaining what the audience in the room could see.&lt;/p>
&lt;p>
&lt;figure id="figure-view-from-stage">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="View from stage" srcset="
/media/rtl/pano_hu13dbddae45a07af144be999962d7c505_9581461_85ae5dff0cc01d3e7b407b77f81e8534.webp 400w,
/media/rtl/pano_hu13dbddae45a07af144be999962d7c505_9581461_b503633b0ef4324c0f8a2dd0045d9306.webp 760w,
/media/rtl/pano_hu13dbddae45a07af144be999962d7c505_9581461_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://drsandor.net/media/rtl/pano_hu13dbddae45a07af144be999962d7c505_9581461_85ae5dff0cc01d3e7b407b77f81e8534.webp"
width="760"
height="194"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
View from stage
&lt;/figcaption>&lt;/figure>
&lt;figure id="figure-audience-view-of-stage">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Audience view of stage" srcset="
/media/rtl/stage_huc5c81e4ab00a316abb82eed7380d6a68_577662_c859b43e2dea225e971b0373b49b770b.webp 400w,
/media/rtl/stage_huc5c81e4ab00a316abb82eed7380d6a68_577662_5ff530d950c2bd588653e2976b777df1.webp 760w,
/media/rtl/stage_huc5c81e4ab00a316abb82eed7380d6a68_577662_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://drsandor.net/media/rtl/stage_huc5c81e4ab00a316abb82eed7380d6a68_577662_c859b43e2dea225e971b0373b49b770b.webp"
width="760"
height="570"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Audience view of stage
&lt;/figcaption>&lt;/figure>
&lt;figure id="figure-setup-details-on-the-right-stage-with-brett-playing-behind-him-traditional-audio-reactive-visuals-2-viewers-with-ar-headsets-in-front-of-the-stage-left-side-split-screen-showing-the-augmented-views-of-the-2-viewers">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Setup details. On the right: stage with Brett playing; behind him traditional audio-reactive visuals; 2 viewers with AR headsets in front of the stage. Left side: split screen, showing the augmented views of the 2 viewers." srcset="
/media/rtl/schema_hub0ff15292ea34398e4f86ba5f2aaa5bd_488378_f46bfc25bcc2119347d582a4176d1611.webp 400w,
/media/rtl/schema_hub0ff15292ea34398e4f86ba5f2aaa5bd_488378_91496829229e096b8f287e95293cca16.webp 760w,
/media/rtl/schema_hub0ff15292ea34398e4f86ba5f2aaa5bd_488378_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://drsandor.net/media/rtl/schema_hub0ff15292ea34398e4f86ba5f2aaa5bd_488378_f46bfc25bcc2119347d582a4176d1611.webp"
width="760"
height="428"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Setup details. On the right: stage with Brett playing; behind him traditional audio-reactive visuals; 2 viewers with AR headsets in front of the stage. Left side: split screen, showing the augmented views of the 2 viewers.
&lt;/figcaption>&lt;/figure>&lt;/p>
&lt;p>The setup shown in the above figure was not clear for viewers of the YouTube stream. It did not help that for the first half of our performance, the stream was only showing the view of 1 viewer.&lt;/p>
&lt;p>&lt;strong>The main challenge&lt;/strong> with our setup was controlling the lighting. Since the
sensors are quite sensitive to external lighting (as well as the AR content) we did our best to control this (including working with the Siggraph AV team), but even with our best efforts we had some glitches during the final performance. That said, we still got very nice compliments from the other participants and the audience.&lt;/p>
&lt;figure id="figure-final-rehearsal-note-the-colorful-background-behind-brett-compared-to-the-black-background-at-the-actual-performance">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Final rehearsal: note the colorful background behind Brett, compared to the black background at the actual performance" srcset="
/media/rtl/venue-rehearsal_hu0b7ba7870b9691ab7eeba5cdc97383b9_5224857_6a74a44848668db074c17e6d92c3be52.webp 400w,
/media/rtl/venue-rehearsal_hu0b7ba7870b9691ab7eeba5cdc97383b9_5224857_bac4666bfaa2864a45bfe5b5731116c3.webp 760w,
/media/rtl/venue-rehearsal_hu0b7ba7870b9691ab7eeba5cdc97383b9_5224857_1200x1200_fit_q75_h2_lanczos_3.webp 1200w"
src="https://drsandor.net/media/rtl/venue-rehearsal_hu0b7ba7870b9691ab7eeba5cdc97383b9_5224857_6a74a44848668db074c17e6d92c3be52.webp"
width="760"
height="384"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Final rehearsal: note the colorful background behind Brett, compared to the black background at the actual performance
&lt;/figcaption>&lt;/figure>
&lt;figure id="figure-final-rehearsal-the-colorful-bright-background-is-very-nice-because-it-illuminates-brett-2-views-on-left-and-shows-off-refractions-of-virtual-water-2-views-on-right--in-the-actual-performance-we-had-to-switch-back-to-black-because-of-tracking-instabilities">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Final rehearsal: the colorful bright background is very nice, because: it illuminates Brett (2 views on left) and shows off refractions of virtual water (2 views on right) . In the actual performance, we had to switch back to black because of tracking instabilities" srcset="
/media/rtl/rehearsal-details_hu6732b02741823ee3381c983d5d4ece3c_175293_f64b37e0f270f1ddb60658f560e03309.webp 400w,
/media/rtl/rehearsal-details_hu6732b02741823ee3381c983d5d4ece3c_175293_2807ec2913c180a051d71c5c32f712bb.webp 760w,
/media/rtl/rehearsal-details_hu6732b02741823ee3381c983d5d4ece3c_175293_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://drsandor.net/media/rtl/rehearsal-details_hu6732b02741823ee3381c983d5d4ece3c_175293_f64b37e0f270f1ddb60658f560e03309.webp"
width="760"
height="283"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Final rehearsal: the colorful bright background is very nice, because: it illuminates Brett (2 views on left) and shows off refractions of virtual water (2 views on right) . In the actual performance, we had to switch back to black because of tracking instabilities
&lt;/figcaption>&lt;/figure>
&lt;p>&lt;strong>Showtime!&lt;/strong> After months of hard work, we were finally ready to demo! The hall filling with the audience was an awe-inspiring moment for me (I estimate about 3500 attendants).&lt;/p>
&lt;video width="720" autoplay loop>
&lt;source src="showtimelapse.mp4" type="video/mp4" />
&lt;/video>
&lt;figure id="figure-selfie-just-before-the-start-of-the-show">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Selfie just before the start of the show" srcset="
/media/rtl/showtime_hu8400259ed9cc12074f04526ca19f5c86_2971141_285ff3afdc452cdaf1cdcfacb7ab255c.webp 400w,
/media/rtl/showtime_hu8400259ed9cc12074f04526ca19f5c86_2971141_4f4d81e45566148609b848161d9de68e.webp 760w,
/media/rtl/showtime_hu8400259ed9cc12074f04526ca19f5c86_2971141_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://drsandor.net/media/rtl/showtime_hu8400259ed9cc12074f04526ca19f5c86_2971141_285ff3afdc452cdaf1cdcfacb7ab255c.webp"
width="760"
height="570"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Selfie just before the start of the show
&lt;/figcaption>&lt;/figure>
&lt;p>Luckily we were the first team to present, so we could relax afterwards and enjoy the show, which contained some truly remarkable demos, including:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://www.youtube.com/live/Gm1B5DT8kE0?si=bXvehrC7aaXZARj_&amp;amp;t=429" target="_blank" rel="noopener">The controller for Jim Henson&amp;rsquo;s muppets&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.youtube.com/live/Gm1B5DT8kE0?si=oKXaQpHSPQ6l-OB5&amp;amp;t=5459" target="_blank" rel="noopener">Movin Tracin&amp;rsquo;&lt;/a>: my personal favorite. I was surprised that this demo did not get any award.&lt;/li>
&lt;li>&lt;a href="https://www.youtube.com/live/Gm1B5DT8kE0?si=Xm3XuXkt52NV09Mr&amp;amp;t=2231" target="_blank" rel="noopener">Mesh Mortal Combat&lt;/a>: winner of both awards (audience &amp;amp; jury).&lt;/li>
&lt;/ul>
&lt;p>You can also just watch &lt;a href="https://www.youtube.com/live/Gm1B5DT8kE0?si=Xm3XuXkt52NV09Mr" target="_blank" rel="noopener">the whole stream&lt;/a>, or jump directly to &lt;a href="https://www.youtube.com/live/Gm1B5DT8kE0?si=MLAwyoSwGnKb75dI&amp;amp;t=1215" target="_blank" rel="noopener">our part&lt;/a>.&lt;/p>
&lt;p>&lt;strong>Conclusions&lt;/strong> It was a great experience to present at Real-Time Live! I hope to return in coming years!&lt;/p>
&lt;p>What we learned from this work is that there is a huge potential for having visuals at live events in AR headsets; we got encouraging feedback at the conference!&lt;/p>
&lt;p>We also learned that it is very challenging, both in terms of technology, as well as in terms of having a viable business model. But, we have ideas for both. Stay tuned!&lt;/p>
&lt;p>&lt;strong>Acknowledgements&lt;/strong>
First and foremost, I would like to thank Brett and Matt to participate in this project. They are exemplars of a rare breed: extremely skilled folks who put creativity before money.&lt;/p>
&lt;p>Second, I would like to thank Canon for advanced support and a financial contribution to our costs.&lt;/p>
&lt;p>Last, but not least, I would also like to thank the RTL organizers, who did a really great job of putting this event together! See you next year!&lt;/p>
&lt;!--
&lt;figure id="figure-the-team">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="The Team" srcset="
/media/rtl/team_huc83d12a88c14e6bb7d48ea036a0b40a4_618767_e94a7d77b8229d8730374d15bb4843d8.webp 400w,
/media/rtl/team_huc83d12a88c14e6bb7d48ea036a0b40a4_618767_18ac242625abffd0033eee0a0f8b21a8.webp 760w,
/media/rtl/team_huc83d12a88c14e6bb7d48ea036a0b40a4_618767_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://drsandor.net/media/rtl/team_huc83d12a88c14e6bb7d48ea036a0b40a4_618767_e94a7d77b8229d8730374d15bb4843d8.webp"
width="760"
height="428"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
The Team
&lt;/figcaption>&lt;/figure>--></description></item></channel></rss>