<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>new | Christian Sandor</title><link>https://drsandor.net/tag/new/</link><atom:link href="https://drsandor.net/tag/new/index.xml" rel="self" type="application/rss+xml"/><description>new</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Sun, 01 Feb 2026 14:00:00 +0000</lastBuildDate><image><url>https://drsandor.net/media/icon_hu287fcf417612116bcf0d00ac1ba165d7_82622_512x512_fill_lanczos_center_3.png</url><title>new</title><link>https://drsandor.net/tag/new/</link></image><item><title>Teaching AI at Elementary School</title><link>https://drsandor.net/project/ai-school/</link><pubDate>Sun, 01 Feb 2026 14:00:00 +0000</pubDate><guid>https://drsandor.net/project/ai-school/</guid><description>&lt;p>Test&lt;/p></description></item><item><title>Course: Generative AI with ComfyUI</title><link>https://drsandor.net/project/course/</link><pubDate>Tue, 18 Nov 2025 14:00:00 +0000</pubDate><guid>https://drsandor.net/project/course/</guid><description>&lt;blockquote class="wp-block-quote">
&lt;p>
The world of Generative AI is complex, and fast-moving. [...] There are so many models, and so many complexities, that this course was essential to cut through all the noise.
&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a href="https://www.linkedin.com/in/matt-swoboda-b820872/" target="_blank" rel="noopener">Matt Swoboda&lt;/a> - &lt;a href="https://www.notch.one" target="_blank" rel="noopener">Notch&lt;/a> - Founder and Lead Developer&lt;/p>
&lt;p>Over the last few years, we have seen a rapid and wide-spread uptake of generative AI in industry and academia. One tool has established itself as the de-facto standard for rapid experimentation, as well as professional production: &lt;a href="https://en.wikipedia.org/wiki/ComfyUI" target="_blank" rel="noopener">ComfyUI&lt;/a>. It is used in projects ranging from commercial productions like &lt;a href="https://en.wikipedia.org/wiki/The_Wizard_of_Oz_at_Sphere" target="_blank" rel="noopener">The Wizard of Oz at the Las Vegas Sphere&lt;/a> up to leading-edge research projects like &lt;a href="https://github.com/bytedance/ComfyUI-HyperLoRA" target="_blank" rel="noopener">Bytedance&amp;rsquo;s HyperLORA&lt;/a>.&lt;/p>
&lt;p>Based on my personal experience with using ComfyUI almost daily from day 1, I have developed a course with unique features:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Only uses open-weights models&lt;/strong> that can be run on your own machine; no cloud services are used.&lt;/li>
&lt;li>&lt;strong>From the ground up:&lt;/strong> after 1.5 hours into this course, you will understand the underlying concept of every single item in the simplest ComfyUI workflow, including: Flow Matching, Variational Autoencoders, CLIP, and CFG.&lt;/li>
&lt;li>&lt;strong>Dozens of hand-crafted workflows&lt;/strong> as starting point for your own explorations.&lt;/li>
&lt;li>&lt;strong>Covers essential system administration tasks&lt;/strong> to allow continuous upgrading and also rollbacks of your ComfyUI installation.&lt;/li>
&lt;/ul>
&lt;!--
&lt;figure id="figure-medley-of-course-content-all-generated-live-during-the-course-please-click-to-enlarge">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Medley of course content, all generated live during the course. Please click to enlarge." srcset="
/project/course/featured_hu023d3431561f09a34c8ed9baf950ffc3_2575599_e83663696f988efb8bf5bc268c2c1a66.webp 400w,
/project/course/featured_hu023d3431561f09a34c8ed9baf950ffc3_2575599_8242d9c57570775911f5b104c654c042.webp 760w,
/project/course/featured_hu023d3431561f09a34c8ed9baf950ffc3_2575599_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://drsandor.net/project/course/featured_hu023d3431561f09a34c8ed9baf950ffc3_2575599_e83663696f988efb8bf5bc268c2c1a66.webp"
width="760"
height="364"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Medley of course content, all generated live during the course. Please click to enlarge.
&lt;/figcaption>&lt;/figure>
&lt;figure id="figure-medley-of-course-content-all-generated-live-during-the-course-please-click-to-enlarge">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="quilt.gif" alt="Medley of course content, all generated live during the course. Please click to enlarge." loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Medley of course content, all generated live during the course. Please click to enlarge.
&lt;/figcaption>&lt;/figure>
-->
&lt;p>Below, a video collage of some of the course content, all generated live during the course.&lt;/p>
&lt;video width="720" controls loop>
&lt;source src="quilt.m4v" type="video/mp4" />
&lt;/video>
&lt;blockquote class="wp-block-quote">
&lt;p>
Participants donâ€™t just reproduce results: they understand the role of each block, how parameters influence outcomes, and how to adapt the system to their own creative goals.
&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a href="https://bammey.com" target="_blank" rel="noopener">Quentin Bammey&lt;/a> - &lt;a href="https://en.wikipedia.org/wiki/T%c3%a9l%c3%a9com_Paris" target="_blank" rel="noopener">TÃ©lÃ©com Paris&lt;/a> - Assistant Professor (Machine Learning)&lt;/p>
&lt;p>The course contains theoretical presentations, demonstrations, and hands-on exercises. The main modules are:&lt;/p>
&lt;ol>
&lt;li>Course Introduction&lt;/li>
&lt;li>Image Generation&lt;/li>
&lt;li>Video Generation&lt;/li>
&lt;li>Complex Workflows; e.g.
&lt;ul>
&lt;li>Spatio-temporal upscaling of videos (e.g. 16fps@720p =&amp;gt; 60fps@4k)&lt;/li>
&lt;li>Powerful workflows combining vision language models, large language models, and video generation models&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ol>
&lt;p>The full course takes 2 days but can be presented as a distilled version in 1 day. Typical class size is 8, but can be scaled from 1-20. The content will be adapted to the client&amp;rsquo;s needs, for example:&lt;/p>
&lt;ul>
&lt;li>Technical vs. design focus&lt;/li>
&lt;li>Audience size&lt;/li>
&lt;li>Available hardware for students&lt;/li>
&lt;/ul>
&lt;p>&lt;em>Please &lt;a href="mailto:christian@sandor.com">email me&lt;/a> your requirements to receive a &lt;strong>custom quote&lt;/strong>.&lt;/em>&lt;/p>
&lt;h2 id="testimonials">Testimonials&lt;/h2>
&lt;blockquote class="wp-block-quote">
&lt;p>
The world of Generative AI is complex, and fast-moving. Dr Sandor's excellent course demystified it for us; first with a solid technical explanation of the concepts, and then a guided exploration of the key workflows and models for image and video generation, via ComfyUI. Hands-on learning and demonstration, plus the excellent course materials, got us to the point of being able to do things for ourselves confidently and quickly. There are so many models, and so many complexities, that this course was essential to cut through all the noise.
&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a href="https://www.linkedin.com/in/matt-swoboda-b820872/" target="_blank" rel="noopener">Matt Swoboda&lt;/a> - &lt;a href="https://www.notch.one" target="_blank" rel="noopener">Notch&lt;/a> - Founder and Lead Developer&lt;/p>
&lt;blockquote class="wp-block-quote">
&lt;p>
Christian Sandorâ€™s ComfyUI tutorial offers a remarkably clear introduction to modern image and video generation. He presents the core ideas behind these models in a way that is intuitive and immediately connected to practice. Rather than letting ComfyUI remain a collection of opaque nodes, he shows how each step of the workflow reflects a meaningful operation in the underlying process, and why the interface is structured the way it is. This gives participants a solid conceptual footing before they even start experimenting.
&lt;/p>
&lt;p>
As the session unfolds, everyone works with templates while also learning how to read, modify, and build workflows with intention. Participants donâ€™t just reproduce results: they understand the role of each block, how parameters influence outcomes, and how to adapt the system to their own creative goals. It is a course that makes complex ideas accessible without simplifying them, and that leaves participants both confident and genuinely capable. I highly recommend Dr. Sandor's course to anyone who wants to use ComfyUI thoughtfully and with real understanding.
&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a href="https://bammey.com" target="_blank" rel="noopener">Quentin Bammey&lt;/a> - &lt;a href="https://en.wikipedia.org/wiki/T%c3%a9l%c3%a9com_Paris" target="_blank" rel="noopener">TÃ©lÃ©com Paris&lt;/a> - Assistant Professor (Machine Learning)&lt;/p>
&lt;blockquote class="wp-block-quote">
&lt;p>
Before the course, I had no real experience working with ComfyUI. I had seen it in action and knew how powerful it is. I wanted to learn its image and video generation workflows, but the amount of information and moving pieces involved can be quite overwhelming at first. Every time I tried to start my own projects, I got lost in the complexity of setting up the tool and finding the right workflow for my ideas, and I never managed to take the time to truly dig in and learn it thoroughly. Fortunately, I had the opportunity to take the course, and it really made all the difference.
&lt;/p>
&lt;p>
Dr Sandorâ€™s excellent instruction, combined with his insight and the courseâ€™s high-quality materials, provided me with extensive hands-on knowledge â€” covering everything from the initial tool setup and the theory behind the workflows to the creation of complex workflows that I now use confidently in my own projects. The course not only gave me a solid understanding of ComfyUI, but also the confidence to explore and build advanced image and video generation pipelines independently. The course materials go even further, providing me with excellent reference points that I can return to whenever needed.
&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a href="https://www.linkedin.com/in/nikoryytty/" target="_blank" rel="noopener">Niko Ryytty&lt;/a> - &lt;a href="https://www.notch.one" target="_blank" rel="noopener">Notch&lt;/a> - Architect &amp;amp; Web Technologies Lead&lt;/p></description></item><item><title>Keynote at ICXR</title><link>https://drsandor.net/project/icxr/</link><pubDate>Fri, 07 Nov 2025 14:00:00 +0000</pubDate><guid>https://drsandor.net/project/icxr/</guid><description>&lt;p>Every time I visit China, I am absolutely astonished by their pace of progress. It is simply not possible to grasp this from the outside, as lots of information is not very accessible from abroad. In this post, I would like to share some of the things I learned during my visit.&lt;/p>
&lt;p>I presented a keynote at the new and upcoming &lt;a href="https://icxr.net/2025/index.html" target="_blank" rel="noopener">International Conference on Extended Reality&lt;/a> (ICXR); even though it was only the second time that this conference took place, it was well attended. It was co-located with &lt;a href="https://chinavr2025.qdvri.com" target="_blank" rel="noopener">ChinaVR&lt;/a>, the biggest VR conference of China (running since 2001).&lt;/p>
&lt;p>It is important to understand how Chinese research is organized. The Chinese research community elects &lt;a href="https://en.wikipedia.org/wiki/Academician_of_the_Chinese_Academy_of_Sciences" target="_blank" rel="noopener">Academicians (ä¸­å›½ç§‘å­¦é™¢é™¢å£«)&lt;/a> for all important research areas. Then, the Academician has strong influence on the policy, research, and commercial developments in China. I don&amp;rsquo;t think neither the US nor Europe have any equivalent of this. 3 Academicians attended the event:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://cg.cs.tsinghua.edu.cn/shimin.htm" target="_blank" rel="noopener">Shi-min Hu&lt;/a>: Computer Graphics&lt;/li>
&lt;li>&lt;a href="https://baike.baidu.com/item/%e8%b5%b5%e6%b2%81%e5%b9%b3/7417905" target="_blank" rel="noopener">Qinping Zhao&lt;/a>: Virtual Reality&lt;/li>
&lt;li>&lt;a href="https://faculty.bjtu.edu.cn/eaie/6358.html" target="_blank" rel="noopener">Hongke Zhang&lt;/a>: Networking&lt;/li>
&lt;/ul>
&lt;p>There were also 2 more researchers of that caliber, who I guess will become Academicians sooner or later:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://baoquanchen.info" target="_blank" rel="noopener">Baoquan Chen&lt;/a>: Computer Vision&lt;/li>
&lt;li>&lt;a href="https://fve.bfa.edu.cn/info/1056/2054.htm" target="_blank" rel="noopener">Yongtian Wang&lt;/a>: Optics for AR/VR&lt;/li>
&lt;/ul>
&lt;p>Several companies showed exhibits, including &lt;strong>Goertek&lt;/strong>. To be honest, I have never heard of this company before! But, they are producing &lt;strong>80% of the world&amp;rsquo;s AR and VR glasses&lt;/strong>! Could you have guessed that based on their &lt;a href="https://en.wikipedia.org/wiki/Goertek" target="_blank" rel="noopener">Wikipedia page&lt;/a>? Unfortunately, photos were strictly prohibited at their large booth. I could clearly see that they are not simply a factory for US companies, but very innovative and contributing key technological developments, including:&lt;/p>
&lt;ul>
&lt;li>High pixel density display panels (Micro OLED)&lt;/li>
&lt;li>Miniaturized water cooling for AR/VR glasses (the water-cooled board they showcased was about 2x4 cm big; water pipes around 2mm diameter)&lt;/li>
&lt;li>Advanced pancake optics&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Hongke Zhang&lt;/strong> presented a thought-provoking keynote along the lines of: What network infrastructure do we need to support the rapidly changing network requirements in an AI world? It included many science-fiction level concepts such as:&lt;/p>
&lt;ul>
&lt;li>A longterm roadmap including 6G and beyond with mobile speeds of Terabits per second&lt;/li>
&lt;li>Mesh-networking with drone swarms&lt;/li>
&lt;li>Streaming neural content (NERFs, Gaussian Splats, etc)&lt;/li>
&lt;/ul>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="" srcset="
/project/icxr/zhang1_hu307ffe06fa0f0a670173d11c2083b3d1_607338_8a43a8be846f13d72cc33c24cba4b58f.webp 400w,
/project/icxr/zhang1_hu307ffe06fa0f0a670173d11c2083b3d1_607338_36d5485571fe887cb67e75c802aaa646.webp 760w,
/project/icxr/zhang1_hu307ffe06fa0f0a670173d11c2083b3d1_607338_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://drsandor.net/project/icxr/zhang1_hu307ffe06fa0f0a670173d11c2083b3d1_607338_8a43a8be846f13d72cc33c24cba4b58f.webp"
width="760"
height="570"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="" srcset="
/project/icxr/zhang2_hu4a70eccacf39bf605e2091d4dd40c932_633050_89802289d51d9d197134dc144cfc2eea.webp 400w,
/project/icxr/zhang2_hu4a70eccacf39bf605e2091d4dd40c932_633050_a685ee0f8e782cd067056cc30638a894.webp 760w,
/project/icxr/zhang2_hu4a70eccacf39bf605e2091d4dd40c932_633050_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://drsandor.net/project/icxr/zhang2_hu4a70eccacf39bf605e2091d4dd40c932_633050_89802289d51d9d197134dc144cfc2eea.webp"
width="760"
height="570"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="" srcset="
/project/icxr/zhang3_hue07d29e6f16d0edda3870e3fa88a1557_629518_e2d2058e8bb3f6c80b97f3304a6a2304.webp 400w,
/project/icxr/zhang3_hue07d29e6f16d0edda3870e3fa88a1557_629518_741f1728e9355540cb4f0051b7cacf1b.webp 760w,
/project/icxr/zhang3_hue07d29e6f16d0edda3870e3fa88a1557_629518_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://drsandor.net/project/icxr/zhang3_hue07d29e6f16d0edda3870e3fa88a1557_629518_e2d2058e8bb3f6c80b97f3304a6a2304.webp"
width="760"
height="570"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;p>Next up was &lt;strong>Baoquan Chen&lt;/strong>. A major thread of his talk was along the lines of (please excuse my informal language):&lt;/p>
&lt;ul>
&lt;li>AI Video generators along the lines of SORA and VEO suck, because they lack understanding of the physical world. They are essentially a dead end.&lt;/li>
&lt;li>Gaussian Splatting is very popular now, but it is by itself also a dead end, because of the inherent inflexibility and non-interactiveness.&lt;/li>
&lt;/ul>
&lt;p>I already came to similar conclusions, albeit for different reasons :-) He then presented several recent works by his group how to overcome these limitations. In short: real instead of hallucinated physics are needed!&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="" srcset="
/project/icxr/chen1_hufe082b017d9719ead8de3b0b5e59b3d3_544830_9982a9761f77d7bd7df3367926c787fd.webp 400w,
/project/icxr/chen1_hufe082b017d9719ead8de3b0b5e59b3d3_544830_e808e18baa805a907cf8a7ec56af595f.webp 760w,
/project/icxr/chen1_hufe082b017d9719ead8de3b0b5e59b3d3_544830_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://drsandor.net/project/icxr/chen1_hufe082b017d9719ead8de3b0b5e59b3d3_544830_9982a9761f77d7bd7df3367926c787fd.webp"
width="760"
height="570"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="" srcset="
/project/icxr/chen2_huadedc5b8b6f11091bcbfafbc1c1ed320_505349_540916450eb775080b984f1bce2a6c3e.webp 400w,
/project/icxr/chen2_huadedc5b8b6f11091bcbfafbc1c1ed320_505349_5ceeb53afb3fb36c5710848b674afd77.webp 760w,
/project/icxr/chen2_huadedc5b8b6f11091bcbfafbc1c1ed320_505349_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://drsandor.net/project/icxr/chen2_huadedc5b8b6f11091bcbfafbc1c1ed320_505349_540916450eb775080b984f1bce2a6c3e.webp"
width="760"
height="570"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="" srcset="
/project/icxr/chen3_hu2784ae0ef6911b4defd144ef4bc2371f_796921_a1c070b5b5ff17cf4fc092428a798286.webp 400w,
/project/icxr/chen3_hu2784ae0ef6911b4defd144ef4bc2371f_796921_511db7516ebfbf70775aad112a26ee4f.webp 760w,
/project/icxr/chen3_hu2784ae0ef6911b4defd144ef4bc2371f_796921_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://drsandor.net/project/icxr/chen3_hu2784ae0ef6911b4defd144ef4bc2371f_796921_a1c070b5b5ff17cf4fc092428a798286.webp"
width="760"
height="570"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;p>I was extremely glad that &lt;strong>Shi-Min Hu&lt;/strong> chiseled out 20 minutes out of his busy schedule to talk to me. Our chat was very intense&amp;hellip; I now understand better China&amp;rsquo;s strategy how to deal with the import restrictions of NVIDIA GPUs. I mean, at the moment, Chinese AI models are already on the same level as their US counterparts (I personally think that they are actually already better, but this is debatable). They achieved this even though their current GPUs are much weaker than the ones Meta, OpenAI etc. are using. If they get comparable compute power as the US than their AI models will run circles around their US counterparts. It will happen much sooner than the public thinks&amp;hellip; I can&amp;rsquo;t wait to switch to Chinese GPUs!&lt;/p>
&lt;p>It was a lucky accident that I sat next to &lt;strong>Yongtian Wang&lt;/strong> at the closing dinner. He shared some incredible stories about the custom optics design software he wrote back in the day (in Fortran!): GOLD = &lt;strong>G&lt;/strong>eneral &lt;strong>O&lt;/strong>ptical &lt;strong>L&lt;/strong>ens &lt;strong>D&lt;/strong>esign system :-) As their group can&amp;rsquo;t use the world-wide standard software (Code V; another US import restriction), they have been recently reviving this decade-old software.&lt;/p>
&lt;p>Finally, I should also spend some words on my keynote. My recent discussions with Notch&amp;rsquo;s Matt Swoboda (the grand wizard of realtime computer graphics) inspired many of the open questions that I discussed in my talk.&lt;/p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="" srcset="
/project/icxr/questions_hu5636f1c492ebb8d576e1c13f2147d42f_394607_f55721850aaf7cf860e43549482454be.webp 400w,
/project/icxr/questions_hu5636f1c492ebb8d576e1c13f2147d42f_394607_76bc53af824a7556e83d173b721936b1.webp 760w,
/project/icxr/questions_hu5636f1c492ebb8d576e1c13f2147d42f_394607_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://drsandor.net/project/icxr/questions_hu5636f1c492ebb8d576e1c13f2147d42f_394607_f55721850aaf7cf860e43549482454be.webp"
width="760"
height="428"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;p>To conclude, a little video from the soundcheck of my presentation. The sound system was really good!&lt;/p>
&lt;video width="1920" controls loop>
&lt;source src="soundcheck.mp4" type="video/mp4" />
&lt;/video></description></item><item><title>Keynote at ADOS Paris</title><link>https://drsandor.net/ai/ados/</link><pubDate>Thu, 10 Apr 2025 14:00:00 +0000</pubDate><guid>https://drsandor.net/ai/ados/</guid><description>&lt;p>On 28 March 2025, I attended the amazing &lt;a href="https://www.lightricks.com/ados" target="_blank" rel="noopener">ADOS&lt;/a> event in Paris at &lt;a href="https://www.artifex-lab.com" target="_blank" rel="noopener">Artifex Lab&lt;/a>, a new space in Paris for bringing together AI arists and technologists. ADOS was co-organized by &lt;a href="https://banodoco.ai" target="_blank" rel="noopener">Banadoco&lt;/a> and &lt;a href="https://www.lightricks.com" target="_blank" rel="noopener">Lightricks&lt;/a>, which was an ideal fit for Artifex Lab: Banadoco is the leading online community for AI art+technology and Lightricks&amp;rsquo;s LTX is the fastest generative AI video model (and it&amp;rsquo;s Open Source on top of that!).&lt;/p>
&lt;p>Coincidentally, our team has recently been investigating LTX deeply and also has been hanging out a lot on Banadoco&amp;rsquo;s inspiring Discord server. One thing led to another and I was invited to present a keynote at ADOS. We brought most of the team, as well as a bleeding edge demo. Unfortuntately, I missed out on the second day of the event, which featured a hackathon, where our PhD student Hovhannes received a prize (see his upcoming blogpost on our &lt;a href="https://ar-ai.org" target="_blank" rel="noopener">team webpage&lt;/a>).&lt;/p>
&lt;p>
&lt;figure id="figure-team-arai-at-ados">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Team ARAI at ADOS" srcset="
/media/ados/team_hu3a541a33e063e69d3b30aeefb30179f7_534763_b783a900c056276757e7f7017585a4b8.webp 400w,
/media/ados/team_hu3a541a33e063e69d3b30aeefb30179f7_534763_4448afd20a1677241c97a762bfe02851.webp 760w,
/media/ados/team_hu3a541a33e063e69d3b30aeefb30179f7_534763_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://drsandor.net/media/ados/team_hu3a541a33e063e69d3b30aeefb30179f7_534763_b783a900c056276757e7f7017585a4b8.webp"
width="760"
height="570"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Team ARAI at ADOS
&lt;/figcaption>&lt;/figure>
&lt;figure id="figure-zeev-farbman-co-founder--ceo-of-lightricks-trying-our-demo">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Zeev Farbman, Co-Founder &amp;amp; CEO of Lightricks trying our demo" srcset="
/media/ados/demo_hua7d9fbdda16f2d02e5b664a9fb28b240_541462_3cc95a501fbfe4de2ff237a29e99ded4.webp 400w,
/media/ados/demo_hua7d9fbdda16f2d02e5b664a9fb28b240_541462_007047a2ff3dd4aaa738306758667246.webp 760w,
/media/ados/demo_hua7d9fbdda16f2d02e5b664a9fb28b240_541462_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://drsandor.net/media/ados/demo_hua7d9fbdda16f2d02e5b664a9fb28b240_541462_3cc95a501fbfe4de2ff237a29e99ded4.webp"
width="760"
height="570"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Zeev Farbman, Co-Founder &amp;amp; CEO of Lightricks trying our demo
&lt;/figcaption>&lt;/figure>&lt;/p>
&lt;p>Before reporting about the keynote and demo, I would like to highlight some of my favourite things that I saw at the event. All talks are &lt;a href="https://www.youtube.com/watch?v=gBeZSbaxMvc" target="_blank" rel="noopener">recorded on youtube&lt;/a>; I provide direct links to the talks in the following.&lt;/p>
&lt;p>I enjoyed &lt;a href="https://www.youtube.com/live/gBeZSbaxMvc?si=orYRKkprSBqWWwBo&amp;amp;t=468" target="_blank" rel="noopener">Emma Catnip&amp;rsquo;s talk&lt;/a>, particularly her work Bloom. It is very soulful and beautifully designed. I will never forget her bitter-sweet explanation: &amp;ldquo;It&amp;rsquo;s about the summer of love I that never had&amp;rdquo;.&lt;/p>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/cDUExDUYRaE" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;p>Another very interesting artist&amp;rsquo;s talk was by &lt;a href="https://udart.dk" target="_blank" rel="noopener">Vibeke Bertelsen (Udart)&lt;/a>; &lt;a href="https://www.youtube.com/live/gBeZSbaxMvc?si=r3DAzpV5a7a7C8jd&amp;amp;t=8304" target="_blank" rel="noopener">timestamp&lt;/a>. Imagine current AI tools in the hands of H.R. Giger&amp;hellip; Below is one of her award-winning short movies. Definitely most unique!&lt;/p>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/8Vodtg8WNiY" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;p>On the tech side, my favourite talk was by &lt;a href="https://github.com/yvann-ba" target="_blank" rel="noopener">Yvann Barbot&lt;/a>. In &lt;a href="https://www.youtube.com/live/gBeZSbaxMvc?si=utqO7-YM0Oq1c5Fx&amp;amp;t=8960" target="_blank" rel="noopener">his talk&lt;/a>, he showed how easy it is to generate cool audio-reactive video clips using his ComfyUI nodes. One example below by &lt;a href="https://www.youtube.com/@visualfrisson" target="_blank" rel="noopener">Visual Frisson&lt;/a>. After the event, I checked Yvann&amp;rsquo;s youtube channel. I can highly recommend it!&lt;/p>
&lt;video width="720" controls>
&lt;source src="frisson.mp4" type="video/mp4" />
&lt;/video>
&lt;p>Finally, let me get to my keynote talk and our demo.&lt;/p>
&lt;p>Due to the extreme technical difficulties involved in our demo, we were working literally till the last minute on it. Chapeau to DÃ¡vid MaruscsÃ¡k and Francesco Dettori for their very hard work near demo time! Very unfortunately, the backend of our demo stopped working 5 minutes before my talk started. As a result, we could not show the demo during my keynote (as we had originally planned), but only 2 hours later. Luckily, most attendants of ADOS were still there and could give it a try!&lt;/p>
&lt;p>We demonstrate how to use a cloud backend for highspeed generation of spatial videos based on image and text inputs. You can see a similar version of the demo below; compared to the demo we showed at ADOS, it is only using 1 local RTX 4090 GPU for inference, as opposed to 4 H100 GPUs at ADOS, so inference speed is significantly lower.&lt;/p>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/Gk2dtoOXDac" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;p>You can see my keynote on &lt;a href="https://www.youtube.com/live/gBeZSbaxMvc?si=qr5cMNz5WXu8Jt7d&amp;amp;t=7061" target="_blank" rel="noopener">youtube&lt;/a>, where I speak about my long past with AR and my more recent past with AI, and how everything converged into this demo :-)&lt;/p>
&lt;p>To conclude, I would like to sincerely thank:&lt;/p>
&lt;ul>
&lt;li>The &lt;a href="https://www.lightricks.com/ados" target="_blank" rel="noopener">ADOS&lt;/a> organizational team: &lt;a href="https://banodoco.ai" target="_blank" rel="noopener">Banadoco&lt;/a>, &lt;a href="https://www.lightricks.com" target="_blank" rel="noopener">Lightricks&lt;/a>, and &lt;a href="https://www.artifex-lab.com" target="_blank" rel="noopener">Artifex Lab&lt;/a>.&lt;/li>
&lt;li>Special thanks to Lightricks for giving us early access to their distilled LTX model, which enabled impressive speedups.&lt;/li>
&lt;li>All &lt;a href="https://ar-ai.org/people/" target="_blank" rel="noopener">ARAI team members&lt;/a> for bearing with my demands :-) Directly involved in the demo were Cyrille Leroux, Thomas Betton, DÃ¡vid MaruscsÃ¡k, Francesco Dettori, and Hovhannes Margaryan.&lt;/li>
&lt;/ul></description></item><item><title>Keynote at ADOS Paris</title><link>https://drsandor.net/project/ai3/</link><pubDate>Thu, 10 Apr 2025 14:00:00 +0000</pubDate><guid>https://drsandor.net/project/ai3/</guid><description>&lt;p>Test&lt;/p></description></item><item><title>Demo at ACM SIGGRAPH Real-Time Live!</title><link>https://drsandor.net/project/rtl/</link><pubDate>Wed, 04 Sep 2024 14:00:00 +0000</pubDate><guid>https://drsandor.net/project/rtl/</guid><description>&lt;p>A long-term dream of mine came true! At the end of July 2024, we showed a demonstration at ACM SIGGRAPH 2024 in Denver for their Real-Time Live! category.&lt;/p>
&lt;p>First of all, &lt;strong>what is ACM SIGRAPH Real-Time Live?&lt;/strong> It is a specific category at the premiere computer graphics conference &lt;a href="https://en.wikipedia.org/wiki/SIGGRAPH" target="_blank" rel="noopener">ACM SIGGRAPH&lt;/a>. While SIGGRAPH has been running every year since 1974, the Real-Time Live! category (RTL for short) has only been around for 15 years. It is very engaging, as presenters have 6 minutes to show a demonstration of real-time graphics to a large audience of computer graphics professionals (~5000 people). For me, this has always been the most fascinating event of SIGGRAPH, as it shows you the maximum of whatâ€™s possible in real-time graphics today. Also, compared to research papers, you can actually see the inventors demonstrating what they made&amp;mdash; this makes much clearer what really works and what doesnâ€™t.&lt;/p>
&lt;p>Here are some of my favorites from recent years:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://youtu.be/MAXJWEoKbxY?si=s9nHKBy8K2WWPvuN&amp;amp;t=822" target="_blank" rel="noopener">AI &amp;amp; Physics-Assisted Character Pose Authoring&lt;/a> (2022)&lt;/li>
&lt;li>&lt;a href="https://www.youtube.com/watch?v=lXZhgkNFGfM" target="_blank" rel="noopener">Bebylon&lt;/a> (2018)&lt;/li>
&lt;li>IQ livecoding with Shadertoy (I canâ€™t find the video on YouTube; guess it was before they started uploading RTL to YouTube)&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>How did our demo come about?&lt;/strong> In December 2023, I contacted Matt Swoboda about submitting something to RTL. We were playing with this idea over the last decade, but it never materialized, until this year! Matt quickly brought in a great artist, while I could secure some sponsorship and support from Canon Japan, which led to this amazing constellation:&lt;/p>
&lt;ul>
&lt;li>Best real-time rendering engine for live events: &lt;a href="https://www.notch.one" target="_blank" rel="noopener">Notch&lt;/a> (Matt is their Co-Founder &amp;amp; CTO)&lt;/li>
&lt;li>Fantastic audio-visual artist: &lt;a href="https://www.brettbolton.net" target="_blank" rel="noopener">Brett Bolton&lt;/a> (Designer of U2â€™s show in the Las Vegas sphere, VJ for the Grateful Dead, etc.)&lt;/li>
&lt;li>The most stressable AR demo crew: &lt;a href="https://ar-ai.org/author/david-maruscak/" target="_blank" rel="noopener">David Maruscsak&lt;/a> and me ðŸ˜Š&lt;/li>
&lt;li>Best AR headset: &lt;a href="https://global.canon/en/technology/mr2019.html" target="_blank" rel="noopener">Canonâ€™s X1&lt;/a> (Sorry Apple, but you need to work much harder! Even though X1 was released 4 years before your Vision Pro, it has a much better image quality, better co-axial alignment of screens and cameras, better tracking infrastructure, easier integration with custom software, much better weight, etc; let me not even get started about Metaâ€™s headsetsâ€¦)&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>What is the concept of our demo?&lt;/strong> You can watch a concise summary of our
concept in the submission video below that we sent to SIGGRAPH. The main feedback that we received from the organizers was that we should expand our demo to have 2 viewers instead of 1 in order to highlight the potential to have visuals interact with multiple viewers in 3D. Challenge accepted!&lt;/p>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/uCb42SsFLk8" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;br>
&lt;p>&lt;strong>What could the audience see?&lt;/strong> Unfortunately, there were several issues with the YouTube stream of the event, so it was much harder for the remote audience to understand what was going on. Let me start by explaining what the audience in the room could see.&lt;/p>
&lt;p>
&lt;figure id="figure-view-from-stage">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="View from stage" srcset="
/media/rtl/pano_hu13dbddae45a07af144be999962d7c505_9581461_85ae5dff0cc01d3e7b407b77f81e8534.webp 400w,
/media/rtl/pano_hu13dbddae45a07af144be999962d7c505_9581461_b503633b0ef4324c0f8a2dd0045d9306.webp 760w,
/media/rtl/pano_hu13dbddae45a07af144be999962d7c505_9581461_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://drsandor.net/media/rtl/pano_hu13dbddae45a07af144be999962d7c505_9581461_85ae5dff0cc01d3e7b407b77f81e8534.webp"
width="760"
height="194"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
View from stage
&lt;/figcaption>&lt;/figure>
&lt;figure id="figure-audience-view-of-stage">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Audience view of stage" srcset="
/media/rtl/stage_huc5c81e4ab00a316abb82eed7380d6a68_577662_c859b43e2dea225e971b0373b49b770b.webp 400w,
/media/rtl/stage_huc5c81e4ab00a316abb82eed7380d6a68_577662_5ff530d950c2bd588653e2976b777df1.webp 760w,
/media/rtl/stage_huc5c81e4ab00a316abb82eed7380d6a68_577662_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://drsandor.net/media/rtl/stage_huc5c81e4ab00a316abb82eed7380d6a68_577662_c859b43e2dea225e971b0373b49b770b.webp"
width="760"
height="570"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Audience view of stage
&lt;/figcaption>&lt;/figure>
&lt;figure id="figure-setup-details-on-the-right-stage-with-brett-playing-behind-him-traditional-audio-reactive-visuals-2-viewers-with-ar-headsets-in-front-of-the-stage-left-side-split-screen-showing-the-augmented-views-of-the-2-viewers">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Setup details. On the right: stage with Brett playing; behind him traditional audio-reactive visuals; 2 viewers with AR headsets in front of the stage. Left side: split screen, showing the augmented views of the 2 viewers." srcset="
/media/rtl/schema_hub0ff15292ea34398e4f86ba5f2aaa5bd_488378_f46bfc25bcc2119347d582a4176d1611.webp 400w,
/media/rtl/schema_hub0ff15292ea34398e4f86ba5f2aaa5bd_488378_91496829229e096b8f287e95293cca16.webp 760w,
/media/rtl/schema_hub0ff15292ea34398e4f86ba5f2aaa5bd_488378_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://drsandor.net/media/rtl/schema_hub0ff15292ea34398e4f86ba5f2aaa5bd_488378_f46bfc25bcc2119347d582a4176d1611.webp"
width="760"
height="428"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Setup details. On the right: stage with Brett playing; behind him traditional audio-reactive visuals; 2 viewers with AR headsets in front of the stage. Left side: split screen, showing the augmented views of the 2 viewers.
&lt;/figcaption>&lt;/figure>&lt;/p>
&lt;p>The setup shown in the above figure was not clear for viewers of the YouTube stream. It did not help that for the first half of our performance, the stream was only showing the view of 1 viewer.&lt;/p>
&lt;p>&lt;strong>The main challenge&lt;/strong> with our setup was controlling the lighting. Since the
sensors are quite sensitive to external lighting (as well as the AR content) we did our best to control this (including working with the Siggraph AV team), but even with our best efforts we had some glitches during the final performance. That said, we still got very nice compliments from the other participants and the audience.&lt;/p>
&lt;figure id="figure-final-rehearsal-note-the-colorful-background-behind-brett-compared-to-the-black-background-at-the-actual-performance">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Final rehearsal: note the colorful background behind Brett, compared to the black background at the actual performance" srcset="
/media/rtl/venue-rehearsal_hu0b7ba7870b9691ab7eeba5cdc97383b9_5224857_6a74a44848668db074c17e6d92c3be52.webp 400w,
/media/rtl/venue-rehearsal_hu0b7ba7870b9691ab7eeba5cdc97383b9_5224857_bac4666bfaa2864a45bfe5b5731116c3.webp 760w,
/media/rtl/venue-rehearsal_hu0b7ba7870b9691ab7eeba5cdc97383b9_5224857_1200x1200_fit_q75_h2_lanczos_3.webp 1200w"
src="https://drsandor.net/media/rtl/venue-rehearsal_hu0b7ba7870b9691ab7eeba5cdc97383b9_5224857_6a74a44848668db074c17e6d92c3be52.webp"
width="760"
height="384"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Final rehearsal: note the colorful background behind Brett, compared to the black background at the actual performance
&lt;/figcaption>&lt;/figure>
&lt;figure id="figure-final-rehearsal-the-colorful-bright-background-is-very-nice-because-it-illuminates-brett-2-views-on-left-and-shows-off-refractions-of-virtual-water-2-views-on-right--in-the-actual-performance-we-had-to-switch-back-to-black-because-of-tracking-instabilities">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Final rehearsal: the colorful bright background is very nice, because: it illuminates Brett (2 views on left) and shows off refractions of virtual water (2 views on right) . In the actual performance, we had to switch back to black because of tracking instabilities" srcset="
/media/rtl/rehearsal-details_hu6732b02741823ee3381c983d5d4ece3c_175293_f64b37e0f270f1ddb60658f560e03309.webp 400w,
/media/rtl/rehearsal-details_hu6732b02741823ee3381c983d5d4ece3c_175293_2807ec2913c180a051d71c5c32f712bb.webp 760w,
/media/rtl/rehearsal-details_hu6732b02741823ee3381c983d5d4ece3c_175293_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://drsandor.net/media/rtl/rehearsal-details_hu6732b02741823ee3381c983d5d4ece3c_175293_f64b37e0f270f1ddb60658f560e03309.webp"
width="760"
height="283"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Final rehearsal: the colorful bright background is very nice, because: it illuminates Brett (2 views on left) and shows off refractions of virtual water (2 views on right) . In the actual performance, we had to switch back to black because of tracking instabilities
&lt;/figcaption>&lt;/figure>
&lt;p>&lt;strong>Showtime!&lt;/strong> After months of hard work, we were finally ready to demo! The hall filling with the audience was an awe-inspiring moment for me (I estimate about 3500 attendants).&lt;/p>
&lt;video width="720" autoplay loop>
&lt;source src="showtimelapse.mp4" type="video/mp4" />
&lt;/video>
&lt;figure id="figure-selfie-just-before-the-start-of-the-show">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Selfie just before the start of the show" srcset="
/media/rtl/showtime_hu8400259ed9cc12074f04526ca19f5c86_2971141_285ff3afdc452cdaf1cdcfacb7ab255c.webp 400w,
/media/rtl/showtime_hu8400259ed9cc12074f04526ca19f5c86_2971141_4f4d81e45566148609b848161d9de68e.webp 760w,
/media/rtl/showtime_hu8400259ed9cc12074f04526ca19f5c86_2971141_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://drsandor.net/media/rtl/showtime_hu8400259ed9cc12074f04526ca19f5c86_2971141_285ff3afdc452cdaf1cdcfacb7ab255c.webp"
width="760"
height="570"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Selfie just before the start of the show
&lt;/figcaption>&lt;/figure>
&lt;p>Luckily we were the first team to present, so we could relax afterwards and enjoy the show, which contained some truly remarkable demos, including:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://www.youtube.com/live/Gm1B5DT8kE0?si=bXvehrC7aaXZARj_&amp;amp;t=429" target="_blank" rel="noopener">The controller for Jim Henson&amp;rsquo;s muppets&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.youtube.com/live/Gm1B5DT8kE0?si=oKXaQpHSPQ6l-OB5&amp;amp;t=5459" target="_blank" rel="noopener">Movin Tracin&amp;rsquo;&lt;/a>: my personal favorite. I was surprised that this demo did not get any award.&lt;/li>
&lt;li>&lt;a href="https://www.youtube.com/live/Gm1B5DT8kE0?si=Xm3XuXkt52NV09Mr&amp;amp;t=2231" target="_blank" rel="noopener">Mesh Mortal Combat&lt;/a>: winner of both awards (audience &amp;amp; jury).&lt;/li>
&lt;/ul>
&lt;p>You can also just watch &lt;a href="https://www.youtube.com/live/Gm1B5DT8kE0?si=Xm3XuXkt52NV09Mr" target="_blank" rel="noopener">the whole stream&lt;/a>, or jump directly to &lt;a href="https://www.youtube.com/live/Gm1B5DT8kE0?si=MLAwyoSwGnKb75dI&amp;amp;t=1215" target="_blank" rel="noopener">our part&lt;/a>.&lt;/p>
&lt;p>&lt;strong>Conclusions&lt;/strong> It was a great experience to present at Real-Time Live! I hope to return in coming years!&lt;/p>
&lt;p>What we learned from this work is that there is a huge potential for having visuals at live events in AR headsets; we got encouraging feedback at the conference!&lt;/p>
&lt;p>We also learned that it is very challenging, both in terms of technology, as well as in terms of having a viable business model. But, we have ideas for both. Stay tuned!&lt;/p>
&lt;p>&lt;strong>Acknowledgements&lt;/strong>
First and foremost, I would like to thank Brett and Matt to participate in this project. They are exemplars of a rare breed: extremely skilled folks who put creativity before money.&lt;/p>
&lt;p>Second, I would like to thank Canon for advanced support and a financial contribution to our costs.&lt;/p>
&lt;p>Last, but not least, I would also like to thank the RTL organizers, who did a really great job of putting this event together! See you next year!&lt;/p>
&lt;!--
&lt;figure id="figure-the-team">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="The Team" srcset="
/media/rtl/team_huc83d12a88c14e6bb7d48ea036a0b40a4_618767_e94a7d77b8229d8730374d15bb4843d8.webp 400w,
/media/rtl/team_huc83d12a88c14e6bb7d48ea036a0b40a4_618767_18ac242625abffd0033eee0a0f8b21a8.webp 760w,
/media/rtl/team_huc83d12a88c14e6bb7d48ea036a0b40a4_618767_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://drsandor.net/media/rtl/team_huc83d12a88c14e6bb7d48ea036a0b40a4_618767_e94a7d77b8229d8730374d15bb4843d8.webp"
width="760"
height="428"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
The Team
&lt;/figcaption>&lt;/figure>--></description></item></channel></rss>