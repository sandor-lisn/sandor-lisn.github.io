<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Projects | Christian Sandor</title><link>https://drsandor.net/project/</link><atom:link href="https://drsandor.net/project/index.xml" rel="self" type="application/rss+xml"/><description>Projects</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Sun, 01 Feb 2026 14:00:00 +0000</lastBuildDate><image><url>https://drsandor.net/media/icon_hu287fcf417612116bcf0d00ac1ba165d7_82622_512x512_fill_lanczos_center_3.png</url><title>Projects</title><link>https://drsandor.net/project/</link></image><item><title>Teaching AI at Elementary School</title><link>https://drsandor.net/project/ai-school/</link><pubDate>Sun, 01 Feb 2026 14:00:00 +0000</pubDate><guid>https://drsandor.net/project/ai-school/</guid><description>&lt;p>Test&lt;/p></description></item><item><title>Course: Generative AI with ComfyUI</title><link>https://drsandor.net/project/course/</link><pubDate>Tue, 18 Nov 2025 14:00:00 +0000</pubDate><guid>https://drsandor.net/project/course/</guid><description>&lt;blockquote class="wp-block-quote">
&lt;p>
The world of Generative AI is complex, and fast-moving. [...] There are so many models, and so many complexities, that this course was essential to cut through all the noise.
&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a href="https://www.linkedin.com/in/matt-swoboda-b820872/" target="_blank" rel="noopener">Matt Swoboda&lt;/a> - &lt;a href="https://www.notch.one" target="_blank" rel="noopener">Notch&lt;/a> - Founder and Lead Developer&lt;/p>
&lt;p>Over the last few years, we have seen a rapid and wide-spread uptake of generative AI in industry and academia. One tool has established itself as the de-facto standard for rapid experimentation, as well as professional production: &lt;a href="https://en.wikipedia.org/wiki/ComfyUI" target="_blank" rel="noopener">ComfyUI&lt;/a>. It is used in projects ranging from commercial productions like &lt;a href="https://en.wikipedia.org/wiki/The_Wizard_of_Oz_at_Sphere" target="_blank" rel="noopener">The Wizard of Oz at the Las Vegas Sphere&lt;/a> up to leading-edge research projects like &lt;a href="https://github.com/bytedance/ComfyUI-HyperLoRA" target="_blank" rel="noopener">Bytedance&amp;rsquo;s HyperLORA&lt;/a>.&lt;/p>
&lt;p>Based on my personal experience with using ComfyUI almost daily from day 1, I have developed a course with unique features:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Only uses open-weights models&lt;/strong> that can be run on your own machine; no cloud services are used.&lt;/li>
&lt;li>&lt;strong>From the ground up:&lt;/strong> after 1.5 hours into this course, you will understand the underlying concept of every single item in the simplest ComfyUI workflow, including: Flow Matching, Variational Autoencoders, CLIP, and CFG.&lt;/li>
&lt;li>&lt;strong>Dozens of hand-crafted workflows&lt;/strong> as starting point for your own explorations.&lt;/li>
&lt;li>&lt;strong>Covers essential system administration tasks&lt;/strong> to allow continuous upgrading and also rollbacks of your ComfyUI installation.&lt;/li>
&lt;/ul>
&lt;!--
&lt;figure id="figure-medley-of-course-content-all-generated-live-during-the-course-please-click-to-enlarge">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Medley of course content, all generated live during the course. Please click to enlarge." srcset="
/project/course/featured_hu023d3431561f09a34c8ed9baf950ffc3_2575599_e83663696f988efb8bf5bc268c2c1a66.webp 400w,
/project/course/featured_hu023d3431561f09a34c8ed9baf950ffc3_2575599_8242d9c57570775911f5b104c654c042.webp 760w,
/project/course/featured_hu023d3431561f09a34c8ed9baf950ffc3_2575599_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://drsandor.net/project/course/featured_hu023d3431561f09a34c8ed9baf950ffc3_2575599_e83663696f988efb8bf5bc268c2c1a66.webp"
width="760"
height="364"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Medley of course content, all generated live during the course. Please click to enlarge.
&lt;/figcaption>&lt;/figure>
&lt;figure id="figure-medley-of-course-content-all-generated-live-during-the-course-please-click-to-enlarge">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="quilt.gif" alt="Medley of course content, all generated live during the course. Please click to enlarge." loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Medley of course content, all generated live during the course. Please click to enlarge.
&lt;/figcaption>&lt;/figure>
-->
&lt;p>Below, a video collage of some of the course content, all generated live during the course.&lt;/p>
&lt;video width="720" controls loop>
&lt;source src="quilt.m4v" type="video/mp4" />
&lt;/video>
&lt;blockquote class="wp-block-quote">
&lt;p>
Participants don‚Äôt just reproduce results: they understand the role of each block, how parameters influence outcomes, and how to adapt the system to their own creative goals.
&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a href="https://bammey.com" target="_blank" rel="noopener">Quentin Bammey&lt;/a> - &lt;a href="https://en.wikipedia.org/wiki/T%c3%a9l%c3%a9com_Paris" target="_blank" rel="noopener">T√©l√©com Paris&lt;/a> - Assistant Professor (Machine Learning)&lt;/p>
&lt;p>The course contains theoretical presentations, demonstrations, and hands-on exercises. The main modules are:&lt;/p>
&lt;ol>
&lt;li>Course Introduction&lt;/li>
&lt;li>Image Generation&lt;/li>
&lt;li>Video Generation&lt;/li>
&lt;li>Complex Workflows; e.g.
&lt;ul>
&lt;li>Spatio-temporal upscaling of videos (e.g. 16fps@720p =&amp;gt; 60fps@4k)&lt;/li>
&lt;li>Powerful workflows combining vision language models, large language models, and video generation models&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ol>
&lt;p>The full course takes 2 days but can be presented as a distilled version in 1 day. Typical class size is 8, but can be scaled from 1-20. The content will be adapted to the client&amp;rsquo;s needs, for example:&lt;/p>
&lt;ul>
&lt;li>Technical vs. design focus&lt;/li>
&lt;li>Audience size&lt;/li>
&lt;li>Available hardware for students&lt;/li>
&lt;/ul>
&lt;p>&lt;em>Please &lt;a href="mailto:christian@sandor.com">email me&lt;/a> your requirements to receive a &lt;strong>custom quote&lt;/strong>.&lt;/em>&lt;/p>
&lt;h2 id="testimonials">Testimonials&lt;/h2>
&lt;blockquote class="wp-block-quote">
&lt;p>
The world of Generative AI is complex, and fast-moving. Dr Sandor's excellent course demystified it for us; first with a solid technical explanation of the concepts, and then a guided exploration of the key workflows and models for image and video generation, via ComfyUI. Hands-on learning and demonstration, plus the excellent course materials, got us to the point of being able to do things for ourselves confidently and quickly. There are so many models, and so many complexities, that this course was essential to cut through all the noise.
&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a href="https://www.linkedin.com/in/matt-swoboda-b820872/" target="_blank" rel="noopener">Matt Swoboda&lt;/a> - &lt;a href="https://www.notch.one" target="_blank" rel="noopener">Notch&lt;/a> - Founder and Lead Developer&lt;/p>
&lt;blockquote class="wp-block-quote">
&lt;p>
Christian Sandor‚Äôs ComfyUI tutorial offers a remarkably clear introduction to modern image and video generation. He presents the core ideas behind these models in a way that is intuitive and immediately connected to practice. Rather than letting ComfyUI remain a collection of opaque nodes, he shows how each step of the workflow reflects a meaningful operation in the underlying process, and why the interface is structured the way it is. This gives participants a solid conceptual footing before they even start experimenting.
&lt;/p>
&lt;p>
As the session unfolds, everyone works with templates while also learning how to read, modify, and build workflows with intention. Participants don‚Äôt just reproduce results: they understand the role of each block, how parameters influence outcomes, and how to adapt the system to their own creative goals. It is a course that makes complex ideas accessible without simplifying them, and that leaves participants both confident and genuinely capable. I highly recommend Dr. Sandor's course to anyone who wants to use ComfyUI thoughtfully and with real understanding.
&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a href="https://bammey.com" target="_blank" rel="noopener">Quentin Bammey&lt;/a> - &lt;a href="https://en.wikipedia.org/wiki/T%c3%a9l%c3%a9com_Paris" target="_blank" rel="noopener">T√©l√©com Paris&lt;/a> - Assistant Professor (Machine Learning)&lt;/p>
&lt;blockquote class="wp-block-quote">
&lt;p>
Before the course, I had no real experience working with ComfyUI. I had seen it in action and knew how powerful it is. I wanted to learn its image and video generation workflows, but the amount of information and moving pieces involved can be quite overwhelming at first. Every time I tried to start my own projects, I got lost in the complexity of setting up the tool and finding the right workflow for my ideas, and I never managed to take the time to truly dig in and learn it thoroughly. Fortunately, I had the opportunity to take the course, and it really made all the difference.
&lt;/p>
&lt;p>
Dr Sandor‚Äôs excellent instruction, combined with his insight and the course‚Äôs high-quality materials, provided me with extensive hands-on knowledge ‚Äî covering everything from the initial tool setup and the theory behind the workflows to the creation of complex workflows that I now use confidently in my own projects. The course not only gave me a solid understanding of ComfyUI, but also the confidence to explore and build advanced image and video generation pipelines independently. The course materials go even further, providing me with excellent reference points that I can return to whenever needed.
&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a href="https://www.linkedin.com/in/nikoryytty/" target="_blank" rel="noopener">Niko Ryytty&lt;/a> - &lt;a href="https://www.notch.one" target="_blank" rel="noopener">Notch&lt;/a> - Architect &amp;amp; Web Technologies Lead&lt;/p></description></item><item><title>Keynote at ICXR</title><link>https://drsandor.net/project/icxr/</link><pubDate>Fri, 07 Nov 2025 14:00:00 +0000</pubDate><guid>https://drsandor.net/project/icxr/</guid><description>&lt;p>Every time I visit China, I am absolutely astonished by their pace of progress. It is simply not possible to grasp this from the outside, as lots of information is not very accessible from abroad. In this post, I would like to share some of the things I learned during my visit.&lt;/p>
&lt;p>I presented a keynote at the new and upcoming &lt;a href="https://icxr.net/2025/index.html" target="_blank" rel="noopener">International Conference on Extended Reality&lt;/a> (ICXR); even though it was only the second time that this conference took place, it was well attended. It was co-located with &lt;a href="https://chinavr2025.qdvri.com" target="_blank" rel="noopener">ChinaVR&lt;/a>, the biggest VR conference of China (running since 2001).&lt;/p>
&lt;p>It is important to understand how Chinese research is organized. The Chinese research community elects &lt;a href="https://en.wikipedia.org/wiki/Academician_of_the_Chinese_Academy_of_Sciences" target="_blank" rel="noopener">Academicians (‰∏≠ÂõΩÁßëÂ≠¶Èô¢Èô¢Â£´)&lt;/a> for all important research areas. Then, the Academician has strong influence on the policy, research, and commercial developments in China. I don&amp;rsquo;t think neither the US nor Europe have any equivalent of this. 3 Academicians attended the event:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://cg.cs.tsinghua.edu.cn/shimin.htm" target="_blank" rel="noopener">Shi-min Hu&lt;/a>: Computer Graphics&lt;/li>
&lt;li>&lt;a href="https://baike.baidu.com/item/%e8%b5%b5%e6%b2%81%e5%b9%b3/7417905" target="_blank" rel="noopener">Qinping Zhao&lt;/a>: Virtual Reality&lt;/li>
&lt;li>&lt;a href="https://faculty.bjtu.edu.cn/eaie/6358.html" target="_blank" rel="noopener">Hongke Zhang&lt;/a>: Networking&lt;/li>
&lt;/ul>
&lt;p>There were also 2 more researchers of that caliber, who I guess will become Academicians sooner or later:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://baoquanchen.info" target="_blank" rel="noopener">Baoquan Chen&lt;/a>: Computer Vision&lt;/li>
&lt;li>&lt;a href="https://fve.bfa.edu.cn/info/1056/2054.htm" target="_blank" rel="noopener">Yongtian Wang&lt;/a>: Optics for AR/VR&lt;/li>
&lt;/ul>
&lt;p>Several companies showed exhibits, including &lt;strong>Goertek&lt;/strong>. To be honest, I have never heard of this company before! But, they are producing &lt;strong>80% of the world&amp;rsquo;s AR and VR glasses&lt;/strong>! Could you have guessed that based on their &lt;a href="https://en.wikipedia.org/wiki/Goertek" target="_blank" rel="noopener">Wikipedia page&lt;/a>? Unfortunately, photos were strictly prohibited at their large booth. I could clearly see that they are not simply a factory for US companies, but very innovative and contributing key technological developments, including:&lt;/p>
&lt;ul>
&lt;li>High pixel density display panels (Micro OLED)&lt;/li>
&lt;li>Miniaturized water cooling for AR/VR glasses (the water-cooled board they showcased was about 2x4 cm big; water pipes around 2mm diameter)&lt;/li>
&lt;li>Advanced pancake optics&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Hongke Zhang&lt;/strong> presented a thought-provoking keynote along the lines of: What network infrastructure do we need to support the rapidly changing network requirements in an AI world? It included many science-fiction level concepts such as:&lt;/p>
&lt;ul>
&lt;li>A longterm roadmap including 6G and beyond with mobile speeds of Terabits per second&lt;/li>
&lt;li>Mesh-networking with drone swarms&lt;/li>
&lt;li>Streaming neural content (NERFs, Gaussian Splats, etc)&lt;/li>
&lt;/ul>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="" srcset="
/project/icxr/zhang1_hu307ffe06fa0f0a670173d11c2083b3d1_607338_8a43a8be846f13d72cc33c24cba4b58f.webp 400w,
/project/icxr/zhang1_hu307ffe06fa0f0a670173d11c2083b3d1_607338_36d5485571fe887cb67e75c802aaa646.webp 760w,
/project/icxr/zhang1_hu307ffe06fa0f0a670173d11c2083b3d1_607338_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://drsandor.net/project/icxr/zhang1_hu307ffe06fa0f0a670173d11c2083b3d1_607338_8a43a8be846f13d72cc33c24cba4b58f.webp"
width="760"
height="570"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="" srcset="
/project/icxr/zhang2_hu4a70eccacf39bf605e2091d4dd40c932_633050_89802289d51d9d197134dc144cfc2eea.webp 400w,
/project/icxr/zhang2_hu4a70eccacf39bf605e2091d4dd40c932_633050_a685ee0f8e782cd067056cc30638a894.webp 760w,
/project/icxr/zhang2_hu4a70eccacf39bf605e2091d4dd40c932_633050_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://drsandor.net/project/icxr/zhang2_hu4a70eccacf39bf605e2091d4dd40c932_633050_89802289d51d9d197134dc144cfc2eea.webp"
width="760"
height="570"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="" srcset="
/project/icxr/zhang3_hue07d29e6f16d0edda3870e3fa88a1557_629518_e2d2058e8bb3f6c80b97f3304a6a2304.webp 400w,
/project/icxr/zhang3_hue07d29e6f16d0edda3870e3fa88a1557_629518_741f1728e9355540cb4f0051b7cacf1b.webp 760w,
/project/icxr/zhang3_hue07d29e6f16d0edda3870e3fa88a1557_629518_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://drsandor.net/project/icxr/zhang3_hue07d29e6f16d0edda3870e3fa88a1557_629518_e2d2058e8bb3f6c80b97f3304a6a2304.webp"
width="760"
height="570"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;p>Next up was &lt;strong>Baoquan Chen&lt;/strong>. A major thread of his talk was along the lines of (please excuse my informal language):&lt;/p>
&lt;ul>
&lt;li>AI Video generators along the lines of SORA and VEO suck, because they lack understanding of the physical world. They are essentially a dead end.&lt;/li>
&lt;li>Gaussian Splatting is very popular now, but it is by itself also a dead end, because of the inherent inflexibility and non-interactiveness.&lt;/li>
&lt;/ul>
&lt;p>I already came to similar conclusions, albeit for different reasons :-) He then presented several recent works by his group how to overcome these limitations. In short: real instead of hallucinated physics are needed!&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="" srcset="
/project/icxr/chen1_hufe082b017d9719ead8de3b0b5e59b3d3_544830_9982a9761f77d7bd7df3367926c787fd.webp 400w,
/project/icxr/chen1_hufe082b017d9719ead8de3b0b5e59b3d3_544830_e808e18baa805a907cf8a7ec56af595f.webp 760w,
/project/icxr/chen1_hufe082b017d9719ead8de3b0b5e59b3d3_544830_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://drsandor.net/project/icxr/chen1_hufe082b017d9719ead8de3b0b5e59b3d3_544830_9982a9761f77d7bd7df3367926c787fd.webp"
width="760"
height="570"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="" srcset="
/project/icxr/chen2_huadedc5b8b6f11091bcbfafbc1c1ed320_505349_540916450eb775080b984f1bce2a6c3e.webp 400w,
/project/icxr/chen2_huadedc5b8b6f11091bcbfafbc1c1ed320_505349_5ceeb53afb3fb36c5710848b674afd77.webp 760w,
/project/icxr/chen2_huadedc5b8b6f11091bcbfafbc1c1ed320_505349_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://drsandor.net/project/icxr/chen2_huadedc5b8b6f11091bcbfafbc1c1ed320_505349_540916450eb775080b984f1bce2a6c3e.webp"
width="760"
height="570"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="" srcset="
/project/icxr/chen3_hu2784ae0ef6911b4defd144ef4bc2371f_796921_a1c070b5b5ff17cf4fc092428a798286.webp 400w,
/project/icxr/chen3_hu2784ae0ef6911b4defd144ef4bc2371f_796921_511db7516ebfbf70775aad112a26ee4f.webp 760w,
/project/icxr/chen3_hu2784ae0ef6911b4defd144ef4bc2371f_796921_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://drsandor.net/project/icxr/chen3_hu2784ae0ef6911b4defd144ef4bc2371f_796921_a1c070b5b5ff17cf4fc092428a798286.webp"
width="760"
height="570"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;p>I was extremely glad that &lt;strong>Shi-Min Hu&lt;/strong> chiseled out 20 minutes out of his busy schedule to talk to me. Our chat was very intense&amp;hellip; I now understand better China&amp;rsquo;s strategy how to deal with the import restrictions of NVIDIA GPUs. I mean, at the moment, Chinese AI models are already on the same level as their US counterparts (I personally think that they are actually already better, but this is debatable). They achieved this even though their current GPUs are much weaker than the ones Meta, OpenAI etc. are using. If they get comparable compute power as the US than their AI models will run circles around their US counterparts. It will happen much sooner than the public thinks&amp;hellip; I can&amp;rsquo;t wait to switch to Chinese GPUs!&lt;/p>
&lt;p>It was a lucky accident that I sat next to &lt;strong>Yongtian Wang&lt;/strong> at the closing dinner. He shared some incredible stories about the custom optics design software he wrote back in the day (in Fortran!): GOLD = &lt;strong>G&lt;/strong>eneral &lt;strong>O&lt;/strong>ptical &lt;strong>L&lt;/strong>ens &lt;strong>D&lt;/strong>esign system :-) As their group can&amp;rsquo;t use the world-wide standard software (Code V; another US import restriction), they have been recently reviving this decade-old software.&lt;/p>
&lt;p>Finally, I should also spend some words on my keynote. My recent discussions with Notch&amp;rsquo;s Matt Swoboda (the grand wizard of realtime computer graphics) inspired many of the open questions that I discussed in my talk.&lt;/p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="" srcset="
/project/icxr/questions_hu5636f1c492ebb8d576e1c13f2147d42f_394607_f55721850aaf7cf860e43549482454be.webp 400w,
/project/icxr/questions_hu5636f1c492ebb8d576e1c13f2147d42f_394607_76bc53af824a7556e83d173b721936b1.webp 760w,
/project/icxr/questions_hu5636f1c492ebb8d576e1c13f2147d42f_394607_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://drsandor.net/project/icxr/questions_hu5636f1c492ebb8d576e1c13f2147d42f_394607_f55721850aaf7cf860e43549482454be.webp"
width="760"
height="428"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;p>To conclude, a little video from the soundcheck of my presentation. The sound system was really good!&lt;/p>
&lt;video width="1920" controls loop>
&lt;source src="soundcheck.mp4" type="video/mp4" />
&lt;/video></description></item><item><title>Short Animation with Wan Video, Flux Kontext, and DeepSeek</title><link>https://drsandor.net/project/ai-minecraft/</link><pubDate>Mon, 23 Jun 2025 14:00:00 +0000</pubDate><guid>https://drsandor.net/project/ai-minecraft/</guid><description>&lt;p>Test&lt;/p></description></item><item><title>Keynote at ADOS Paris</title><link>https://drsandor.net/project/ai3/</link><pubDate>Thu, 10 Apr 2025 14:00:00 +0000</pubDate><guid>https://drsandor.net/project/ai3/</guid><description>&lt;p>Test&lt;/p></description></item><item><title>AI Video Generation - The Future is already here</title><link>https://drsandor.net/project/ai2/</link><pubDate>Wed, 18 Dec 2024 14:00:00 +0000</pubDate><guid>https://drsandor.net/project/ai2/</guid><description/></item><item><title>AI Experiments</title><link>https://drsandor.net/project/ai/</link><pubDate>Sun, 29 Sep 2024 14:00:00 +0000</pubDate><guid>https://drsandor.net/project/ai/</guid><description/></item><item><title>Demo at ACM SIGGRAPH Real-Time Live!</title><link>https://drsandor.net/project/rtl/</link><pubDate>Wed, 04 Sep 2024 14:00:00 +0000</pubDate><guid>https://drsandor.net/project/rtl/</guid><description>&lt;p>A long-term dream of mine came true! At the end of July 2024, we showed a demonstration at ACM SIGGRAPH 2024 in Denver for their Real-Time Live! category.&lt;/p>
&lt;p>First of all, &lt;strong>what is ACM SIGRAPH Real-Time Live?&lt;/strong> It is a specific category at the premiere computer graphics conference &lt;a href="https://en.wikipedia.org/wiki/SIGGRAPH" target="_blank" rel="noopener">ACM SIGGRAPH&lt;/a>. While SIGGRAPH has been running every year since 1974, the Real-Time Live! category (RTL for short) has only been around for 15 years. It is very engaging, as presenters have 6 minutes to show a demonstration of real-time graphics to a large audience of computer graphics professionals (~5000 people). For me, this has always been the most fascinating event of SIGGRAPH, as it shows you the maximum of what‚Äôs possible in real-time graphics today. Also, compared to research papers, you can actually see the inventors demonstrating what they made&amp;mdash; this makes much clearer what really works and what doesn‚Äôt.&lt;/p>
&lt;p>Here are some of my favorites from recent years:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://youtu.be/MAXJWEoKbxY?si=s9nHKBy8K2WWPvuN&amp;amp;t=822" target="_blank" rel="noopener">AI &amp;amp; Physics-Assisted Character Pose Authoring&lt;/a> (2022)&lt;/li>
&lt;li>&lt;a href="https://www.youtube.com/watch?v=lXZhgkNFGfM" target="_blank" rel="noopener">Bebylon&lt;/a> (2018)&lt;/li>
&lt;li>IQ livecoding with Shadertoy (I can‚Äôt find the video on YouTube; guess it was before they started uploading RTL to YouTube)&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>How did our demo come about?&lt;/strong> In December 2023, I contacted Matt Swoboda about submitting something to RTL. We were playing with this idea over the last decade, but it never materialized, until this year! Matt quickly brought in a great artist, while I could secure some sponsorship and support from Canon Japan, which led to this amazing constellation:&lt;/p>
&lt;ul>
&lt;li>Best real-time rendering engine for live events: &lt;a href="https://www.notch.one" target="_blank" rel="noopener">Notch&lt;/a> (Matt is their Co-Founder &amp;amp; CTO)&lt;/li>
&lt;li>Fantastic audio-visual artist: &lt;a href="https://www.brettbolton.net" target="_blank" rel="noopener">Brett Bolton&lt;/a> (Designer of U2‚Äôs show in the Las Vegas sphere, VJ for the Grateful Dead, etc.)&lt;/li>
&lt;li>The most stressable AR demo crew: &lt;a href="https://ar-ai.org/author/david-maruscak/" target="_blank" rel="noopener">David Maruscsak&lt;/a> and me üòä&lt;/li>
&lt;li>Best AR headset: &lt;a href="https://global.canon/en/technology/mr2019.html" target="_blank" rel="noopener">Canon‚Äôs X1&lt;/a> (Sorry Apple, but you need to work much harder! Even though X1 was released 4 years before your Vision Pro, it has a much better image quality, better co-axial alignment of screens and cameras, better tracking infrastructure, easier integration with custom software, much better weight, etc; let me not even get started about Meta‚Äôs headsets‚Ä¶)&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>What is the concept of our demo?&lt;/strong> You can watch a concise summary of our
concept in the submission video below that we sent to SIGGRAPH. The main feedback that we received from the organizers was that we should expand our demo to have 2 viewers instead of 1 in order to highlight the potential to have visuals interact with multiple viewers in 3D. Challenge accepted!&lt;/p>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/uCb42SsFLk8" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;br>
&lt;p>&lt;strong>What could the audience see?&lt;/strong> Unfortunately, there were several issues with the YouTube stream of the event, so it was much harder for the remote audience to understand what was going on. Let me start by explaining what the audience in the room could see.&lt;/p>
&lt;p>
&lt;figure id="figure-view-from-stage">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="View from stage" srcset="
/media/rtl/pano_hu13dbddae45a07af144be999962d7c505_9581461_85ae5dff0cc01d3e7b407b77f81e8534.webp 400w,
/media/rtl/pano_hu13dbddae45a07af144be999962d7c505_9581461_b503633b0ef4324c0f8a2dd0045d9306.webp 760w,
/media/rtl/pano_hu13dbddae45a07af144be999962d7c505_9581461_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://drsandor.net/media/rtl/pano_hu13dbddae45a07af144be999962d7c505_9581461_85ae5dff0cc01d3e7b407b77f81e8534.webp"
width="760"
height="194"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
View from stage
&lt;/figcaption>&lt;/figure>
&lt;figure id="figure-audience-view-of-stage">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Audience view of stage" srcset="
/media/rtl/stage_huc5c81e4ab00a316abb82eed7380d6a68_577662_c859b43e2dea225e971b0373b49b770b.webp 400w,
/media/rtl/stage_huc5c81e4ab00a316abb82eed7380d6a68_577662_5ff530d950c2bd588653e2976b777df1.webp 760w,
/media/rtl/stage_huc5c81e4ab00a316abb82eed7380d6a68_577662_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://drsandor.net/media/rtl/stage_huc5c81e4ab00a316abb82eed7380d6a68_577662_c859b43e2dea225e971b0373b49b770b.webp"
width="760"
height="570"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Audience view of stage
&lt;/figcaption>&lt;/figure>
&lt;figure id="figure-setup-details-on-the-right-stage-with-brett-playing-behind-him-traditional-audio-reactive-visuals-2-viewers-with-ar-headsets-in-front-of-the-stage-left-side-split-screen-showing-the-augmented-views-of-the-2-viewers">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Setup details. On the right: stage with Brett playing; behind him traditional audio-reactive visuals; 2 viewers with AR headsets in front of the stage. Left side: split screen, showing the augmented views of the 2 viewers." srcset="
/media/rtl/schema_hub0ff15292ea34398e4f86ba5f2aaa5bd_488378_f46bfc25bcc2119347d582a4176d1611.webp 400w,
/media/rtl/schema_hub0ff15292ea34398e4f86ba5f2aaa5bd_488378_91496829229e096b8f287e95293cca16.webp 760w,
/media/rtl/schema_hub0ff15292ea34398e4f86ba5f2aaa5bd_488378_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://drsandor.net/media/rtl/schema_hub0ff15292ea34398e4f86ba5f2aaa5bd_488378_f46bfc25bcc2119347d582a4176d1611.webp"
width="760"
height="428"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Setup details. On the right: stage with Brett playing; behind him traditional audio-reactive visuals; 2 viewers with AR headsets in front of the stage. Left side: split screen, showing the augmented views of the 2 viewers.
&lt;/figcaption>&lt;/figure>&lt;/p>
&lt;p>The setup shown in the above figure was not clear for viewers of the YouTube stream. It did not help that for the first half of our performance, the stream was only showing the view of 1 viewer.&lt;/p>
&lt;p>&lt;strong>The main challenge&lt;/strong> with our setup was controlling the lighting. Since the
sensors are quite sensitive to external lighting (as well as the AR content) we did our best to control this (including working with the Siggraph AV team), but even with our best efforts we had some glitches during the final performance. That said, we still got very nice compliments from the other participants and the audience.&lt;/p>
&lt;figure id="figure-final-rehearsal-note-the-colorful-background-behind-brett-compared-to-the-black-background-at-the-actual-performance">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Final rehearsal: note the colorful background behind Brett, compared to the black background at the actual performance" srcset="
/media/rtl/venue-rehearsal_hu0b7ba7870b9691ab7eeba5cdc97383b9_5224857_6a74a44848668db074c17e6d92c3be52.webp 400w,
/media/rtl/venue-rehearsal_hu0b7ba7870b9691ab7eeba5cdc97383b9_5224857_bac4666bfaa2864a45bfe5b5731116c3.webp 760w,
/media/rtl/venue-rehearsal_hu0b7ba7870b9691ab7eeba5cdc97383b9_5224857_1200x1200_fit_q75_h2_lanczos_3.webp 1200w"
src="https://drsandor.net/media/rtl/venue-rehearsal_hu0b7ba7870b9691ab7eeba5cdc97383b9_5224857_6a74a44848668db074c17e6d92c3be52.webp"
width="760"
height="384"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Final rehearsal: note the colorful background behind Brett, compared to the black background at the actual performance
&lt;/figcaption>&lt;/figure>
&lt;figure id="figure-final-rehearsal-the-colorful-bright-background-is-very-nice-because-it-illuminates-brett-2-views-on-left-and-shows-off-refractions-of-virtual-water-2-views-on-right--in-the-actual-performance-we-had-to-switch-back-to-black-because-of-tracking-instabilities">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Final rehearsal: the colorful bright background is very nice, because: it illuminates Brett (2 views on left) and shows off refractions of virtual water (2 views on right) . In the actual performance, we had to switch back to black because of tracking instabilities" srcset="
/media/rtl/rehearsal-details_hu6732b02741823ee3381c983d5d4ece3c_175293_f64b37e0f270f1ddb60658f560e03309.webp 400w,
/media/rtl/rehearsal-details_hu6732b02741823ee3381c983d5d4ece3c_175293_2807ec2913c180a051d71c5c32f712bb.webp 760w,
/media/rtl/rehearsal-details_hu6732b02741823ee3381c983d5d4ece3c_175293_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://drsandor.net/media/rtl/rehearsal-details_hu6732b02741823ee3381c983d5d4ece3c_175293_f64b37e0f270f1ddb60658f560e03309.webp"
width="760"
height="283"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Final rehearsal: the colorful bright background is very nice, because: it illuminates Brett (2 views on left) and shows off refractions of virtual water (2 views on right) . In the actual performance, we had to switch back to black because of tracking instabilities
&lt;/figcaption>&lt;/figure>
&lt;p>&lt;strong>Showtime!&lt;/strong> After months of hard work, we were finally ready to demo! The hall filling with the audience was an awe-inspiring moment for me (I estimate about 3500 attendants).&lt;/p>
&lt;video width="720" autoplay loop>
&lt;source src="showtimelapse.mp4" type="video/mp4" />
&lt;/video>
&lt;figure id="figure-selfie-just-before-the-start-of-the-show">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Selfie just before the start of the show" srcset="
/media/rtl/showtime_hu8400259ed9cc12074f04526ca19f5c86_2971141_285ff3afdc452cdaf1cdcfacb7ab255c.webp 400w,
/media/rtl/showtime_hu8400259ed9cc12074f04526ca19f5c86_2971141_4f4d81e45566148609b848161d9de68e.webp 760w,
/media/rtl/showtime_hu8400259ed9cc12074f04526ca19f5c86_2971141_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://drsandor.net/media/rtl/showtime_hu8400259ed9cc12074f04526ca19f5c86_2971141_285ff3afdc452cdaf1cdcfacb7ab255c.webp"
width="760"
height="570"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Selfie just before the start of the show
&lt;/figcaption>&lt;/figure>
&lt;p>Luckily we were the first team to present, so we could relax afterwards and enjoy the show, which contained some truly remarkable demos, including:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://www.youtube.com/live/Gm1B5DT8kE0?si=bXvehrC7aaXZARj_&amp;amp;t=429" target="_blank" rel="noopener">The controller for Jim Henson&amp;rsquo;s muppets&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.youtube.com/live/Gm1B5DT8kE0?si=oKXaQpHSPQ6l-OB5&amp;amp;t=5459" target="_blank" rel="noopener">Movin Tracin&amp;rsquo;&lt;/a>: my personal favorite. I was surprised that this demo did not get any award.&lt;/li>
&lt;li>&lt;a href="https://www.youtube.com/live/Gm1B5DT8kE0?si=Xm3XuXkt52NV09Mr&amp;amp;t=2231" target="_blank" rel="noopener">Mesh Mortal Combat&lt;/a>: winner of both awards (audience &amp;amp; jury).&lt;/li>
&lt;/ul>
&lt;p>You can also just watch &lt;a href="https://www.youtube.com/live/Gm1B5DT8kE0?si=Xm3XuXkt52NV09Mr" target="_blank" rel="noopener">the whole stream&lt;/a>, or jump directly to &lt;a href="https://www.youtube.com/live/Gm1B5DT8kE0?si=MLAwyoSwGnKb75dI&amp;amp;t=1215" target="_blank" rel="noopener">our part&lt;/a>.&lt;/p>
&lt;p>&lt;strong>Conclusions&lt;/strong> It was a great experience to present at Real-Time Live! I hope to return in coming years!&lt;/p>
&lt;p>What we learned from this work is that there is a huge potential for having visuals at live events in AR headsets; we got encouraging feedback at the conference!&lt;/p>
&lt;p>We also learned that it is very challenging, both in terms of technology, as well as in terms of having a viable business model. But, we have ideas for both. Stay tuned!&lt;/p>
&lt;p>&lt;strong>Acknowledgements&lt;/strong>
First and foremost, I would like to thank Brett and Matt to participate in this project. They are exemplars of a rare breed: extremely skilled folks who put creativity before money.&lt;/p>
&lt;p>Second, I would like to thank Canon for advanced support and a financial contribution to our costs.&lt;/p>
&lt;p>Last, but not least, I would also like to thank the RTL organizers, who did a really great job of putting this event together! See you next year!&lt;/p>
&lt;!--
&lt;figure id="figure-the-team">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="The Team" srcset="
/media/rtl/team_huc83d12a88c14e6bb7d48ea036a0b40a4_618767_e94a7d77b8229d8730374d15bb4843d8.webp 400w,
/media/rtl/team_huc83d12a88c14e6bb7d48ea036a0b40a4_618767_18ac242625abffd0033eee0a0f8b21a8.webp 760w,
/media/rtl/team_huc83d12a88c14e6bb7d48ea036a0b40a4_618767_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://drsandor.net/media/rtl/team_huc83d12a88c14e6bb7d48ea036a0b40a4_618767_e94a7d77b8229d8730374d15bb4843d8.webp"
width="760"
height="428"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
The Team
&lt;/figcaption>&lt;/figure>--></description></item><item><title>Invited Talk at MLBriefs</title><link>https://drsandor.net/project/ipol/</link><pubDate>Fri, 31 May 2024 14:00:00 +0000</pubDate><guid>https://drsandor.net/project/ipol/</guid><description/></item><item><title>Best XR Demo at ACM SIGGRAPH Asia</title><link>https://drsandor.net/project/sigaxr/</link><pubDate>Fri, 15 Dec 2023 16:00:00 +0000</pubDate><guid>https://drsandor.net/project/sigaxr/</guid><description/></item><item><title>Invited Talk in Beijing</title><link>https://drsandor.net/project/bit/</link><pubDate>Sun, 30 Oct 2022 16:00:00 +0000</pubDate><guid>https://drsandor.net/project/bit/</guid><description/></item><item><title>Invited Talk at Big Techday</title><link>https://drsandor.net/project/btd/</link><pubDate>Sat, 17 Sep 2022 16:00:00 +0000</pubDate><guid>https://drsandor.net/project/btd/</guid><description/></item><item><title>Keynote at MediaFutures</title><link>https://drsandor.net/project/bergen/</link><pubDate>Sat, 17 Sep 2022 16:00:00 +0000</pubDate><guid>https://drsandor.net/project/bergen/</guid><description/></item><item><title>Keynote at XR Salento</title><link>https://drsandor.net/project/salento/</link><pubDate>Fri, 22 Apr 2022 14:00:00 +0000</pubDate><guid>https://drsandor.net/project/salento/</guid><description/></item><item><title>Talk at Coll√®ge de France</title><link>https://drsandor.net/project/cdf/</link><pubDate>Fri, 22 Apr 2022 14:00:00 +0000</pubDate><guid>https://drsandor.net/project/cdf/</guid><description>&lt;p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p>
&lt;p>Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p>
&lt;p>Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p>
&lt;p>Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p>
&lt;p>Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p></description></item></channel></rss>