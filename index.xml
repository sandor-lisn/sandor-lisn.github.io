<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Christian Sandor</title><link>https://drsandor.net/</link><atom:link href="https://drsandor.net/index.xml" rel="self" type="application/rss+xml"/><description>Christian Sandor</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Thu, 10 Apr 2025 14:00:00 +0000</lastBuildDate><image><url>https://drsandor.net/media/icon_hu287fcf417612116bcf0d00ac1ba165d7_82622_512x512_fill_lanczos_center_3.png</url><title>Christian Sandor</title><link>https://drsandor.net/</link></image><item><title>Academic Template</title><link>https://drsandor.net/home-unused/demo/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://drsandor.net/home-unused/demo/</guid><description>&lt;h2 id="-welcome-to-the-academic-template">ðŸ‘‹ Welcome to the Academic Template&lt;/h2>
&lt;p>The Wowchemy &lt;strong>Academic ResumÃ© Template&lt;/strong> for Hugo empowers you to create your job-winning online resumÃ© and showcase your academic publications.&lt;/p>
&lt;p>&lt;a href="https://academic-demo.netlify.app" target="_blank" rel="noopener">Check out the latest demo&lt;/a> of what you&amp;rsquo;ll get in less than 10 minutes, or &lt;a href="https://wowchemy.com/user-stories/" target="_blank" rel="noopener">view the showcase&lt;/a>.&lt;/p>
&lt;p>&lt;a href="https://wowchemy.com" target="_blank" rel="noopener">&lt;strong>Wowchemy&lt;/strong>&lt;/a> makes it easy to create a beautiful website for free. Edit your site in Markdown, Jupyter, or RStudio (via Blogdown), generate it with Hugo, and deploy with GitHub or Netlify. Customize anything on your site with widgets, themes, and language packs.&lt;/p>
&lt;ul>
&lt;li>ðŸ‘‰ &lt;a href="https://wowchemy.com/docs/install/" target="_blank" rel="noopener">&lt;strong>Get Started&lt;/strong>&lt;/a>&lt;/li>
&lt;li>ðŸ“š &lt;a href="https://wowchemy.com/docs/" target="_blank" rel="noopener">View the &lt;strong>documentation&lt;/strong>&lt;/a>&lt;/li>
&lt;li>ðŸ’¬ &lt;a href="https://discord.gg/z8wNYzb" target="_blank" rel="noopener">Chat with the &lt;strong>Wowchemy community&lt;/strong>&lt;/a> or &lt;a href="https://discourse.gohugo.io" target="_blank" rel="noopener">&lt;strong>Hugo community&lt;/strong>&lt;/a>&lt;/li>
&lt;li>ðŸ¦ Twitter: &lt;a href="https://twitter.com/wowchemy" target="_blank" rel="noopener">@wowchemy&lt;/a> &lt;a href="https://twitter.com/GeorgeCushen" target="_blank" rel="noopener">@GeorgeCushen&lt;/a> &lt;a href="https://twitter.com/search?q=%23MadeWithWowchemy&amp;amp;src=typed_query" target="_blank" rel="noopener">#MadeWithWowchemy&lt;/a>&lt;/li>
&lt;li>ðŸ’¡ &lt;a href="https://github.com/wowchemy/wowchemy-hugo-themes/issues" target="_blank" rel="noopener">Request a &lt;strong>feature&lt;/strong> or report a &lt;strong>bug&lt;/strong> for &lt;em>Wowchemy&lt;/em>&lt;/a>&lt;/li>
&lt;li>â¬†ï¸ &lt;strong>Updating Wowchemy?&lt;/strong> View the &lt;a href="https://wowchemy.com/docs/update/" target="_blank" rel="noopener">Update Guide&lt;/a> and &lt;a href="https://wowchemy.com/updates/" target="_blank" rel="noopener">Release Notes&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="crowd-funded-open-source-software">Crowd-funded open-source software&lt;/h2>
&lt;p>To help us develop this template and software sustainably under the MIT license, we ask all individuals and businesses that use it to help support its ongoing maintenance and development via sponsorship.&lt;/p>
&lt;h3 id="-click-here-to-unlock-rewards-with-sponsorshiphttpswowchemycomplans">&lt;a href="https://wowchemy.com/plans/" target="_blank" rel="noopener">â¤ï¸ Click here to unlock rewards with sponsorship&lt;/a>&lt;/h3>
&lt;h2 id="youre-looking-at-a-wowchemy-_widget_">You&amp;rsquo;re looking at a Wowchemy &lt;em>widget&lt;/em>&lt;/h2>
&lt;div class="alert alert-note">
&lt;div>
&lt;p>This homepage section is an example of adding &lt;a href="https://wowchemy.com/docs/content/writing-markdown-latex/" target="_blank" rel="noopener">elements&lt;/a> to the &lt;a href="https://wowchemy.com/docs/widget/" target="_blank" rel="noopener">&lt;em>Blank&lt;/em> widget&lt;/a>.&lt;/p>
&lt;p>Backgrounds can be applied to any section. Here, the &lt;em>background&lt;/em> option is set give a &lt;em>color gradient&lt;/em>.&lt;/p>
&lt;p>&lt;strong>To remove this section, delete &lt;code>content/home/demo.md&lt;/code>.&lt;/strong>&lt;/p>
&lt;/div>
&lt;/div>
&lt;h2 id="get-inspired">Get inspired&lt;/h2>
&lt;p>&lt;a href="https://github.com/wowchemy/starter-academic/tree/master/exampleSite" target="_blank" rel="noopener">Check out the Markdown files&lt;/a> which power the &lt;a href="https://academic-demo.netlify.app" target="_blank" rel="noopener">Academic Demo&lt;/a>, or &lt;a href="https://wowchemy.com/user-stories/" target="_blank" rel="noopener">view the showcase&lt;/a>.&lt;/p></description></item><item><title>Skills</title><link>https://drsandor.net/home-unused/skills/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://drsandor.net/home-unused/skills/</guid><description/></item><item><title>Experience</title><link>https://drsandor.net/home-unused/experience/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://drsandor.net/home-unused/experience/</guid><description/></item><item><title>Accomplish&amp;shy;ments</title><link>https://drsandor.net/home-unused/accomplishments/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://drsandor.net/home-unused/accomplishments/</guid><description/></item><item><title>Recent Posts</title><link>https://drsandor.net/home-unused/posts/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://drsandor.net/home-unused/posts/</guid><description/></item><item><title>Talks</title><link>https://drsandor.net/home-unused/talks/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://drsandor.net/home-unused/talks/</guid><description/></item><item><title>Featured Publications</title><link>https://drsandor.net/home-unused/featured/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://drsandor.net/home-unused/featured/</guid><description/></item><item><title>Recent Publications</title><link>https://drsandor.net/home-unused/publications/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://drsandor.net/home-unused/publications/</guid><description>&lt;div class="alert alert-note">
&lt;div>
Quickly discover relevant content by &lt;a href="./publication/">filtering publications&lt;/a>.
&lt;/div>
&lt;/div></description></item><item><title>Popular Topics</title><link>https://drsandor.net/home-unused/tags/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://drsandor.net/home-unused/tags/</guid><description/></item><item><title>Contact</title><link>https://drsandor.net/home-unused/contact/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://drsandor.net/home-unused/contact/</guid><description/></item><item><title>Keynote at ADOS Paris</title><link>https://drsandor.net/ai/ados/</link><pubDate>Thu, 10 Apr 2025 14:00:00 +0000</pubDate><guid>https://drsandor.net/ai/ados/</guid><description>&lt;p>On 28 March 2025, I attended the amazing &lt;a href="https://www.lightricks.com/ados" target="_blank" rel="noopener">ADOS&lt;/a> event in Paris at &lt;a href="https://www.artifex-lab.com" target="_blank" rel="noopener">Artifex Lab&lt;/a>, a new space in Paris for bringing together AI arists and technologists. ADOS was co-organized by &lt;a href="https://banodoco.ai" target="_blank" rel="noopener">Banadoco&lt;/a> and &lt;a href="https://www.lightricks.com" target="_blank" rel="noopener">Lightricks&lt;/a>, which was an ideal fit for Artifex Lab: Banadoco is the leading online community for AI art+technology and Lightricks&amp;rsquo;s LTX is the fastest generative AI video model (and it&amp;rsquo;s Open Source on top of that!).&lt;/p>
&lt;p>Coincidentally, our team has recently been investigating LTX deeply and also has been hanging out a lot on Banadoco&amp;rsquo;s inspiring Discord server. One thing led to another and I was invited to present a keynote at ADOS. We brought most of the team, as well as a bleeding edge demo. Unfortuntately, I missed out on the second day of the event, which featured a hackathon, where our PhD student Hovhannes received a prize (see his upcoming blogpost on our &lt;a href="https://ar-ai.org" target="_blank" rel="noopener">team webpage&lt;/a>).&lt;/p>
&lt;p>
&lt;figure id="figure-team-arai-at-ados">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Team ARAI at ADOS" srcset="
/media/ados/team_hu3a541a33e063e69d3b30aeefb30179f7_534763_b783a900c056276757e7f7017585a4b8.webp 400w,
/media/ados/team_hu3a541a33e063e69d3b30aeefb30179f7_534763_4448afd20a1677241c97a762bfe02851.webp 760w,
/media/ados/team_hu3a541a33e063e69d3b30aeefb30179f7_534763_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://drsandor.net/media/ados/team_hu3a541a33e063e69d3b30aeefb30179f7_534763_b783a900c056276757e7f7017585a4b8.webp"
width="760"
height="570"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Team ARAI at ADOS
&lt;/figcaption>&lt;/figure>
&lt;figure id="figure-zeev-farbman-co-founder--ceo-of-lightricks-trying-our-demo">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Zeev Farbman, Co-Founder &amp;amp; CEO of Lightricks trying our demo" srcset="
/media/ados/demo_hua7d9fbdda16f2d02e5b664a9fb28b240_541462_3cc95a501fbfe4de2ff237a29e99ded4.webp 400w,
/media/ados/demo_hua7d9fbdda16f2d02e5b664a9fb28b240_541462_007047a2ff3dd4aaa738306758667246.webp 760w,
/media/ados/demo_hua7d9fbdda16f2d02e5b664a9fb28b240_541462_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://drsandor.net/media/ados/demo_hua7d9fbdda16f2d02e5b664a9fb28b240_541462_3cc95a501fbfe4de2ff237a29e99ded4.webp"
width="760"
height="570"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Zeev Farbman, Co-Founder &amp;amp; CEO of Lightricks trying our demo
&lt;/figcaption>&lt;/figure>&lt;/p>
&lt;p>Before reporting about the keynote and demo, I would like to highlight some of my favourite things that I saw at the event. All talks are &lt;a href="https://www.youtube.com/watch?v=gBeZSbaxMvc" target="_blank" rel="noopener">recorded on youtube&lt;/a>; I provide direct links to the talks in the following.&lt;/p>
&lt;p>I enjoyed &lt;a href="https://www.youtube.com/live/gBeZSbaxMvc?si=orYRKkprSBqWWwBo&amp;amp;t=468" target="_blank" rel="noopener">Emma Catnip&amp;rsquo;s talk&lt;/a>, particularly her work Bloom. It is very soulful and beautifully designed. I will never forget her bitter-sweet explanation: &amp;ldquo;It&amp;rsquo;s about the summer of love I that never had&amp;rdquo;.&lt;/p>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/cDUExDUYRaE" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;p>Another very interesting artist&amp;rsquo;s talk was by &lt;a href="https://udart.dk" target="_blank" rel="noopener">Vibeke Bertelsen (Udart)&lt;/a>; &lt;a href="https://www.youtube.com/live/gBeZSbaxMvc?si=r3DAzpV5a7a7C8jd&amp;amp;t=8304" target="_blank" rel="noopener">timestamp&lt;/a>. Imagine current AI tools in the hands of H.R. Giger&amp;hellip; Below is one of her award-winning short movies. Definitely most unique!&lt;/p>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/8Vodtg8WNiY" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;p>On the tech side, my favourite talk was by &lt;a href="https://github.com/yvann-ba" target="_blank" rel="noopener">Yvann Barbot&lt;/a>. In &lt;a href="https://www.youtube.com/live/gBeZSbaxMvc?si=utqO7-YM0Oq1c5Fx&amp;amp;t=8960" target="_blank" rel="noopener">his talk&lt;/a>, he showed how easy it is to generate cool audio-reactive video clips using his ComfyUI nodes. One example below by &lt;a href="https://www.youtube.com/@visualfrisson" target="_blank" rel="noopener">Visual Frisson&lt;/a>. After the event, I checked Yvann&amp;rsquo;s youtube channel. I can highly recommend it!&lt;/p>
&lt;video width="720" controls>
&lt;source src="frisson.mp4" type="video/mp4" />
&lt;/video>
&lt;p>Finally, let me get to my keynote talk and our demo.&lt;/p>
&lt;p>Due to the extreme technical difficulties involved in our demo, we were working literally till the last minute on it. Chapeau to DÃ¡vid MaruscsÃ¡k and Francesco Dettori for their very hard work near demo time! Very unfortunately, the backend of our demo stopped working 5 minutes before my talk started. As a result, we could not show the demo during my keynote (as we had originally planned), but only 2 hours later. Luckily, most attendants of ADOS were still there and could give it a try!&lt;/p>
&lt;p>We demonstrate how to use a cloud backend for highspeed generation of spatial videos based on image and text inputs. You can see a similar version of the demo below; however, compared to the demo we showed at ADOS, it is only using 1 RTX4090 GPU for inference, as opposed to 4 H100 GPUs at ADOS, so it runs significantly slower.&lt;/p>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/Gk2dtoOXDac" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;p>You can see my keynote on &lt;a href="https://www.youtube.com/live/gBeZSbaxMvc?si=qr5cMNz5WXu8Jt7d&amp;amp;t=7061" target="_blank" rel="noopener">youtube&lt;/a>, where I speak about my long past with AR and my more recent past with AI, and how everything converged into this demo :-)&lt;/p>
&lt;p>To conclude, I would like to sincerely thank:&lt;/p>
&lt;ul>
&lt;li>The &lt;a href="https://www.lightricks.com/ados" target="_blank" rel="noopener">ADOS&lt;/a> organizational team: &lt;a href="https://banodoco.ai" target="_blank" rel="noopener">Banadoco&lt;/a>, &lt;a href="https://www.lightricks.com" target="_blank" rel="noopener">Lightricks&lt;/a>, and &lt;a href="https://www.artifex-lab.com" target="_blank" rel="noopener">Artifex Lab&lt;/a>.&lt;/li>
&lt;li>Special thanks to Lightricks for giving us early access to their distilled LTX model, which enabled impressive speedups.&lt;/li>
&lt;li>All &lt;a href="https://ar-ai.org/people/" target="_blank" rel="noopener">ARAI team members&lt;/a> for bearing with my demands :-) Directly involved in the demo were Cyrille Leroux, Thomas Betton, DÃ¡vid MaruscsÃ¡k, Francesco Dettori, and Hovhannes Margaryan.&lt;/li>
&lt;/ul></description></item><item><title>Keynote at ADOS Paris</title><link>https://drsandor.net/project/ai3/</link><pubDate>Thu, 10 Apr 2025 14:00:00 +0000</pubDate><guid>https://drsandor.net/project/ai3/</guid><description>&lt;p>Test&lt;/p></description></item><item><title>AI Video Generation - The Future is already here</title><link>https://drsandor.net/ai/good-bad-ugly/</link><pubDate>Wed, 18 Dec 2024 14:00:00 +0000</pubDate><guid>https://drsandor.net/ai/good-bad-ugly/</guid><description>&lt;p>I concluded my last AI blog post in October with &amp;ldquo;open source video
generation models&amp;hellip; will overtake closed sourced ones sooner than
expected!&amp;rdquo;. &lt;em>Surprise: it has already happened!&lt;/em>&lt;/p>
&lt;p>In this lengthy blogpost, I want to:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Explain the &lt;em>enormous work&lt;/em> that we did over the &lt;em>last 10 days&lt;/em>, resulting in a funny AI
movie (jump to the end if you are only here for that)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Explain the &lt;em>big picture&lt;/em> of generative AI for videos&lt;/p>
&lt;/li>
&lt;/ul>
&lt;details class="toc-inpage d-print-none " open>
&lt;summary class="font-weight-bold">Table of Contents&lt;/summary>
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-the-ideal-video-model">1. The Ideal Video Model&lt;/a>
&lt;ul>
&lt;li>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#2-current-video-models">2. Current Video Models&lt;/a>&lt;/li>
&lt;li>&lt;a href="#3-case-study-ai-remake-of-the-good-the-bad-and-the-ugly">3. Case Study: AI Remake of &amp;ldquo;The Good, The Bad, and The Ugly&amp;rdquo;&lt;/a>
&lt;ul>
&lt;li>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/details>
&lt;p>Since my last blog post, a ton of generative AI video models have been
released and I have tested the most important ones. To be upfront with
my conclusion: &lt;a href="https://github.com/Tencent/HunyuanVideo" target="_blank" rel="noopener">Tencent&amp;rsquo;s Hunyuan Video&lt;/a>
is &lt;em>absolutely astonishing&lt;/em> and
will have wide impact. The last time I had this feeling was when Stable
Diffusion came out in 2022, which was the foundation for successful
commercial products such as Midjourney and is still being used widely
(e.g. the new Mountain Dew commercials are made with it). I am convinced:
&lt;em>Hunyuan will be for video generation what Stable Diffusion has been
for image generation.&lt;/em>&lt;/p>
&lt;h2 id="1-the-ideal-video-model">1. The Ideal Video Model&lt;/h2>
&lt;p>Obviously, we want video models to be expressive and generate videos of
high resolution and framerate. Some not-so-obvious requirements include:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;em>Open source:&lt;/em> model can be downloaded and run locally&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;em>Uncensored:&lt;/em> censored models lead to many problems&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Allow the development of an &lt;em>ecosystem of tools&lt;/em> around it&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>Following the analogy to Stable Diffusion I drew earlier: all of these
were true for it and were key to why it won.&lt;/p>
&lt;h4 id="model-censorship">Model Censorship&lt;/h4>
&lt;p>In general, this is a difficult question. On the one hand, I concur that
some extreme content should simply not exist at all, so models should
not be able to generate it. On the other hand, over-censoring of models
has some unintended side effects.&lt;/p>
&lt;p>Earlier this year, Google&amp;rsquo;s image generation model Gemini was &lt;a href="https://www.nytimes.com/2024/02/22/technology/google-gemini-german-uniforms.html" target="_blank" rel="noopener">widely ridiculed&lt;/a> when a
prompt for &amp;ldquo;Nazi soldiers&amp;rdquo; would generate Black and Asian soldiers. When
OpenAI released DALLE in 2022, I tested it with the prompt &amp;ldquo;My Bavarian
mother unintentionally takes a selfie with her mobile phone&amp;rdquo;, also
returning me a selection of ethnicities in the results, which are unlikely
to match what I actually want. What a bad user experience!&lt;/p>
&lt;p>This year, Stability AI released Stable Diffusion 3, which the community
had been waiting for in great anticipation. It quickly turned out that this
model is quite useless. The training data only contained a very small
amount of lightly-dressed humans, leading to a total incomprehension of
human anatomy. The prompt &amp;ldquo;woman lying on grass&amp;rdquo; quickly became a meme.&lt;/p>
&lt;figure id="figure-woman-lying-on-grass-according-to-stable-diffusion-3httpsstabilityainewsstable-diffusion-3">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="&amp;#39;&amp;#39;Woman lying on grass&amp;#39;, according to [Stable Diffusion 3](https://stability.ai/news/stable-diffusion-3)" srcset="
/ai/good-bad-ugly/1_hu9c3d30bf53313f395dc9309d4b7f68f9_2061541_4736902eab9f0ecd57bf02bf8b4ee839.webp 400w,
/ai/good-bad-ugly/1_hu9c3d30bf53313f395dc9309d4b7f68f9_2061541_5acf54ae7b89121f468fb098ea842425.webp 760w,
/ai/good-bad-ugly/1_hu9c3d30bf53313f395dc9309d4b7f68f9_2061541_1200x1200_fit_q75_h2_lanczos_3.webp 1200w"
src="https://drsandor.net/ai/good-bad-ugly/1_hu9c3d30bf53313f395dc9309d4b7f68f9_2061541_4736902eab9f0ecd57bf02bf8b4ee839.webp"
width="760"
height="430"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption data-pre="Figure&amp;nbsp;" data-post=":&amp;nbsp;" class="numbered">
&amp;lsquo;&amp;lsquo;Woman lying on grass&amp;rsquo;, according to &lt;a href="https://stability.ai/news/stable-diffusion-3" target="_blank" rel="noopener">Stable Diffusion 3&lt;/a>
&lt;/figcaption>&lt;/figure>
&lt;h4 id="ecosystem">Ecosystem&lt;/h4>
&lt;p>In my last blog post, I showed how I built a pipeline out of several
open source models. This ability is essential for many use cases,
including professional media production and research. It also
supports the creation of an ecosystem of supporting tools (more on this below).&lt;/p>
&lt;p>The models developed by big Silicon Valley companies like Meta, Google,
and OpenAI don&amp;rsquo;t support such use cases and probably don&amp;rsquo;t even want to;
they are happy if users can modify a video they took in a cafÃ© to show a
Kombucha instead of a Coke or an Avocado Toast instead of a Hamburger.&lt;/p>
&lt;p>An ecosystem of tools around foundation models is critical, because it
allows, among other things, the very precise &lt;em>specification of outputs&lt;/em>.
Simply speaking, one of the most useless ways to specify the image or
video that I want to be generated is by text. Much more useful are:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;em>Controlnets:&lt;/em> can specify output by visual constraints (colors, body
poses, depth information, edge information etc.)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;em>IPAdapters:&lt;/em> can be used to transfer non-tangible information, such
as the style of an image&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;em>LORAs:&lt;/em> a more heavyweight approach than IPAdapters that allows much
more control of styles, materials, and humans in the output&lt;/p>
&lt;/li>
&lt;/ul>
&lt;figure id="figure-left-behind-the-scenes-of-a-video-i-posted-on-linkedin-in-022024httpswwwlinkedincompostsdr-christian-sandor-b7240890_i-did-some-dance-training-over-the-holidays-activity-7156976991240355840-28_outm_sourceshareutm_mediummember_desktop-using-a-lora-i-had-trained-on-my-appearance-and-a-controlnet-for-body-pose-right-my-coworker-huyenhttpsar-aiorgauthorhuyen-nguyen-generated--using-an-ipadapter-conditioned-with-4-photos-of-her">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Left: behind the scenes of a video I [posted on Linkedin in 02/2024](https://www.linkedin.com/posts/dr-christian-sandor-b7240890_i-did-some-dance-training-over-the-holidays-activity-7156976991240355840-28_O?utm_source=share&amp;amp;utm_medium=member_desktop), using a LORA I had trained on my appearance and a controlnet for body pose. Right: my coworker [Huyen](https://ar-ai.org/author/huyen-nguyen/), generated using an IPAdapter conditioned with 4 photos of her." srcset="
/ai/good-bad-ugly/2_hufba5692dd20cec5e2bc1458418e9c891_1409682_c5752d928a33c7648640a778276f4fef.webp 400w,
/ai/good-bad-ugly/2_hufba5692dd20cec5e2bc1458418e9c891_1409682_0ee009d1ae63cc7e8937dc82134f59fa.webp 760w,
/ai/good-bad-ugly/2_hufba5692dd20cec5e2bc1458418e9c891_1409682_1200x1200_fit_q75_h2_lanczos_3.webp 1200w"
src="https://drsandor.net/ai/good-bad-ugly/2_hufba5692dd20cec5e2bc1458418e9c891_1409682_c5752d928a33c7648640a778276f4fef.webp"
width="760"
height="450"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption data-pre="Figure&amp;nbsp;" data-post=":&amp;nbsp;" class="numbered">
Left: behind the scenes of a video I &lt;a href="https://www.linkedin.com/posts/dr-christian-sandor-b7240890_i-did-some-dance-training-over-the-holidays-activity-7156976991240355840-28_O?utm_source=share&amp;amp;utm_medium=member_desktop" target="_blank" rel="noopener">posted on Linkedin in 02/2024&lt;/a>, using a LORA I had trained on my appearance and a controlnet for body pose. Right: my coworker &lt;a href="https://ar-ai.org/author/huyen-nguyen/" target="_blank" rel="noopener">Huyen&lt;/a>, generated using an IPAdapter conditioned with 4 photos of her.
&lt;/figcaption>&lt;/figure>
&lt;h2 id="2-current-video-models">2. Current Video Models&lt;/h2>
&lt;p>Since my last AI blog post, I consider the most important models that
have been released: LTX-Video (1 month ago), Hunyuan (3 weeks ago), and
many additions to CogVideoX. Their main strengths are:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;em>LTX:&lt;/em> Lightning fast&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;em>CogVideoX:&lt;/em> Extreme controllability&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;em>Hunyuan:&lt;/em> Best visual quality&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>First, let me show an interactive session of me with &lt;em>LTX video&lt;/em>, captured
in real-time from my screen. The speed is absolutely mind-blowing!&lt;/p>
&lt;video width="720" autoplay loop>
&lt;source src="ltx.mp4" type="video/mp4" />
&lt;/video>
&lt;p>The &lt;em>CogVideoX&lt;/em> family stands out by how many things can be controlled
about the videos that are generated, including:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Using images to specify the start &amp;amp; end of the video&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Define trajectories of objects in the video&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Exact specification of camera movements&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Using LORAs, we can already specify styles like &amp;ldquo;Disney Movie&amp;rdquo;, with
more coming out on a daily basis&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Using a variety of controlnets&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>Video generation with CogVideoX using controlnet (courtesy of &lt;a href="https://github.com/kijai" target="_blank" rel="noopener">Kijai&lt;/a>):&lt;/p>
&lt;video width="720" autoplay loop>
&lt;source src="kijai.mp4" type="video/mp4" />
&lt;/video>
&lt;p>Finally: &lt;em>Hunyuan&lt;/em>. I very quickly got very convinced by it.
Here is the sixth test video that I ever created with it.
The quality is just unbelievable for a quick test video.
As input, I used Huyen&amp;rsquo;s image from Figure 2 piped through an LLM for captioning and motion instructions.&lt;/p>
&lt;video width="720" autoplay loop>
&lt;source src="huyen.mp4" type="video/mp4" />
&lt;/video>
&lt;h2 id="3-case-study-ai-remake-of-the-good-the-bad-and-the-ugly">3. Case Study: AI Remake of &amp;ldquo;The Good, The Bad, and The Ugly&amp;rdquo;&lt;/h2>
&lt;p>The goal of this experiment was to understand Hunyuan capabilities more
deeply. It took 10 intense days from start to finish. It&amp;rsquo;s important to
note that if I had simply wanted to generate the best possible AI video
when I started, I would have used CogVideoX, because of its strong
controllability. And indeed, controllability was the main challenge in
this experiment, as Hunyuan only accepted 2 types of inputs when we
started:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Text prompts&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Videos for specifying motion vectors&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>Together with my most artistic PhD student (&lt;a href="https://ar-ai.org/author/david-maruscsak/" target="_blank" rel="noopener">DÃ¡vid MaruscsÃ¡k&lt;/a>), we set out
to make a short AI movie with Hunyuan. Our plan of attack was:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Model exploration: understand censorship &amp;amp; prompting capabilities&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Creative exploration: which kind of movie can we make given the
technical constraints?&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Production&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h4 id="model-exploration">Model Exploration&lt;/h4>
&lt;p>A very weird characteristic of Hunyuan is that to get the best outputs,
you can&amp;rsquo;t really use natural language prompts (same issue as for e.g.
Flux or LTX-Video). Instead, you must use an LLM that transforms what
you want into a prompt that suits the model. Tencent very kindly also
released the LLM that they are using for that, but it&amp;rsquo;s VRAM requirement
is 700GB&amp;hellip; To put this in perspective, you would need 44 of the best
consumer graphics cards available right now. It seemed too heavy.
Instead, we used a quantized version of Llama 3.3 that only needs 35GB
VRAM, so it could fit into our H100 card.&lt;/p>
&lt;p>Without LORAs, the standard trick to achieve consistency between shots
is to use celebrities as actors in your AI movie. So, we started by
testing 120 celebrities .&lt;/p>
&lt;p>The pipeline was fully automatic using Llama and Hunyuan. First, Llama
generated a list of the top-20 celebrities for certain domains (sports,
music, politics, fictional characters, etc.); then, it created a prompt
in the expected format for Hunyuan. We generated four 5-second videos
per celebrity, resulting in 480 clips that DÃ¡vid MaruscsÃ¡k watched and
rated. The whole process took 8 hours on a single H100 GPU.&lt;/p>
&lt;p>Armed with the results, we proceeded to explore different storylines for
our movie.&lt;/p>
&lt;h4 id="creative-exploration">Creative Exploration&lt;/h4>
&lt;p>Our first idea was to remake the arthouse movie &lt;a href="https://en.wikipedia.org/wiki/Coffee_and_Cigarettes" target="_blank" rel="noopener">Coffee and
Cigarettes&lt;/a>. Initial results were very encouraging:&lt;/p>
&lt;figure id="figure-input-screencap-of-coffee-and-cigarettes-below-sample-outputs-of-automated-pipeline">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Input screencap of Coffee and Cigarettes. Below: sample outputs of automated pipeline." srcset="
/ai/good-bad-ugly/cc_hu3ebab9955c4ef13d01d84409b44410e3_4088521_1d5f95cc72399937db7f510830518c1d.webp 400w,
/ai/good-bad-ugly/cc_hu3ebab9955c4ef13d01d84409b44410e3_4088521_8e69f39dfdc54929eb96796b2e10d5e7.webp 760w,
/ai/good-bad-ugly/cc_hu3ebab9955c4ef13d01d84409b44410e3_4088521_1200x1200_fit_q75_h2_lanczos_3.webp 1200w"
src="https://drsandor.net/ai/good-bad-ugly/cc_hu3ebab9955c4ef13d01d84409b44410e3_4088521_1d5f95cc72399937db7f510830518c1d.webp"
width="760"
height="402"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption data-pre="Figure&amp;nbsp;" data-post=":&amp;nbsp;" class="numbered">
Input screencap of Coffee and Cigarettes. Below: sample outputs of automated pipeline.
&lt;/figcaption>&lt;/figure>
&lt;video width="720" autoplay loop>
&lt;source src="cc1.mp4" type="video/mp4" />
&lt;/video>
&lt;video width="720" autoplay loop>
&lt;source src="cc2.mp4" type="video/mp4" />
&lt;/video>
&lt;p>However, we quickly realized that with the currently incomplete
ecosystem around Hunyuan, we can only depict 1 celebrity per shot
reliably. In general, the success rate when having 2 goes down dramatically (90% =&amp;gt;
1%).&lt;/p>
&lt;p>So, we had to find a movie genre that mainly shows single actors in
shots, which led us to &amp;ldquo;The Good, The Bad, and The Ugly&amp;rdquo;.&lt;/p>
&lt;p>Based on our test, the celebrity that works by far the best among all is
Donald Trump. So, he had to be one of them. I remembered that Donald
Trump had some beef with Taylor Swift (&amp;ldquo;cat lady&amp;rdquo;), so we considered it
to be a good fit to make Taylor &amp;ldquo;The Good&amp;rdquo;, as she kills &amp;ldquo;The Bad&amp;rdquo; at
the end of the movie. We were only left with determining &amp;ldquo;The Ugly&amp;rdquo;.
While Vladimir Putin works very well in Hunyuan and could be a good fit,
we were not too keen to get to experience Novichok agent. So, we settled
for Elon Musk (If you read this Elon: Please don&amp;rsquo;t disable the breaks
while I am driving my car. It is only a joke! Thank you!).&lt;/p>
&lt;h4 id="production">Production&lt;/h4>
&lt;p>To be honest, it was a painful process. We had about 20 short shots that
we had to generate. Only very few of them worked through a completely
automated pipeline. For the others, it was more like:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Generate 16 clips with automated pipeline (~5 minutes)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Look at clips, pick best one, refine prompt&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Goto 1.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>But, oh boy! When it worked, it worked astonishingly well:&lt;/p>
&lt;video width="720" autoplay loop>
&lt;source src="elon.mp4" type="video/mp4" />
&lt;/video>
&lt;p>A few clips were completely impossible to generate, as there is a
strange dependency between:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Celebrity in the shot&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Activity that the celebrity is doing&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Overall scene&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>I still don&amp;rsquo;t fully understand this point. Changing a single word in the
prompt can get you from &amp;ldquo;completely unusable&amp;rdquo; to &amp;ldquo;perfect&amp;rdquo;.&lt;/p>
&lt;p>It took me about 10 hours of work to generate the 20 shots. Finally,
David used his great video editing skills to put everything together.
Enjoy!&lt;/p>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/FpGGfpizIi4" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;p>&lt;strong>Conclusions&lt;/strong> Hunyuan video demonstrated great potential in this
project. I also had to dive into LLMs (something that I religiously
avoided before that), which was a very interesting experience! I can&amp;rsquo;t
wait for Tencent to release further capabilities for their model (most
importantly: image2video), as well as the open source community creating
more and especially better LORAs (I found the ones that are currently
available to not work very well).&lt;/p>
&lt;p>&lt;strong>Acknowledgements&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;a href="https://ar-ai.org/author/david-maruscsak/" target="_blank" rel="noopener">DÃ¡vid MaruscsÃ¡k&lt;/a>: Creative input, video editing&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Banadoco discord: Technical support and joint explorations of the model&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://github.com/kijai" target="_blank" rel="noopener">Kijai&lt;/a>: advanced technical support&lt;/p>
&lt;/li>
&lt;/ul></description></item><item><title>AI Video Generation - The Future is already here</title><link>https://drsandor.net/project/ai2/</link><pubDate>Wed, 18 Dec 2024 14:00:00 +0000</pubDate><guid>https://drsandor.net/project/ai2/</guid><description/></item><item><title>Video Generation with Open Source Models</title><link>https://drsandor.net/ai/attenborough/</link><pubDate>Wed, 02 Oct 2024 14:00:00 +0000</pubDate><guid>https://drsandor.net/ai/attenborough/</guid><description>&lt;p>Social media has recently been flooded with impressive AI videos made with closed source tools like Sora, Runway, or Kling. In this experiment, I wanted to investigate how far you can get by using only open source AI models.&lt;/p>
&lt;p>What is the relevance of open source AI models? First, as Yann LeCun and others argue, there are implications on the highest level; AI is becoming an absolute key technology of our times; it can&amp;rsquo;t be good for humanity if this power is in the hand of small group of people leading companies in Silicon Valley and Beijing. Second, there is a technical dimension to it. As we saw with Linux vs. Windows: Open Source leads to technical superiority in the long run. Applied to AI, we could observe that it did not take long for open text-to-image models to overtake closed ones (e.g. Stable Diffusion vs. Midjourney).&lt;/p>
&lt;p>In the following, I describe details of my experiment, which took 10 hours from concept to final cut. You can also skip directly to the end to see the final result.&lt;/p>
&lt;h2 id="automatic-pipeline">Automatic Pipeline&lt;/h2>
&lt;p>As first step, I set up an automatic pipeline to create videos from images. As input images, I used some beautiful nature photographs (links to all used resources at the bottom of this page). Then, I adapted an awesome ComfyUI workflow to run in my environment (A100 with 80GB VRAM). Final computation speed was just below 2 minutes from an image to a video clip consisting of 49 frames (720x480 pixels). One important aspect of this workflow is that while it includes some text-to-image models, it does not require me to enter text prompts in order to get images; I much prefer a completely visual pipeline.&lt;/p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="" srcset="
/ai/attenborough/pipeline_hue860506cb1c1a8ced4983eb02da0f130_122916_d23f71f966a831b9bec743bbcfd330c0.webp 400w,
/ai/attenborough/pipeline_hue860506cb1c1a8ced4983eb02da0f130_122916_7ce085067317ddb28b4edcd7649ccecc.webp 760w,
/ai/attenborough/pipeline_hue860506cb1c1a8ced4983eb02da0f130_122916_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://drsandor.net/ai/attenborough/pipeline_hue860506cb1c1a8ced4983eb02da0f130_122916_d23f71f966a831b9bec743bbcfd330c0.webp"
width="760"
height="428"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;p>Based on 25 input images, I generated 96 clips in 3 hours, taking just below 2 minutes per clip.&lt;/p>
&lt;h2 id="manual-postprocessing">Manual Postprocessing&lt;/h2>
&lt;p>Out of the 96 generated clips, about half were sufficiently good. I then picked the 23 I liked best and performed spatio-temporal upscaling in Topaz AI to 1920p resolution at 30 fps. Finally, I quickly arranged the clips in iMovie, together with an audio track from Sir Attenborough.&lt;/p>
&lt;h2 id="final-result--conclusions">Final Result &amp;amp; Conclusions&lt;/h2>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/-IhoM75vOyI" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;p>The final result is clearly not as good as what can be done with closed source tools (e.g. a recent favorite of mine: &lt;a href="https://x.com/tunguz/status/1832493397694660914" target="_blank" rel="noopener">La Baie Area&lt;/a>). However, it confirmed my opinion about the trajectory of open source video generation models (I have been testing them all&amp;hellip;): they will overtake closed sourced ones sooner than expected!&lt;/p>
&lt;p>I also learned more about the limitations of CogVideoX: I did a similar experiment with humans in the input images, but they did not work out well. While camera trajectories and actions in the background worked very well, consistency of humans was lacking.&lt;/p>
&lt;h3 id="credits">Credits&lt;/h3>
&lt;ul>
&lt;li>Audio: David Attenborough - For All Nature (&lt;a href="https://www.youtube.com/watch?v=-a_dMFnib-s" target="_blank" rel="noopener">https://www.youtube.com/watch?v=-a_dMFnib-s&lt;/a>)&lt;/li>
&lt;li>Wildlife photographs: &lt;a href="https://www.smithsonianmag.com/smart-news/see-25-breathtaking-images-from-the-wildlife-photographer-of-the-year-contest-180983516/" target="_blank" rel="noopener">https://www.smithsonianmag.com/smart-news/see-25-breathtaking-images-from-the-wildlife-photographer-of-the-year-contest-180983516/&lt;/a>&lt;/li>
&lt;li>ComfyUI Workflow: &lt;a href="https://github.com/henrique-galimberti/i2v-workflow/blob/main/CogVideoX-I2V-workflow_v2.json" target="_blank" rel="noopener">https://github.com/henrique-galimberti/i2v-workflow/blob/main/CogVideoX-I2V-workflow_v2.json&lt;/a>&lt;/li>
&lt;li>Models:
&lt;ul>
&lt;li>Florence-2-large&lt;/li>
&lt;li>Lexi-Llama-3-8B-Uncensored_Q4_K_M.gguf&lt;/li>
&lt;li>CogVideoX-5b&lt;/li>
&lt;li>Outpainting: RealVisXL V3.0 Inpainting&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul></description></item><item><title>AI Experiments</title><link>https://drsandor.net/project/ai/</link><pubDate>Sun, 29 Sep 2024 14:00:00 +0000</pubDate><guid>https://drsandor.net/project/ai/</guid><description/></item><item><title>Demo at ACM SIGGRAPH Real-Time Live!</title><link>https://drsandor.net/project/rtl/</link><pubDate>Wed, 04 Sep 2024 14:00:00 +0000</pubDate><guid>https://drsandor.net/project/rtl/</guid><description>&lt;p>A long-term dream of mine came true! At the end of July 2024, we showed a demonstration at ACM SIGGRAPH 2024 in Denver for their Real-Time Live! category.&lt;/p>
&lt;p>First of all, &lt;strong>what is ACM SIGRAPH Real-Time Live?&lt;/strong> It is a specific category at the premiere computer graphics conference &lt;a href="https://en.wikipedia.org/wiki/SIGGRAPH" target="_blank" rel="noopener">ACM SIGGRAPH&lt;/a>. While SIGGRAPH has been running every year since 1974, the Real-Time Live! category (RTL for short) has only been around for 15 years. It is very engaging, as presenters have 6 minutes to show a demonstration of real-time graphics to a large audience of computer graphics professionals (~5000 people). For me, this has always been the most fascinating event of SIGGRAPH, as it shows you the maximum of whatâ€™s possible in real-time graphics today. Also, compared to research papers, you can actually see the inventors demonstrating what they made&amp;mdash; this makes much clearer what really works and what doesnâ€™t.&lt;/p>
&lt;p>Here are some of my favorites from recent years:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://youtu.be/MAXJWEoKbxY?si=s9nHKBy8K2WWPvuN&amp;amp;t=822" target="_blank" rel="noopener">AI &amp;amp; Physics-Assisted Character Pose Authoring&lt;/a> (2022)&lt;/li>
&lt;li>&lt;a href="https://www.youtube.com/watch?v=lXZhgkNFGfM" target="_blank" rel="noopener">Bebylon&lt;/a> (2018)&lt;/li>
&lt;li>IQ livecoding with Shadertoy (I canâ€™t find the video on YouTube; guess it was before they started uploading RTL to YouTube)&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>How did our demo come about?&lt;/strong> In December 2023, I contacted Matt Swoboda about submitting something to RTL. We were playing with this idea over the last decade, but it never materialized, until this year! Matt quickly brought in a great artist, while I could secure some sponsorship and support from Canon Japan, which led to this amazing constellation:&lt;/p>
&lt;ul>
&lt;li>Best real-time rendering engine for live events: &lt;a href="https://www.notch.one" target="_blank" rel="noopener">Notch&lt;/a> (Matt is their Co-Founder &amp;amp; CTO)&lt;/li>
&lt;li>Fantastic audio-visual artist: &lt;a href="https://www.brettbolton.net" target="_blank" rel="noopener">Brett Bolton&lt;/a> (Designer of U2â€™s show in the Las Vegas sphere, VJ for the Grateful Dead, etc.)&lt;/li>
&lt;li>The most stressable AR demo crew: &lt;a href="https://ar-ai.org/author/david-maruscak/" target="_blank" rel="noopener">David Maruscsak&lt;/a> and me ðŸ˜Š&lt;/li>
&lt;li>Best AR headset: &lt;a href="https://global.canon/en/technology/mr2019.html" target="_blank" rel="noopener">Canonâ€™s X1&lt;/a> (Sorry Apple, but you need to work much harder! Even though X1 was released 4 years before your Vision Pro, it has a much better image quality, better co-axial alignment of screens and cameras, better tracking infrastructure, easier integration with custom software, much better weight, etc; let me not even get started about Metaâ€™s headsetsâ€¦)&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>What is the concept of our demo?&lt;/strong> You can watch a concise summary of our
concept in the submission video below that we sent to SIGGRAPH. The main feedback that we received from the organizers was that we should expand our demo to have 2 viewers instead of 1 in order to highlight the potential to have visuals interact with multiple viewers in 3D. Challenge accepted!&lt;/p>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/uCb42SsFLk8" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;br>
&lt;p>&lt;strong>What could the audience see?&lt;/strong> Unfortunately, there were several issues with the YouTube stream of the event, so it was much harder for the remote audience to understand what was going on. Let me start by explaining what the audience in the room could see.&lt;/p>
&lt;p>
&lt;figure id="figure-view-from-stage">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="View from stage" srcset="
/media/rtl/pano_hu13dbddae45a07af144be999962d7c505_9581461_85ae5dff0cc01d3e7b407b77f81e8534.webp 400w,
/media/rtl/pano_hu13dbddae45a07af144be999962d7c505_9581461_b503633b0ef4324c0f8a2dd0045d9306.webp 760w,
/media/rtl/pano_hu13dbddae45a07af144be999962d7c505_9581461_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://drsandor.net/media/rtl/pano_hu13dbddae45a07af144be999962d7c505_9581461_85ae5dff0cc01d3e7b407b77f81e8534.webp"
width="760"
height="194"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
View from stage
&lt;/figcaption>&lt;/figure>
&lt;figure id="figure-audience-view-of-stage">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Audience view of stage" srcset="
/media/rtl/stage_huc5c81e4ab00a316abb82eed7380d6a68_577662_c859b43e2dea225e971b0373b49b770b.webp 400w,
/media/rtl/stage_huc5c81e4ab00a316abb82eed7380d6a68_577662_5ff530d950c2bd588653e2976b777df1.webp 760w,
/media/rtl/stage_huc5c81e4ab00a316abb82eed7380d6a68_577662_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://drsandor.net/media/rtl/stage_huc5c81e4ab00a316abb82eed7380d6a68_577662_c859b43e2dea225e971b0373b49b770b.webp"
width="760"
height="570"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Audience view of stage
&lt;/figcaption>&lt;/figure>
&lt;figure id="figure-setup-details-on-the-right-stage-with-brett-playing-behind-him-traditional-audio-reactive-visuals-2-viewers-with-ar-headsets-in-front-of-the-stage-left-side-split-screen-showing-the-augmented-views-of-the-2-viewers">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Setup details. On the right: stage with Brett playing; behind him traditional audio-reactive visuals; 2 viewers with AR headsets in front of the stage. Left side: split screen, showing the augmented views of the 2 viewers." srcset="
/media/rtl/schema_hub0ff15292ea34398e4f86ba5f2aaa5bd_488378_f46bfc25bcc2119347d582a4176d1611.webp 400w,
/media/rtl/schema_hub0ff15292ea34398e4f86ba5f2aaa5bd_488378_91496829229e096b8f287e95293cca16.webp 760w,
/media/rtl/schema_hub0ff15292ea34398e4f86ba5f2aaa5bd_488378_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://drsandor.net/media/rtl/schema_hub0ff15292ea34398e4f86ba5f2aaa5bd_488378_f46bfc25bcc2119347d582a4176d1611.webp"
width="760"
height="428"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Setup details. On the right: stage with Brett playing; behind him traditional audio-reactive visuals; 2 viewers with AR headsets in front of the stage. Left side: split screen, showing the augmented views of the 2 viewers.
&lt;/figcaption>&lt;/figure>&lt;/p>
&lt;p>The setup shown in the above figure was not clear for viewers of the YouTube stream. It did not help that for the first half of our performance, the stream was only showing the view of 1 viewer.&lt;/p>
&lt;p>&lt;strong>The main challenge&lt;/strong> with our setup was controlling the lighting. Since the
sensors are quite sensitive to external lighting (as well as the AR content) we did our best to control this (including working with the Siggraph AV team), but even with our best efforts we had some glitches during the final performance. That said, we still got very nice compliments from the other participants and the audience.&lt;/p>
&lt;figure id="figure-final-rehearsal-note-the-colorful-background-behind-brett-compared-to-the-black-background-at-the-actual-performance">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Final rehearsal: note the colorful background behind Brett, compared to the black background at the actual performance" srcset="
/media/rtl/venue-rehearsal_hu0b7ba7870b9691ab7eeba5cdc97383b9_5224857_6a74a44848668db074c17e6d92c3be52.webp 400w,
/media/rtl/venue-rehearsal_hu0b7ba7870b9691ab7eeba5cdc97383b9_5224857_bac4666bfaa2864a45bfe5b5731116c3.webp 760w,
/media/rtl/venue-rehearsal_hu0b7ba7870b9691ab7eeba5cdc97383b9_5224857_1200x1200_fit_q75_h2_lanczos_3.webp 1200w"
src="https://drsandor.net/media/rtl/venue-rehearsal_hu0b7ba7870b9691ab7eeba5cdc97383b9_5224857_6a74a44848668db074c17e6d92c3be52.webp"
width="760"
height="384"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Final rehearsal: note the colorful background behind Brett, compared to the black background at the actual performance
&lt;/figcaption>&lt;/figure>
&lt;figure id="figure-final-rehearsal-the-colorful-bright-background-is-very-nice-because-it-illuminates-brett-2-views-on-left-and-shows-off-refractions-of-virtual-water-2-views-on-right--in-the-actual-performance-we-had-to-switch-back-to-black-because-of-tracking-instabilities">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Final rehearsal: the colorful bright background is very nice, because: it illuminates Brett (2 views on left) and shows off refractions of virtual water (2 views on right) . In the actual performance, we had to switch back to black because of tracking instabilities" srcset="
/media/rtl/rehearsal-details_hu6732b02741823ee3381c983d5d4ece3c_175293_f64b37e0f270f1ddb60658f560e03309.webp 400w,
/media/rtl/rehearsal-details_hu6732b02741823ee3381c983d5d4ece3c_175293_2807ec2913c180a051d71c5c32f712bb.webp 760w,
/media/rtl/rehearsal-details_hu6732b02741823ee3381c983d5d4ece3c_175293_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://drsandor.net/media/rtl/rehearsal-details_hu6732b02741823ee3381c983d5d4ece3c_175293_f64b37e0f270f1ddb60658f560e03309.webp"
width="760"
height="283"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Final rehearsal: the colorful bright background is very nice, because: it illuminates Brett (2 views on left) and shows off refractions of virtual water (2 views on right) . In the actual performance, we had to switch back to black because of tracking instabilities
&lt;/figcaption>&lt;/figure>
&lt;p>&lt;strong>Showtime!&lt;/strong> After months of hard work, we were finally ready to demo! The hall filling with the audience was an awe-inspiring moment for me (I estimate about 3500 attendants).&lt;/p>
&lt;video width="720" autoplay loop>
&lt;source src="showtimelapse.mp4" type="video/mp4" />
&lt;/video>
&lt;figure id="figure-selfie-just-before-the-start-of-the-show">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Selfie just before the start of the show" srcset="
/media/rtl/showtime_hu8400259ed9cc12074f04526ca19f5c86_2971141_285ff3afdc452cdaf1cdcfacb7ab255c.webp 400w,
/media/rtl/showtime_hu8400259ed9cc12074f04526ca19f5c86_2971141_4f4d81e45566148609b848161d9de68e.webp 760w,
/media/rtl/showtime_hu8400259ed9cc12074f04526ca19f5c86_2971141_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://drsandor.net/media/rtl/showtime_hu8400259ed9cc12074f04526ca19f5c86_2971141_285ff3afdc452cdaf1cdcfacb7ab255c.webp"
width="760"
height="570"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Selfie just before the start of the show
&lt;/figcaption>&lt;/figure>
&lt;p>Luckily we were the first team to present, so we could relax afterwards and enjoy the show, which contained some truly remarkable demos, including:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://www.youtube.com/live/Gm1B5DT8kE0?si=bXvehrC7aaXZARj_&amp;amp;t=429" target="_blank" rel="noopener">The controller for Jim Henson&amp;rsquo;s muppets&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.youtube.com/live/Gm1B5DT8kE0?si=oKXaQpHSPQ6l-OB5&amp;amp;t=5459" target="_blank" rel="noopener">Movin Tracin&amp;rsquo;&lt;/a>: my personal favorite. I was surprised that this demo did not get any award.&lt;/li>
&lt;li>&lt;a href="https://www.youtube.com/live/Gm1B5DT8kE0?si=Xm3XuXkt52NV09Mr&amp;amp;t=2231" target="_blank" rel="noopener">Mesh Mortal Combat&lt;/a>: winner of both awards (audience &amp;amp; jury).&lt;/li>
&lt;/ul>
&lt;p>You can also just watch &lt;a href="https://www.youtube.com/live/Gm1B5DT8kE0?si=Xm3XuXkt52NV09Mr" target="_blank" rel="noopener">the whole stream&lt;/a>, or jump directly to &lt;a href="https://www.youtube.com/live/Gm1B5DT8kE0?si=MLAwyoSwGnKb75dI&amp;amp;t=1215" target="_blank" rel="noopener">our part&lt;/a>.&lt;/p>
&lt;p>&lt;strong>Conclusions&lt;/strong> It was a great experience to present at Real-Time Live! I hope to return in coming years!&lt;/p>
&lt;p>What we learned from this work is that there is a huge potential for having visuals at live events in AR headsets; we got encouraging feedback at the conference!&lt;/p>
&lt;p>We also learned that it is very challenging, both in terms of technology, as well as in terms of having a viable business model. But, we have ideas for both. Stay tuned!&lt;/p>
&lt;p>&lt;strong>Acknowledgements&lt;/strong>
First and foremost, I would like to thank Brett and Matt to participate in this project. They are exemplars of a rare breed: extremely skilled folks who put creativity before money.&lt;/p>
&lt;p>Second, I would like to thank Canon for advanced support and a financial contribution to our costs.&lt;/p>
&lt;p>Last, but not least, I would also like to thank the RTL organizers, who did a really great job of putting this event together! See you next year!&lt;/p>
&lt;!--
&lt;figure id="figure-the-team">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="The Team" srcset="
/media/rtl/team_huc83d12a88c14e6bb7d48ea036a0b40a4_618767_e94a7d77b8229d8730374d15bb4843d8.webp 400w,
/media/rtl/team_huc83d12a88c14e6bb7d48ea036a0b40a4_618767_18ac242625abffd0033eee0a0f8b21a8.webp 760w,
/media/rtl/team_huc83d12a88c14e6bb7d48ea036a0b40a4_618767_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://drsandor.net/media/rtl/team_huc83d12a88c14e6bb7d48ea036a0b40a4_618767_e94a7d77b8229d8730374d15bb4843d8.webp"
width="760"
height="428"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
The Team
&lt;/figcaption>&lt;/figure>--></description></item><item><title>Invited Talk at MLBriefs</title><link>https://drsandor.net/project/ipol/</link><pubDate>Fri, 31 May 2024 14:00:00 +0000</pubDate><guid>https://drsandor.net/project/ipol/</guid><description/></item><item><title>Best XR Demo at ACM SIGGRAPH Asia</title><link>https://drsandor.net/project/sigaxr/</link><pubDate>Fri, 15 Dec 2023 16:00:00 +0000</pubDate><guid>https://drsandor.net/project/sigaxr/</guid><description/></item><item><title>Invited Talk in Beijing</title><link>https://drsandor.net/project/bit/</link><pubDate>Sun, 30 Oct 2022 16:00:00 +0000</pubDate><guid>https://drsandor.net/project/bit/</guid><description/></item><item><title>Invited Talk at Big Techday</title><link>https://drsandor.net/project/btd/</link><pubDate>Sat, 17 Sep 2022 16:00:00 +0000</pubDate><guid>https://drsandor.net/project/btd/</guid><description/></item><item><title>Keynote at MediaFutures</title><link>https://drsandor.net/project/bergen/</link><pubDate>Sat, 17 Sep 2022 16:00:00 +0000</pubDate><guid>https://drsandor.net/project/bergen/</guid><description/></item><item><title>Keynote at XR Salento</title><link>https://drsandor.net/project/salento/</link><pubDate>Fri, 22 Apr 2022 14:00:00 +0000</pubDate><guid>https://drsandor.net/project/salento/</guid><description/></item><item><title>Talk at CollÃ¨ge de France</title><link>https://drsandor.net/project/cdf/</link><pubDate>Fri, 22 Apr 2022 14:00:00 +0000</pubDate><guid>https://drsandor.net/project/cdf/</guid><description>&lt;p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p>
&lt;p>Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p>
&lt;p>Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p>
&lt;p>Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p>
&lt;p>Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p></description></item><item><title>Welcome to Wowchemy, the website builder for Hugo</title><link>https://drsandor.net/post/getting-started/</link><pubDate>Sun, 13 Dec 2020 00:00:00 +0000</pubDate><guid>https://drsandor.net/post/getting-started/</guid><description>&lt;h2 id="overview">Overview&lt;/h2>
&lt;ol>
&lt;li>The Wowchemy website builder for Hugo, along with its starter templates, is designed for professional creators, educators, and teams/organizations - although it can be used to create any kind of site&lt;/li>
&lt;li>The template can be modified and customised to suit your needs. It&amp;rsquo;s a good platform for anyone looking to take control of their data and online identity whilst having the convenience to start off with a &lt;strong>no-code solution (write in Markdown and customize with YAML parameters)&lt;/strong> and having &lt;strong>flexibility to later add even deeper personalization with HTML and CSS&lt;/strong>&lt;/li>
&lt;li>You can work with all your favourite tools and apps with hundreds of plugins and integrations to speed up your workflows, interact with your readers, and much more&lt;/li>
&lt;/ol>
&lt;p>&lt;a href="https://wowchemy.com" target="_blank" rel="noopener">
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://raw.githubusercontent.com/wowchemy/wowchemy-hugo-modules/main/starters/academic/preview.png" alt="The template is mobile first with a responsive design to ensure that your site looks stunning on every device." loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/a>&lt;/p>
&lt;h2 id="get-started">Get Started&lt;/h2>
&lt;ul>
&lt;li>ðŸ‘‰ &lt;a href="https://wowchemy.com/templates/" target="_blank" rel="noopener">&lt;strong>Create a new site&lt;/strong>&lt;/a>&lt;/li>
&lt;li>ðŸ“š &lt;a href="https://wowchemy.com/docs/" target="_blank" rel="noopener">&lt;strong>Personalize your site&lt;/strong>&lt;/a>&lt;/li>
&lt;li>ðŸ’¬ &lt;a href="https://discord.gg/z8wNYzb" target="_blank" rel="noopener">Chat with the &lt;strong>Wowchemy community&lt;/strong>&lt;/a> or &lt;a href="https://discourse.gohugo.io" target="_blank" rel="noopener">&lt;strong>Hugo community&lt;/strong>&lt;/a>&lt;/li>
&lt;li>ðŸ¦ Twitter: &lt;a href="https://twitter.com/wowchemy" target="_blank" rel="noopener">@wowchemy&lt;/a> &lt;a href="https://twitter.com/GeorgeCushen" target="_blank" rel="noopener">@GeorgeCushen&lt;/a> &lt;a href="https://twitter.com/search?q=%23MadeWithWowchemy&amp;amp;src=typed_query" target="_blank" rel="noopener">#MadeWithWowchemy&lt;/a>&lt;/li>
&lt;li>ðŸ’¡ &lt;a href="https://github.com/wowchemy/wowchemy-hugo-themes/issues" target="_blank" rel="noopener">Request a &lt;strong>feature&lt;/strong> or report a &lt;strong>bug&lt;/strong> for &lt;em>Wowchemy&lt;/em>&lt;/a>&lt;/li>
&lt;li>â¬†ï¸ &lt;strong>Updating Wowchemy?&lt;/strong> View the &lt;a href="https://wowchemy.com/docs/hugo-tutorials/update/" target="_blank" rel="noopener">Update Tutorial&lt;/a> and &lt;a href="https://wowchemy.com/updates/" target="_blank" rel="noopener">Release Notes&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="crowd-funded-open-source-software">Crowd-funded open-source software&lt;/h2>
&lt;p>To help us develop this template and software sustainably under the MIT license, we ask all individuals and businesses that use it to help support its ongoing maintenance and development via sponsorship.&lt;/p>
&lt;h3 id="-click-here-to-become-a-sponsor-and-help-support-wowchemys-future-httpswowchemycomsponsor">&lt;a href="https://wowchemy.com/sponsor/" target="_blank" rel="noopener">â¤ï¸ Click here to become a sponsor and help support Wowchemy&amp;rsquo;s future â¤ï¸&lt;/a>&lt;/h3>
&lt;p>As a token of appreciation for sponsoring, you can &lt;strong>unlock &lt;a href="https://wowchemy.com/sponsor/" target="_blank" rel="noopener">these&lt;/a> awesome rewards and extra features ðŸ¦„âœ¨&lt;/strong>&lt;/p>
&lt;h2 id="ecosystem">Ecosystem&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>&lt;a href="https://github.com/wowchemy/hugo-academic-cli" target="_blank" rel="noopener">Hugo Academic CLI&lt;/a>:&lt;/strong> Automatically import publications from BibTeX&lt;/li>
&lt;/ul>
&lt;h2 id="inspiration">Inspiration&lt;/h2>
&lt;p>&lt;a href="https://academic-demo.netlify.com/" target="_blank" rel="noopener">Check out the latest &lt;strong>demo&lt;/strong>&lt;/a> of what you&amp;rsquo;ll get in less than 10 minutes, or &lt;a href="https://wowchemy.com/user-stories/" target="_blank" rel="noopener">view the &lt;strong>showcase&lt;/strong>&lt;/a> of personal, project, and business sites.&lt;/p>
&lt;h2 id="features">Features&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>Page builder&lt;/strong> - Create &lt;em>anything&lt;/em> with &lt;a href="https://wowchemy.com/docs/page-builder/" target="_blank" rel="noopener">&lt;strong>widgets&lt;/strong>&lt;/a> and &lt;a href="https://wowchemy.com/docs/content/writing-markdown-latex/" target="_blank" rel="noopener">&lt;strong>elements&lt;/strong>&lt;/a>&lt;/li>
&lt;li>&lt;strong>Edit any type of content&lt;/strong> - Blog posts, publications, talks, slides, projects, and more!&lt;/li>
&lt;li>&lt;strong>Create content&lt;/strong> in &lt;a href="https://wowchemy.com/docs/content/writing-markdown-latex/" target="_blank" rel="noopener">&lt;strong>Markdown&lt;/strong>&lt;/a>, &lt;a href="https://wowchemy.com/docs/import/jupyter/" target="_blank" rel="noopener">&lt;strong>Jupyter&lt;/strong>&lt;/a>, or &lt;a href="https://wowchemy.com/docs/install-locally/" target="_blank" rel="noopener">&lt;strong>RStudio&lt;/strong>&lt;/a>&lt;/li>
&lt;li>&lt;strong>Plugin System&lt;/strong> - Fully customizable &lt;a href="https://wowchemy.com/docs/customization/" target="_blank" rel="noopener">&lt;strong>color&lt;/strong> and &lt;strong>font themes&lt;/strong>&lt;/a>&lt;/li>
&lt;li>&lt;strong>Display Code and Math&lt;/strong> - Code highlighting and &lt;a href="https://en.wikibooks.org/wiki/LaTeX/Mathematics" target="_blank" rel="noopener">LaTeX math&lt;/a> supported&lt;/li>
&lt;li>&lt;strong>Integrations&lt;/strong> - &lt;a href="https://analytics.google.com" target="_blank" rel="noopener">Google Analytics&lt;/a>, &lt;a href="https://disqus.com" target="_blank" rel="noopener">Disqus commenting&lt;/a>, Maps, Contact Forms, and more!&lt;/li>
&lt;li>&lt;strong>Beautiful Site&lt;/strong> - Simple and refreshing one page design&lt;/li>
&lt;li>&lt;strong>Industry-Leading SEO&lt;/strong> - Help get your website found on search engines and social media&lt;/li>
&lt;li>&lt;strong>Media Galleries&lt;/strong> - Display your images and videos with captions in a customizable gallery&lt;/li>
&lt;li>&lt;strong>Mobile Friendly&lt;/strong> - Look amazing on every screen with a mobile friendly version of your site&lt;/li>
&lt;li>&lt;strong>Multi-language&lt;/strong> - 34+ language packs including English, ä¸­æ–‡, and PortuguÃªs&lt;/li>
&lt;li>&lt;strong>Multi-user&lt;/strong> - Each author gets their own profile page&lt;/li>
&lt;li>&lt;strong>Privacy Pack&lt;/strong> - Assists with GDPR&lt;/li>
&lt;li>&lt;strong>Stand Out&lt;/strong> - Bring your site to life with animation, parallax backgrounds, and scroll effects&lt;/li>
&lt;li>&lt;strong>One-Click Deployment&lt;/strong> - No servers. No databases. Only files.&lt;/li>
&lt;/ul>
&lt;h2 id="themes">Themes&lt;/h2>
&lt;p>Wowchemy and its templates come with &lt;strong>automatic day (light) and night (dark) mode&lt;/strong> built-in. Alternatively, visitors can choose their preferred mode - click the moon icon in the top right of the &lt;a href="https://academic-demo.netlify.com/" target="_blank" rel="noopener">Demo&lt;/a> to see it in action! Day/night mode can also be disabled by the site admin in &lt;code>params.toml&lt;/code>.&lt;/p>
&lt;p>&lt;a href="https://wowchemy.com/docs/customization" target="_blank" rel="noopener">Choose a stunning &lt;strong>theme&lt;/strong> and &lt;strong>font&lt;/strong>&lt;/a> for your site. Themes are fully customizable.&lt;/p>
&lt;h2 id="license">License&lt;/h2>
&lt;p>Copyright 2016-present &lt;a href="https://georgecushen.com" target="_blank" rel="noopener">George Cushen&lt;/a>.&lt;/p>
&lt;p>Released under the &lt;a href="https://github.com/wowchemy/wowchemy-hugo-themes/blob/master/LICENSE.md" target="_blank" rel="noopener">MIT&lt;/a> license.&lt;/p></description></item><item><title>Slides</title><link>https://drsandor.net/slides/example/</link><pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate><guid>https://drsandor.net/slides/example/</guid><description>&lt;h1 id="create-slides-in-markdown-with-wowchemy">Create slides in Markdown with Wowchemy&lt;/h1>
&lt;p>&lt;a href="https://wowchemy.com/" target="_blank" rel="noopener">Wowchemy&lt;/a> | &lt;a href="https://wowchemy.com/docs/content/slides/" target="_blank" rel="noopener">Documentation&lt;/a>&lt;/p>
&lt;hr>
&lt;h2 id="features">Features&lt;/h2>
&lt;ul>
&lt;li>Efficiently write slides in Markdown&lt;/li>
&lt;li>3-in-1: Create, Present, and Publish your slides&lt;/li>
&lt;li>Supports speaker notes&lt;/li>
&lt;li>Mobile friendly slides&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="controls">Controls&lt;/h2>
&lt;ul>
&lt;li>Next: &lt;code>Right Arrow&lt;/code> or &lt;code>Space&lt;/code>&lt;/li>
&lt;li>Previous: &lt;code>Left Arrow&lt;/code>&lt;/li>
&lt;li>Start: &lt;code>Home&lt;/code>&lt;/li>
&lt;li>Finish: &lt;code>End&lt;/code>&lt;/li>
&lt;li>Overview: &lt;code>Esc&lt;/code>&lt;/li>
&lt;li>Speaker notes: &lt;code>S&lt;/code>&lt;/li>
&lt;li>Fullscreen: &lt;code>F&lt;/code>&lt;/li>
&lt;li>Zoom: &lt;code>Alt + Click&lt;/code>&lt;/li>
&lt;li>&lt;a href="https://revealjs.com/pdf-export/" target="_blank" rel="noopener">PDF Export&lt;/a>&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="code-highlighting">Code Highlighting&lt;/h2>
&lt;p>Inline code: &lt;code>variable&lt;/code>&lt;/p>
&lt;p>Code block:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="n">porridge&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s2">&amp;#34;blueberry&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">if&lt;/span> &lt;span class="n">porridge&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="s2">&amp;#34;blueberry&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;Eating...&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;hr>
&lt;h2 id="math">Math&lt;/h2>
&lt;p>In-line math: $x + y = z$&lt;/p>
&lt;p>Block math:&lt;/p>
&lt;p>$$
f\left( x \right) = ;\frac{{2\left( {x + 4} \right)\left( {x - 4} \right)}}{{\left( {x + 4} \right)\left( {x + 1} \right)}}
$$&lt;/p>
&lt;hr>
&lt;h2 id="fragments">Fragments&lt;/h2>
&lt;p>Make content appear incrementally&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">{{% fragment %}} One {{% /fragment %}}
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">{{% fragment %}} **Two** {{% /fragment %}}
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">{{% fragment %}} Three {{% /fragment %}}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Press &lt;code>Space&lt;/code> to play!&lt;/p>
&lt;span class="fragment " >
One
&lt;/span>
&lt;span class="fragment " >
&lt;strong>Two&lt;/strong>
&lt;/span>
&lt;span class="fragment " >
Three
&lt;/span>
&lt;hr>
&lt;p>A fragment can accept two optional parameters:&lt;/p>
&lt;ul>
&lt;li>&lt;code>class&lt;/code>: use a custom style (requires definition in custom CSS)&lt;/li>
&lt;li>&lt;code>weight&lt;/code>: sets the order in which a fragment appears&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="speaker-notes">Speaker Notes&lt;/h2>
&lt;p>Add speaker notes to your presentation&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-markdown" data-lang="markdown">&lt;span class="line">&lt;span class="cl">{{% speaker_note %}}
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">-&lt;/span> Only the speaker can read these notes
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">-&lt;/span> Press &lt;span class="sb">`S`&lt;/span> key to view
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> {{% /speaker_note %}}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Press the &lt;code>S&lt;/code> key to view the speaker notes!&lt;/p>
&lt;aside class="notes">
&lt;ul>
&lt;li>Only the speaker can read these notes&lt;/li>
&lt;li>Press &lt;code>S&lt;/code> key to view&lt;/li>
&lt;/ul>
&lt;/aside>
&lt;hr>
&lt;h2 id="themes">Themes&lt;/h2>
&lt;ul>
&lt;li>black: Black background, white text, blue links (default)&lt;/li>
&lt;li>white: White background, black text, blue links&lt;/li>
&lt;li>league: Gray background, white text, blue links&lt;/li>
&lt;li>beige: Beige background, dark text, brown links&lt;/li>
&lt;li>sky: Blue background, thin dark text, blue links&lt;/li>
&lt;/ul>
&lt;hr>
&lt;ul>
&lt;li>night: Black background, thick white text, orange links&lt;/li>
&lt;li>serif: Cappuccino background, gray text, brown links&lt;/li>
&lt;li>simple: White background, black text, blue links&lt;/li>
&lt;li>solarized: Cream-colored background, dark green text, blue links&lt;/li>
&lt;/ul>
&lt;hr>
&lt;section data-noprocess data-shortcode-slide
data-background-image="/media/boards.jpg"
>
&lt;h2 id="custom-slide">Custom Slide&lt;/h2>
&lt;p>Customize the slide style and background&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-markdown" data-lang="markdown">&lt;span class="line">&lt;span class="cl">{{&lt;span class="p">&amp;lt;&lt;/span> &lt;span class="nt">slide&lt;/span> &lt;span class="na">background-image&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s">&amp;#34;/media/boards.jpg&amp;#34;&lt;/span> &lt;span class="p">&amp;gt;&lt;/span>}}
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">{{&lt;span class="p">&amp;lt;&lt;/span> &lt;span class="nt">slide&lt;/span> &lt;span class="na">background-color&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s">&amp;#34;#0000FF&amp;#34;&lt;/span> &lt;span class="p">&amp;gt;&lt;/span>}}
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">{{&lt;span class="p">&amp;lt;&lt;/span> &lt;span class="nt">slide&lt;/span> &lt;span class="na">class&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s">&amp;#34;my-style&amp;#34;&lt;/span> &lt;span class="p">&amp;gt;&lt;/span>}}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;hr>
&lt;h2 id="custom-css-example">Custom CSS Example&lt;/h2>
&lt;p>Let&amp;rsquo;s make headers navy colored.&lt;/p>
&lt;p>Create &lt;code>assets/css/reveal_custom.css&lt;/code> with:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-css" data-lang="css">&lt;span class="line">&lt;span class="cl">&lt;span class="p">.&lt;/span>&lt;span class="nc">reveal&lt;/span> &lt;span class="nt">section&lt;/span> &lt;span class="nt">h1&lt;/span>&lt;span class="o">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">.&lt;/span>&lt;span class="nc">reveal&lt;/span> &lt;span class="nt">section&lt;/span> &lt;span class="nt">h2&lt;/span>&lt;span class="o">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">.&lt;/span>&lt;span class="nc">reveal&lt;/span> &lt;span class="nt">section&lt;/span> &lt;span class="nt">h3&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">color&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="kc">navy&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;hr>
&lt;h1 id="questions">Questions?&lt;/h1>
&lt;p>&lt;a href="https://discord.gg/z8wNYzb" target="_blank" rel="noopener">Ask&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://wowchemy.com/docs/content/slides/" target="_blank" rel="noopener">Documentation&lt;/a>&lt;/p></description></item><item><title>An example conference paper</title><link>https://drsandor.net/publication/example/</link><pubDate>Mon, 01 Jul 2013 00:00:00 +0000</pubDate><guid>https://drsandor.net/publication/example/</guid><description>&lt;div class="alert alert-note">
&lt;div>
Click the &lt;em>Cite&lt;/em> button above to demo the feature to enable visitors to import publication metadata into their reference management software.
&lt;/div>
&lt;/div>
&lt;div class="alert alert-note">
&lt;div>
Create your slides in Markdown - click the &lt;em>Slides&lt;/em> button to check out the example.
&lt;/div>
&lt;/div>
&lt;p>Supplementary notes can be added here, including &lt;a href="https://wowchemy.com/docs/writing-markdown-latex/" target="_blank" rel="noopener">code, math, and images&lt;/a>.&lt;/p></description></item><item><title/><link>https://drsandor.net/admin/config.yml</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://drsandor.net/admin/config.yml</guid><description/></item><item><title>Chess</title><link>https://drsandor.net/chess/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://drsandor.net/chess/</guid><description>&lt;p>One of my biggest passions is chess. In my teenager years, I had the honor to work with &lt;a href="https://en.wikipedia.org/wiki/Tibor_K%c3%a1rolyi_%28chess_player%29" target="_blank" rel="noopener">IM Tibor Karolyi&lt;/a>. As a result, I made rapid improvements.&lt;/p>
&lt;figure id="figure-player-profile-on-chessbasecomhttpsplayerschessbasecomenplayersandor_christian228735">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="[Player profile on chessbase.com](https://players.chessbase.com/en/player/Sandor_Christian/228735)" srcset="
/media/chessbase_hu25c06c04fc8bb7f83c5b1604c7d3e123_680000_4463c112e80f47fcefb85d88b603b971.webp 400w,
/media/chessbase_hu25c06c04fc8bb7f83c5b1604c7d3e123_680000_991e1d576ff562dcc25b4532300c28e5.webp 760w,
/media/chessbase_hu25c06c04fc8bb7f83c5b1604c7d3e123_680000_1200x1200_fit_q75_h2_lanczos_3.webp 1200w"
src="https://drsandor.net/media/chessbase_hu25c06c04fc8bb7f83c5b1604c7d3e123_680000_4463c112e80f47fcefb85d88b603b971.webp"
width="760"
height="383"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
&lt;a href="https://players.chessbase.com/en/player/Sandor_Christian/228735" target="_blank" rel="noopener">Player profile on chessbase.com&lt;/a>
&lt;/figcaption>&lt;/figure>
&lt;p>In 1994, I became blitz champion of Bavaria in the adult category (Bayerischer Meister im Blitzschach). I also managed to win a serious game against &lt;a href="https://en.wikipedia.org/wiki/Robert_H%c3%bcbner" target="_blank" rel="noopener">GM Robert HÃ¼bner&lt;/a>, the former number 3 in the world rankings. You can read a commented version of this game as a pdf &lt;a href="sandor-hubner.pdf">here&lt;/a>.&lt;/p>
&lt;p>I was then accepted as a member of the special chess unit in the German Army (SportfÃ¶rdergruppe Schach). While being exempted from regular duty, I got trained by former world champion candidate &lt;a href="https://en.wikipedia.org/wiki/Klaus_Darga" target="_blank" rel="noopener">GM Klaus Darga&lt;/a>. In 1996, I received the title International Master (IM) from FIDE, the world chess federation. Soon after, I decided to pursue chess only as a hobby.&lt;/p></description></item></channel></rss>