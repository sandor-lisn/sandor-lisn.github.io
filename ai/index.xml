<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>AI Experiments | Christian Sandor</title><link>https://drsandor.net/ai/</link><atom:link href="https://drsandor.net/ai/index.xml" rel="self" type="application/rss+xml"/><description>AI Experiments</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Mon, 23 Jun 2025 14:00:00 +0000</lastBuildDate><image><url>https://drsandor.net/media/icon_hu287fcf417612116bcf0d00ac1ba165d7_82622_512x512_fill_lanczos_center_3.png</url><title>AI Experiments</title><link>https://drsandor.net/ai/</link></image><item><title>Short Animation with Wan Video, Flux Kontext, and DeepSeek</title><link>https://drsandor.net/ai/minecraft/</link><pubDate>Mon, 23 Jun 2025 14:00:00 +0000</pubDate><guid>https://drsandor.net/ai/minecraft/</guid><description>&lt;p>My daughter Kate (7 years old) really loves Minecraft! Together, we used several generative AI tools to create a 1-minute animation based on only 1 input photo of her. The whole project took around 20 hours of work and I learned several lessons that I want to share here.&lt;/p>
&lt;h3 id="context">Context&lt;/h3>
&lt;p>I am still trying to get used to the enormous speed with which generative AI is progressing. 6 months ago, I was blogging about &lt;a href="https://drsandor.net/ai/good-bad-ugly">my experiments with Tencent&amp;rsquo;s Hunyuan Video&lt;/a>, which was an absolute breakthrough at that time. A lot has changed since then! The open-weights generative AI community has fully embraced Alibaba&amp;rsquo;s Wan Video as a superior replacement for Hunyuan. What makes WAN so powerful is that several extensions have been shared openly:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/Wan-Video/Wan2.1" target="_blank" rel="noopener">Alibaba themselves&lt;/a> have released a zoo of base models with different capabilities: text to video, image to video, first-and-last-frame to video, Wan-Fun-Control accepts a variety of conditioning inputs&lt;/li>
&lt;li>LORAs: A wide range have been trained. Compatibility between LORAs and the various base models is complicated.&lt;/li>
&lt;li>&lt;a href="https://github.com/ali-vilab/VACE" target="_blank" rel="noopener">VACE&lt;/a>: powerful control of generated videos&lt;/li>
&lt;li>&lt;a href="https://causvid.github.io" target="_blank" rel="noopener">CausVid&lt;/a> and its successor &lt;a href="https://self-forcing.github.io" target="_blank" rel="noopener">SelfForcing&lt;/a>: incredible speed gains&lt;/li>
&lt;li>&amp;hellip;and this is just the tip of the iceberg!&lt;/li>
&lt;/ul>
&lt;h3 id="experiment-summary">Experiment Summary&lt;/h3>
&lt;p>My goals for this experiment were:&lt;/p>
&lt;ul>
&lt;li>Have fun with Kate and involve her as much as possible in the conception of the story&lt;/li>
&lt;li>Play with the Wan Video ecosystem to produce a 1-minute animation&lt;/li>
&lt;li>There should only be 1 explicit real-world visual input: a photo of Kate wearing a tiger mask and a pink yukata.&lt;/li>
&lt;/ul>
&lt;p>In total, we spent about 20 hours on the project:&lt;/p>
&lt;ul>
&lt;li>1 hour: storyboarding&lt;/li>
&lt;li>4 hour: workflow building&lt;/li>
&lt;li>15 hours rendering&lt;/li>
&lt;li>1 hour: selecting best results&lt;/li>
&lt;/ul>
&lt;p>Out of the box, Wan can create clips of about 5 second length. For each of our 13 scenes, we rendered 4-8 variations (1-2 hours rendering time on an H100 GPU). We let it run over night on 2 H100s (15 GPU hours in total). Afterwards, we picked the best results among the variations, which was comparatively much quicker.&lt;/p>
&lt;p>In the remainder of the article, I describe the remaining 2 phases: storyboarding &amp;amp; workflow building.&lt;/p>
&lt;h3 id="storyboarding">Storyboarding&lt;/h3>
&lt;p>
&lt;figure id="figure-final-storyboard-made-with-flux-kontext">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Final Storyboard made with Flux Kontext" srcset="
/ai/minecraft/storyboard_hu71eebfa1490ae85140f5f5e02c9081c4_2127435_4f2db40b4c927cdee4274add7ee71872.webp 400w,
/ai/minecraft/storyboard_hu71eebfa1490ae85140f5f5e02c9081c4_2127435_c9d5eeac03606f18fa5302260df2c8e4.webp 760w,
/ai/minecraft/storyboard_hu71eebfa1490ae85140f5f5e02c9081c4_2127435_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://drsandor.net/ai/minecraft/storyboard_hu71eebfa1490ae85140f5f5e02c9081c4_2127435_4f2db40b4c927cdee4274add7ee71872.webp"
width="760"
height="428"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Final Storyboard made with Flux Kontext
&lt;/figcaption>&lt;/figure>
On the top-left was our input photo. We then developed the images for the storyboard one-by-one using &lt;a href="https://bfl.ai/announcements/flux-1-kontext" target="_blank" rel="noopener">Flux Kontext&lt;/a>, which has been widely noted for the image editing power that it gives users, while also being able to preserve character consistency across larger edits. My test very quickly convinced me that this is indeed a remarkable model.&lt;/p>
&lt;p>From the top-left onwards, prompts were as simple as:&lt;/p>
&lt;ul>
&lt;li>Make the character look like a pixelated minecraft character.&lt;/li>
&lt;li>Put the character into a lush green minecraft landscape.&lt;/li>
&lt;li>The chacter turns to the side and puts their hand on the top of the head of a minecraft villager&lt;/li>
&lt;/ul>
&lt;p>As Black Forest Labs currently keeps the community waiting for the open weights of this model, the only way for me to use it was to pay for API access (one of the very few instances where I paid for closed-source AI rather than to do it locally). But, costs were very fair (3 EUR for making the storyboard and playing some more with it).&lt;/p>
&lt;p>This phase of the project went much quicker than I would have expected, thanks to the amazing power of Flux Kontext.&lt;/p>
&lt;h3 id="workflow-building">Workflow Building&lt;/h3>
&lt;p>Next, I had to create a ComfyUI workflow that generates nice clips based on the storyboard. My first decision was to select the Wan foundation model. I opted for FLF2V-14B, which can generate clips based on the 3 inputs: the desired first and last frame, along with a text prompt describing the clip.&lt;/p>
&lt;p>Next, I invested some time into evaluating speedup methods for Wan, which is notoriously slow. The by far best method at the moment is &lt;a href="https://self-forcing.github.io" target="_blank" rel="noopener">Self Forcing&lt;/a>. Some helpful folks have extracted a simple LORA for it, which can be used in many different Wan workflows. In my tests, I could get clear and impressive speedups in the order of 5x. When iterating a low-qality preview video, it&amp;rsquo;s a huge win to wait 30 seconds (with Self Forcing) vs. 3 minutes (without).&lt;/p>
&lt;p>Unfortunately, I had to find out that the Self Forcing LORA I was using is incompatible with the FLF2V-14B model. The moment I turned on Self Forcing, prompts were ignored completely. I even tried translating my prompts into Chinese, as some users have noted that it works much better with Chinese rather than English prompts; an experience I could not replicate. With hindsight, I should have now abandoned FLF2V-14B at this point and should have switched to a workflow with the I2V-14B model for image-to-video and VACE-14B for control, all acceleratable with Self Forcing. Oh well, lessons learned&amp;hellip;&lt;/p>
&lt;p>The final missing puzzle piece is: how did I get the prompt to describe what happens in the clip between first and last frame? Following a related technique that David Snow shared on Banadoco, my gameplan was:&lt;/p>
&lt;ul>
&lt;li>Florence: describe what&amp;rsquo;s in the 2 images&lt;/li>
&lt;li>LLM: Based on these 2 descriptions, generate a prompt that describes what happens in between&lt;/li>
&lt;li>Render a few quick preview videos and edit prompt by hand if needed (only about 50% of scenes needed prompt editing)
This workflow worked like a charm!&lt;/li>
&lt;/ul>
&lt;p>The main time sink was that I fell into a little side-quest to catch up on LLMs. I tried the new contenders (DeepSeek R1-0528, Qwen 3, DeepSeek R1-0528-Qwen3-8B, etc.), based on 2 programming tests (flappy bird, balls inside a rotating haptagon). Flappy bird is too easy, and rotating heptagon is too hard (see below for how most models fail); I wonder what the sweet spot would be for testing?!&lt;/p>
&lt;video width="720" autoplay loop>
&lt;source src="balls.mp4" type="video/mp4" />
&lt;/video>
&lt;p>I almost liked Qwen 3, because of its fast reponse time, combined with 2 modes of operation (thinking vs. not-thinking). Unfortunately, the 32B model is somewhat underpowered, wheras the 235B model is too big to fit nicely on my H100. I wish they had a ~70B parameter model! In the end, I settled for my trusted DeepSeek-R1-Distill-Llama-70B-Q5_K_M.gguf, which has a good balance between speed and quality of replies. I still can&amp;rsquo;t express well how impressed I am with DeepSeek, raising a bad model (Llama3) to a great one!&lt;/p>
&lt;p>Next, I had to slightly edit David Snow&amp;rsquo;s prompt to arrive at:&lt;/p>
&lt;blockquote>
You are a creative video prompt generator specialized in crafting unique, vivid descriptions for ultra-short videos (approximately 5 seconds or 81 frames at 16fps). Your task is to generate one single highly detailed, imaginative prompt that can tell a complete visual story within this brief timeframe.
&lt;p>You will be given a description of two images. The phrase &amp;ldquo;&amp;ndash; end of image one &amp;ndash;&amp;rdquo; describes the first frame of the animation. Next follows a description of the last frame of the animation, delimited by the phrase &amp;ldquo;&amp;ndash; end of image two &amp;ndash;&amp;rdquo;. This description is immediately followed by a prompt that describes how to create a dynamic animation.&lt;/p>
&lt;p>Write a prompt to create a dynamic animation.&lt;/p>
&lt;p>Avoid verbose, overly complex language and focus on simple, clear English. Prompt for interesting movements of characters and the camera. Be creative and prompt for unexpected things. 160 tokens maximum.&lt;/p>
&lt;p>Be creative and unique with your transitions.&lt;/p>
&lt;/blockquote>
&lt;h3 id="results">Results&lt;/h3>
&lt;p>The end result is not very refined, but very respectable for a first draft. Imagine that someone had shown you this video a few years ago; how amazed would you have been by the production speed and cost!
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/xl8nnnACrFo" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;/p>
&lt;p>Below, I want to highlight a few scenes:
&lt;figure id="figure-the-llm-took-the-instructions-to-be-unique-and-creative-very-seriously-the-tigers-head-is-exploding-into-playing-cards">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="The LLM took the instructions to be unique and creative very seriously. The tiger&amp;#39;s head is exploding into playing cards." srcset="
/ai/minecraft/cards_hu1fd8a2c167e71c2fc6c2426a100de0a0_2669745_a65f36c05b2c32ce142be93409d4b1c9.webp 400w,
/ai/minecraft/cards_hu1fd8a2c167e71c2fc6c2426a100de0a0_2669745_c1986d3817584ca7e6f0d952a2ea4896.webp 760w,
/ai/minecraft/cards_hu1fd8a2c167e71c2fc6c2426a100de0a0_2669745_1200x1200_fit_q75_h2_lanczos_3.webp 1200w"
src="https://drsandor.net/ai/minecraft/cards_hu1fd8a2c167e71c2fc6c2426a100de0a0_2669745_a65f36c05b2c32ce142be93409d4b1c9.webp"
width="760"
height="699"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
The LLM took the instructions to be unique and creative very seriously. The tiger&amp;rsquo;s head is exploding into playing cards.
&lt;/figcaption>&lt;/figure>
&lt;figure id="figure-the-way-the-tiger-looks-at-the-dragon-is-just-too-cute">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="The way the tiger looks at the dragon is just too cute." srcset="
/ai/minecraft/dragon_hud5b48fe50b26838ec55d1e76ee5edc6a_3987406_9f7238159c44f608f54059ea2e158993.webp 400w,
/ai/minecraft/dragon_hud5b48fe50b26838ec55d1e76ee5edc6a_3987406_d9f4d9f863f1995bf6eb543de12c0ef9.webp 760w,
/ai/minecraft/dragon_hud5b48fe50b26838ec55d1e76ee5edc6a_3987406_1200x1200_fit_q75_h2_lanczos_3.webp 1200w"
src="https://drsandor.net/ai/minecraft/dragon_hud5b48fe50b26838ec55d1e76ee5edc6a_3987406_9f7238159c44f608f54059ea2e158993.webp"
width="760"
height="429"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
The way the tiger looks at the dragon is just too cute.
&lt;/figcaption>&lt;/figure>
&lt;figure id="figure-an-interesting-hallucination-at-some-point-in-the-movie-the-tiger-lost-its-wings-however-the-final-frame-from-the-storyboard-clearly-shows-a-wing-the-pipeline-worked-around-this-problem-by-making-a-bird-fly-in-and-morph-into-the-wings">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="An interesting hallucination. At some point in the movie, the tiger lost its wings. However, the final frame from the storyboard clearly shows a wing! The pipeline worked around this problem by making a bird fly in and morph into the wings!" srcset="
/ai/minecraft/wing_hu563eb47fe584df5923becb564682076a_96100_2315c9ea7c54af6d9ad2ba93722674c7.webp 400w,
/ai/minecraft/wing_hu563eb47fe584df5923becb564682076a_96100_e8154f88623fbc98f26a2098252e197f.webp 760w,
/ai/minecraft/wing_hu563eb47fe584df5923becb564682076a_96100_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://drsandor.net/ai/minecraft/wing_hu563eb47fe584df5923becb564682076a_96100_2315c9ea7c54af6d9ad2ba93722674c7.webp"
width="760"
height="292"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
An interesting hallucination. At some point in the movie, the tiger lost its wings. However, the final frame from the storyboard clearly shows a wing! The pipeline worked around this problem by making a bird fly in and morph into the wings!
&lt;/figcaption>&lt;/figure>&lt;/p>
&lt;h3 id="lessons-learned">Lessons Learned&lt;/h3>
&lt;ul>
&lt;li>Self Forcing is great and is strongly recommended to speed up generation time.&lt;/li>
&lt;li>Wan FLF2V-14B is a shaky model. Next time, I would rather use I2V-14B with VACE because of the incompatibility with Self Forcing&lt;/li>
&lt;li>I can&amp;rsquo;t wait for the Flux Kontext weights to be released!&lt;/li>
&lt;li>Adobe Research: Great job with Self Forcing! I can&amp;rsquo;t wait for the next iteration of your work.&lt;/li>
&lt;/ul></description></item><item><title>Keynote at ADOS Paris</title><link>https://drsandor.net/ai/ados/</link><pubDate>Thu, 10 Apr 2025 14:00:00 +0000</pubDate><guid>https://drsandor.net/ai/ados/</guid><description>&lt;p>On 28 March 2025, I attended the amazing &lt;a href="https://www.lightricks.com/ados" target="_blank" rel="noopener">ADOS&lt;/a> event in Paris at &lt;a href="https://www.artifex-lab.com" target="_blank" rel="noopener">Artifex Lab&lt;/a>, a new space in Paris for bringing together AI arists and technologists. ADOS was co-organized by &lt;a href="https://banodoco.ai" target="_blank" rel="noopener">Banadoco&lt;/a> and &lt;a href="https://www.lightricks.com" target="_blank" rel="noopener">Lightricks&lt;/a>, which was an ideal fit for Artifex Lab: Banadoco is the leading online community for AI art+technology and Lightricks&amp;rsquo;s LTX is the fastest generative AI video model (and it&amp;rsquo;s Open Source on top of that!).&lt;/p>
&lt;p>Coincidentally, our team has recently been investigating LTX deeply and also has been hanging out a lot on Banadoco&amp;rsquo;s inspiring Discord server. One thing led to another and I was invited to present a keynote at ADOS. We brought most of the team, as well as a bleeding edge demo. Unfortuntately, I missed out on the second day of the event, which featured a hackathon, where our PhD student Hovhannes received a prize (see his upcoming blogpost on our &lt;a href="https://ar-ai.org" target="_blank" rel="noopener">team webpage&lt;/a>).&lt;/p>
&lt;p>
&lt;figure id="figure-team-arai-at-ados">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Team ARAI at ADOS" srcset="
/media/ados/team_hu3a541a33e063e69d3b30aeefb30179f7_534763_b783a900c056276757e7f7017585a4b8.webp 400w,
/media/ados/team_hu3a541a33e063e69d3b30aeefb30179f7_534763_4448afd20a1677241c97a762bfe02851.webp 760w,
/media/ados/team_hu3a541a33e063e69d3b30aeefb30179f7_534763_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://drsandor.net/media/ados/team_hu3a541a33e063e69d3b30aeefb30179f7_534763_b783a900c056276757e7f7017585a4b8.webp"
width="760"
height="570"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Team ARAI at ADOS
&lt;/figcaption>&lt;/figure>
&lt;figure id="figure-zeev-farbman-co-founder--ceo-of-lightricks-trying-our-demo">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Zeev Farbman, Co-Founder &amp;amp; CEO of Lightricks trying our demo" srcset="
/media/ados/demo_hua7d9fbdda16f2d02e5b664a9fb28b240_541462_3cc95a501fbfe4de2ff237a29e99ded4.webp 400w,
/media/ados/demo_hua7d9fbdda16f2d02e5b664a9fb28b240_541462_007047a2ff3dd4aaa738306758667246.webp 760w,
/media/ados/demo_hua7d9fbdda16f2d02e5b664a9fb28b240_541462_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://drsandor.net/media/ados/demo_hua7d9fbdda16f2d02e5b664a9fb28b240_541462_3cc95a501fbfe4de2ff237a29e99ded4.webp"
width="760"
height="570"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Zeev Farbman, Co-Founder &amp;amp; CEO of Lightricks trying our demo
&lt;/figcaption>&lt;/figure>&lt;/p>
&lt;p>Before reporting about the keynote and demo, I would like to highlight some of my favourite things that I saw at the event. All talks are &lt;a href="https://www.youtube.com/watch?v=gBeZSbaxMvc" target="_blank" rel="noopener">recorded on youtube&lt;/a>; I provide direct links to the talks in the following.&lt;/p>
&lt;p>I enjoyed &lt;a href="https://www.youtube.com/live/gBeZSbaxMvc?si=orYRKkprSBqWWwBo&amp;amp;t=468" target="_blank" rel="noopener">Emma Catnip&amp;rsquo;s talk&lt;/a>, particularly her work Bloom. It is very soulful and beautifully designed. I will never forget her bitter-sweet explanation: &amp;ldquo;It&amp;rsquo;s about the summer of love I that never had&amp;rdquo;.&lt;/p>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/cDUExDUYRaE" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;p>Another very interesting artist&amp;rsquo;s talk was by &lt;a href="https://udart.dk" target="_blank" rel="noopener">Vibeke Bertelsen (Udart)&lt;/a>; &lt;a href="https://www.youtube.com/live/gBeZSbaxMvc?si=r3DAzpV5a7a7C8jd&amp;amp;t=8304" target="_blank" rel="noopener">timestamp&lt;/a>. Imagine current AI tools in the hands of H.R. Giger&amp;hellip; Below is one of her award-winning short movies. Definitely most unique!&lt;/p>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/8Vodtg8WNiY" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;p>On the tech side, my favourite talk was by &lt;a href="https://github.com/yvann-ba" target="_blank" rel="noopener">Yvann Barbot&lt;/a>. In &lt;a href="https://www.youtube.com/live/gBeZSbaxMvc?si=utqO7-YM0Oq1c5Fx&amp;amp;t=8960" target="_blank" rel="noopener">his talk&lt;/a>, he showed how easy it is to generate cool audio-reactive video clips using his ComfyUI nodes. One example below by &lt;a href="https://www.youtube.com/@visualfrisson" target="_blank" rel="noopener">Visual Frisson&lt;/a>. After the event, I checked Yvann&amp;rsquo;s youtube channel. I can highly recommend it!&lt;/p>
&lt;video width="720" controls>
&lt;source src="frisson.mp4" type="video/mp4" />
&lt;/video>
&lt;p>Finally, let me get to my keynote talk and our demo.&lt;/p>
&lt;p>Due to the extreme technical difficulties involved in our demo, we were working literally till the last minute on it. Chapeau to Dávid Maruscsák and Francesco Dettori for their very hard work near demo time! Very unfortunately, the backend of our demo stopped working 5 minutes before my talk started. As a result, we could not show the demo during my keynote (as we had originally planned), but only 2 hours later. Luckily, most attendants of ADOS were still there and could give it a try!&lt;/p>
&lt;p>We demonstrate how to use a cloud backend for highspeed generation of spatial videos based on image and text inputs. You can see a similar version of the demo below; compared to the demo we showed at ADOS, it is only using 1 local RTX 4090 GPU for inference, as opposed to 4 H100 GPUs at ADOS, so inference speed is significantly lower.&lt;/p>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/Gk2dtoOXDac" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;p>You can see my keynote on &lt;a href="https://www.youtube.com/live/gBeZSbaxMvc?si=qr5cMNz5WXu8Jt7d&amp;amp;t=7061" target="_blank" rel="noopener">youtube&lt;/a>, where I speak about my long past with AR and my more recent past with AI, and how everything converged into this demo :-)&lt;/p>
&lt;p>To conclude, I would like to sincerely thank:&lt;/p>
&lt;ul>
&lt;li>The &lt;a href="https://www.lightricks.com/ados" target="_blank" rel="noopener">ADOS&lt;/a> organizational team: &lt;a href="https://banodoco.ai" target="_blank" rel="noopener">Banadoco&lt;/a>, &lt;a href="https://www.lightricks.com" target="_blank" rel="noopener">Lightricks&lt;/a>, and &lt;a href="https://www.artifex-lab.com" target="_blank" rel="noopener">Artifex Lab&lt;/a>.&lt;/li>
&lt;li>Special thanks to Lightricks for giving us early access to their distilled LTX model, which enabled impressive speedups.&lt;/li>
&lt;li>All &lt;a href="https://ar-ai.org/people/" target="_blank" rel="noopener">ARAI team members&lt;/a> for bearing with my demands :-) Directly involved in the demo were Cyrille Leroux, Thomas Betton, Dávid Maruscsák, Francesco Dettori, and Hovhannes Margaryan.&lt;/li>
&lt;/ul></description></item><item><title>AI Video Generation - The Future is already here</title><link>https://drsandor.net/ai/good-bad-ugly/</link><pubDate>Wed, 18 Dec 2024 14:00:00 +0000</pubDate><guid>https://drsandor.net/ai/good-bad-ugly/</guid><description>&lt;p>I concluded my last AI blog post in October with &amp;ldquo;open source video
generation models&amp;hellip; will overtake closed sourced ones sooner than
expected!&amp;rdquo;. &lt;em>Surprise: it has already happened!&lt;/em>&lt;/p>
&lt;p>In this lengthy blogpost, I want to:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Explain the &lt;em>enormous work&lt;/em> that we did over the &lt;em>last 10 days&lt;/em>, resulting in a funny AI
movie (jump to the end if you are only here for that)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Explain the &lt;em>big picture&lt;/em> of generative AI for videos&lt;/p>
&lt;/li>
&lt;/ul>
&lt;details class="toc-inpage d-print-none " open>
&lt;summary class="font-weight-bold">Table of Contents&lt;/summary>
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-the-ideal-video-model">1. The Ideal Video Model&lt;/a>
&lt;ul>
&lt;li>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#2-current-video-models">2. Current Video Models&lt;/a>&lt;/li>
&lt;li>&lt;a href="#3-case-study-ai-remake-of-the-good-the-bad-and-the-ugly">3. Case Study: AI Remake of &amp;ldquo;The Good, The Bad, and The Ugly&amp;rdquo;&lt;/a>
&lt;ul>
&lt;li>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/details>
&lt;p>Since my last blog post, a ton of generative AI video models have been
released and I have tested the most important ones. To be upfront with
my conclusion: &lt;a href="https://github.com/Tencent/HunyuanVideo" target="_blank" rel="noopener">Tencent&amp;rsquo;s Hunyuan Video&lt;/a>
is &lt;em>absolutely astonishing&lt;/em> and
will have wide impact. The last time I had this feeling was when Stable
Diffusion came out in 2022, which was the foundation for successful
commercial products such as Midjourney and is still being used widely
(e.g. the new Mountain Dew commercials are made with it). I am convinced:
&lt;em>Hunyuan will be for video generation what Stable Diffusion has been
for image generation.&lt;/em>&lt;/p>
&lt;h2 id="1-the-ideal-video-model">1. The Ideal Video Model&lt;/h2>
&lt;p>Obviously, we want video models to be expressive and generate videos of
high resolution and framerate. Some not-so-obvious requirements include:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;em>Open source:&lt;/em> model can be downloaded and run locally&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;em>Uncensored:&lt;/em> censored models lead to many problems&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Allow the development of an &lt;em>ecosystem of tools&lt;/em> around it&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>Following the analogy to Stable Diffusion I drew earlier: all of these
were true for it and were key to why it won.&lt;/p>
&lt;h4 id="model-censorship">Model Censorship&lt;/h4>
&lt;p>In general, this is a difficult question. On the one hand, I concur that
some extreme content should simply not exist at all, so models should
not be able to generate it. On the other hand, over-censoring of models
has some unintended side effects.&lt;/p>
&lt;p>Earlier this year, Google&amp;rsquo;s image generation model Gemini was &lt;a href="https://www.nytimes.com/2024/02/22/technology/google-gemini-german-uniforms.html" target="_blank" rel="noopener">widely ridiculed&lt;/a> when a
prompt for &amp;ldquo;Nazi soldiers&amp;rdquo; would generate Black and Asian soldiers. When
OpenAI released DALLE in 2022, I tested it with the prompt &amp;ldquo;My Bavarian
mother unintentionally takes a selfie with her mobile phone&amp;rdquo;, also
returning me a selection of ethnicities in the results, which are unlikely
to match what I actually want. What a bad user experience!&lt;/p>
&lt;p>This year, Stability AI released Stable Diffusion 3, which the community
had been waiting for in great anticipation. It quickly turned out that this
model is quite useless. The training data only contained a very small
amount of lightly-dressed humans, leading to a total incomprehension of
human anatomy. The prompt &amp;ldquo;woman lying on grass&amp;rdquo; quickly became a meme.&lt;/p>
&lt;figure id="figure-woman-lying-on-grass-according-to-stable-diffusion-3httpsstabilityainewsstable-diffusion-3">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="&amp;#39;&amp;#39;Woman lying on grass&amp;#39;, according to [Stable Diffusion 3](https://stability.ai/news/stable-diffusion-3)" srcset="
/ai/good-bad-ugly/1_hu9c3d30bf53313f395dc9309d4b7f68f9_2061541_4736902eab9f0ecd57bf02bf8b4ee839.webp 400w,
/ai/good-bad-ugly/1_hu9c3d30bf53313f395dc9309d4b7f68f9_2061541_5acf54ae7b89121f468fb098ea842425.webp 760w,
/ai/good-bad-ugly/1_hu9c3d30bf53313f395dc9309d4b7f68f9_2061541_1200x1200_fit_q75_h2_lanczos_3.webp 1200w"
src="https://drsandor.net/ai/good-bad-ugly/1_hu9c3d30bf53313f395dc9309d4b7f68f9_2061541_4736902eab9f0ecd57bf02bf8b4ee839.webp"
width="760"
height="430"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption data-pre="Figure&amp;nbsp;" data-post=":&amp;nbsp;" class="numbered">
&amp;lsquo;&amp;lsquo;Woman lying on grass&amp;rsquo;, according to &lt;a href="https://stability.ai/news/stable-diffusion-3" target="_blank" rel="noopener">Stable Diffusion 3&lt;/a>
&lt;/figcaption>&lt;/figure>
&lt;h4 id="ecosystem">Ecosystem&lt;/h4>
&lt;p>In my last blog post, I showed how I built a pipeline out of several
open source models. This ability is essential for many use cases,
including professional media production and research. It also
supports the creation of an ecosystem of supporting tools (more on this below).&lt;/p>
&lt;p>The models developed by big Silicon Valley companies like Meta, Google,
and OpenAI don&amp;rsquo;t support such use cases and probably don&amp;rsquo;t even want to;
they are happy if users can modify a video they took in a café to show a
Kombucha instead of a Coke or an Avocado Toast instead of a Hamburger.&lt;/p>
&lt;p>An ecosystem of tools around foundation models is critical, because it
allows, among other things, the very precise &lt;em>specification of outputs&lt;/em>.
Simply speaking, one of the most useless ways to specify the image or
video that I want to be generated is by text. Much more useful are:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;em>Controlnets:&lt;/em> can specify output by visual constraints (colors, body
poses, depth information, edge information etc.)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;em>IPAdapters:&lt;/em> can be used to transfer non-tangible information, such
as the style of an image&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;em>LORAs:&lt;/em> a more heavyweight approach than IPAdapters that allows much
more control of styles, materials, and humans in the output&lt;/p>
&lt;/li>
&lt;/ul>
&lt;figure id="figure-left-behind-the-scenes-of-a-video-i-posted-on-linkedin-in-022024httpswwwlinkedincompostsdr-christian-sandor-b7240890_i-did-some-dance-training-over-the-holidays-activity-7156976991240355840-28_outm_sourceshareutm_mediummember_desktop-using-a-lora-i-had-trained-on-my-appearance-and-a-controlnet-for-body-pose-right-my-coworker-huyenhttpsar-aiorgauthorhuyen-nguyen-generated--using-an-ipadapter-conditioned-with-4-photos-of-her">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Left: behind the scenes of a video I [posted on Linkedin in 02/2024](https://www.linkedin.com/posts/dr-christian-sandor-b7240890_i-did-some-dance-training-over-the-holidays-activity-7156976991240355840-28_O?utm_source=share&amp;amp;utm_medium=member_desktop), using a LORA I had trained on my appearance and a controlnet for body pose. Right: my coworker [Huyen](https://ar-ai.org/author/huyen-nguyen/), generated using an IPAdapter conditioned with 4 photos of her." srcset="
/ai/good-bad-ugly/2_hufba5692dd20cec5e2bc1458418e9c891_1409682_c5752d928a33c7648640a778276f4fef.webp 400w,
/ai/good-bad-ugly/2_hufba5692dd20cec5e2bc1458418e9c891_1409682_0ee009d1ae63cc7e8937dc82134f59fa.webp 760w,
/ai/good-bad-ugly/2_hufba5692dd20cec5e2bc1458418e9c891_1409682_1200x1200_fit_q75_h2_lanczos_3.webp 1200w"
src="https://drsandor.net/ai/good-bad-ugly/2_hufba5692dd20cec5e2bc1458418e9c891_1409682_c5752d928a33c7648640a778276f4fef.webp"
width="760"
height="450"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption data-pre="Figure&amp;nbsp;" data-post=":&amp;nbsp;" class="numbered">
Left: behind the scenes of a video I &lt;a href="https://www.linkedin.com/posts/dr-christian-sandor-b7240890_i-did-some-dance-training-over-the-holidays-activity-7156976991240355840-28_O?utm_source=share&amp;amp;utm_medium=member_desktop" target="_blank" rel="noopener">posted on Linkedin in 02/2024&lt;/a>, using a LORA I had trained on my appearance and a controlnet for body pose. Right: my coworker &lt;a href="https://ar-ai.org/author/huyen-nguyen/" target="_blank" rel="noopener">Huyen&lt;/a>, generated using an IPAdapter conditioned with 4 photos of her.
&lt;/figcaption>&lt;/figure>
&lt;h2 id="2-current-video-models">2. Current Video Models&lt;/h2>
&lt;p>Since my last AI blog post, I consider the most important models that
have been released: LTX-Video (1 month ago), Hunyuan (3 weeks ago), and
many additions to CogVideoX. Their main strengths are:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;em>LTX:&lt;/em> Lightning fast&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;em>CogVideoX:&lt;/em> Extreme controllability&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;em>Hunyuan:&lt;/em> Best visual quality&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>First, let me show an interactive session of me with &lt;em>LTX video&lt;/em>, captured
in real-time from my screen. The speed is absolutely mind-blowing!&lt;/p>
&lt;video width="720" autoplay loop>
&lt;source src="ltx.mp4" type="video/mp4" />
&lt;/video>
&lt;p>The &lt;em>CogVideoX&lt;/em> family stands out by how many things can be controlled
about the videos that are generated, including:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Using images to specify the start &amp;amp; end of the video&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Define trajectories of objects in the video&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Exact specification of camera movements&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Using LORAs, we can already specify styles like &amp;ldquo;Disney Movie&amp;rdquo;, with
more coming out on a daily basis&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Using a variety of controlnets&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>Video generation with CogVideoX using controlnet (courtesy of &lt;a href="https://github.com/kijai" target="_blank" rel="noopener">Kijai&lt;/a>):&lt;/p>
&lt;video width="720" autoplay loop>
&lt;source src="kijai.mp4" type="video/mp4" />
&lt;/video>
&lt;p>Finally: &lt;em>Hunyuan&lt;/em>. I very quickly got very convinced by it.
Here is the sixth test video that I ever created with it.
The quality is just unbelievable for a quick test video.
As input, I used Huyen&amp;rsquo;s image from Figure 2 piped through an LLM for captioning and motion instructions.&lt;/p>
&lt;video width="720" autoplay loop>
&lt;source src="huyen.mp4" type="video/mp4" />
&lt;/video>
&lt;h2 id="3-case-study-ai-remake-of-the-good-the-bad-and-the-ugly">3. Case Study: AI Remake of &amp;ldquo;The Good, The Bad, and The Ugly&amp;rdquo;&lt;/h2>
&lt;p>The goal of this experiment was to understand Hunyuan capabilities more
deeply. It took 10 intense days from start to finish. It&amp;rsquo;s important to
note that if I had simply wanted to generate the best possible AI video
when I started, I would have used CogVideoX, because of its strong
controllability. And indeed, controllability was the main challenge in
this experiment, as Hunyuan only accepted 2 types of inputs when we
started:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Text prompts&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Videos for specifying motion vectors&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>Together with my most artistic PhD student (&lt;a href="https://ar-ai.org/author/david-maruscsak/" target="_blank" rel="noopener">Dávid Maruscsák&lt;/a>), we set out
to make a short AI movie with Hunyuan. Our plan of attack was:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Model exploration: understand censorship &amp;amp; prompting capabilities&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Creative exploration: which kind of movie can we make given the
technical constraints?&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Production&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h4 id="model-exploration">Model Exploration&lt;/h4>
&lt;p>A very weird characteristic of Hunyuan is that to get the best outputs,
you can&amp;rsquo;t really use natural language prompts (same issue as for e.g.
Flux or LTX-Video). Instead, you must use an LLM that transforms what
you want into a prompt that suits the model. Tencent very kindly also
released the LLM that they are using for that, but it&amp;rsquo;s VRAM requirement
is 700GB&amp;hellip; To put this in perspective, you would need 44 of the best
consumer graphics cards available right now. It seemed too heavy.
Instead, we used a quantized version of Llama 3.3 that only needs 35GB
VRAM, so it could fit into our H100 card.&lt;/p>
&lt;p>Without LORAs, the standard trick to achieve consistency between shots
is to use celebrities as actors in your AI movie. So, we started by
testing 120 celebrities .&lt;/p>
&lt;p>The pipeline was fully automatic using Llama and Hunyuan. First, Llama
generated a list of the top-20 celebrities for certain domains (sports,
music, politics, fictional characters, etc.); then, it created a prompt
in the expected format for Hunyuan. We generated four 5-second videos
per celebrity, resulting in 480 clips that Dávid Maruscsák watched and
rated. The whole process took 8 hours on a single H100 GPU.&lt;/p>
&lt;p>Armed with the results, we proceeded to explore different storylines for
our movie.&lt;/p>
&lt;h4 id="creative-exploration">Creative Exploration&lt;/h4>
&lt;p>Our first idea was to remake the arthouse movie &lt;a href="https://en.wikipedia.org/wiki/Coffee_and_Cigarettes" target="_blank" rel="noopener">Coffee and
Cigarettes&lt;/a>. Initial results were very encouraging:&lt;/p>
&lt;figure id="figure-input-screencap-of-coffee-and-cigarettes-below-sample-outputs-of-automated-pipeline">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Input screencap of Coffee and Cigarettes. Below: sample outputs of automated pipeline." srcset="
/ai/good-bad-ugly/cc_hu3ebab9955c4ef13d01d84409b44410e3_4088521_1d5f95cc72399937db7f510830518c1d.webp 400w,
/ai/good-bad-ugly/cc_hu3ebab9955c4ef13d01d84409b44410e3_4088521_8e69f39dfdc54929eb96796b2e10d5e7.webp 760w,
/ai/good-bad-ugly/cc_hu3ebab9955c4ef13d01d84409b44410e3_4088521_1200x1200_fit_q75_h2_lanczos_3.webp 1200w"
src="https://drsandor.net/ai/good-bad-ugly/cc_hu3ebab9955c4ef13d01d84409b44410e3_4088521_1d5f95cc72399937db7f510830518c1d.webp"
width="760"
height="402"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption data-pre="Figure&amp;nbsp;" data-post=":&amp;nbsp;" class="numbered">
Input screencap of Coffee and Cigarettes. Below: sample outputs of automated pipeline.
&lt;/figcaption>&lt;/figure>
&lt;video width="720" autoplay loop>
&lt;source src="cc1.mp4" type="video/mp4" />
&lt;/video>
&lt;video width="720" autoplay loop>
&lt;source src="cc2.mp4" type="video/mp4" />
&lt;/video>
&lt;p>However, we quickly realized that with the currently incomplete
ecosystem around Hunyuan, we can only depict 1 celebrity per shot
reliably. In general, the success rate when having 2 goes down dramatically (90% =&amp;gt;
1%).&lt;/p>
&lt;p>So, we had to find a movie genre that mainly shows single actors in
shots, which led us to &amp;ldquo;The Good, The Bad, and The Ugly&amp;rdquo;.&lt;/p>
&lt;p>Based on our test, the celebrity that works by far the best among all is
Donald Trump. So, he had to be one of them. I remembered that Donald
Trump had some beef with Taylor Swift (&amp;ldquo;cat lady&amp;rdquo;), so we considered it
to be a good fit to make Taylor &amp;ldquo;The Good&amp;rdquo;, as she kills &amp;ldquo;The Bad&amp;rdquo; at
the end of the movie. We were only left with determining &amp;ldquo;The Ugly&amp;rdquo;.
While Vladimir Putin works very well in Hunyuan and could be a good fit,
we were not too keen to get to experience Novichok agent. So, we settled
for Elon Musk (If you read this Elon: Please don&amp;rsquo;t disable the breaks
while I am driving my car. It is only a joke! Thank you!).&lt;/p>
&lt;h4 id="production">Production&lt;/h4>
&lt;p>To be honest, it was a painful process. We had about 20 short shots that
we had to generate. Only very few of them worked through a completely
automated pipeline. For the others, it was more like:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Generate 16 clips with automated pipeline (~5 minutes)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Look at clips, pick best one, refine prompt&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Goto 1.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>But, oh boy! When it worked, it worked astonishingly well:&lt;/p>
&lt;video width="720" autoplay loop>
&lt;source src="elon.mp4" type="video/mp4" />
&lt;/video>
&lt;p>A few clips were completely impossible to generate, as there is a
strange dependency between:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Celebrity in the shot&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Activity that the celebrity is doing&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Overall scene&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>I still don&amp;rsquo;t fully understand this point. Changing a single word in the
prompt can get you from &amp;ldquo;completely unusable&amp;rdquo; to &amp;ldquo;perfect&amp;rdquo;.&lt;/p>
&lt;p>It took me about 10 hours of work to generate the 20 shots. Finally,
David used his great video editing skills to put everything together.
Enjoy!&lt;/p>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/FpGGfpizIi4" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;p>&lt;strong>Conclusions&lt;/strong> Hunyuan video demonstrated great potential in this
project. I also had to dive into LLMs (something that I religiously
avoided before that), which was a very interesting experience! I can&amp;rsquo;t
wait for Tencent to release further capabilities for their model (most
importantly: image2video), as well as the open source community creating
more and especially better LORAs (I found the ones that are currently
available to not work very well).&lt;/p>
&lt;p>&lt;strong>Acknowledgements&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;a href="https://ar-ai.org/author/david-maruscsak/" target="_blank" rel="noopener">Dávid Maruscsák&lt;/a>: Creative input, video editing&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Banadoco discord: Technical support and joint explorations of the model&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://github.com/kijai" target="_blank" rel="noopener">Kijai&lt;/a>: advanced technical support&lt;/p>
&lt;/li>
&lt;/ul></description></item><item><title>Video Generation with Open Source Models</title><link>https://drsandor.net/ai/attenborough/</link><pubDate>Wed, 02 Oct 2024 14:00:00 +0000</pubDate><guid>https://drsandor.net/ai/attenborough/</guid><description>&lt;p>Social media has recently been flooded with impressive AI videos made with closed source tools like Sora, Runway, or Kling. In this experiment, I wanted to investigate how far you can get by using only open source AI models.&lt;/p>
&lt;p>What is the relevance of open source AI models? First, as Yann LeCun and others argue, there are implications on the highest level; AI is becoming an absolute key technology of our times; it can&amp;rsquo;t be good for humanity if this power is in the hand of small group of people leading companies in Silicon Valley and Beijing. Second, there is a technical dimension to it. As we saw with Linux vs. Windows: Open Source leads to technical superiority in the long run. Applied to AI, we could observe that it did not take long for open text-to-image models to overtake closed ones (e.g. Stable Diffusion vs. Midjourney).&lt;/p>
&lt;p>In the following, I describe details of my experiment, which took 10 hours from concept to final cut. You can also skip directly to the end to see the final result.&lt;/p>
&lt;h2 id="automatic-pipeline">Automatic Pipeline&lt;/h2>
&lt;p>As first step, I set up an automatic pipeline to create videos from images. As input images, I used some beautiful nature photographs (links to all used resources at the bottom of this page). Then, I adapted an awesome ComfyUI workflow to run in my environment (A100 with 80GB VRAM). Final computation speed was just below 2 minutes from an image to a video clip consisting of 49 frames (720x480 pixels). One important aspect of this workflow is that while it includes some text-to-image models, it does not require me to enter text prompts in order to get images; I much prefer a completely visual pipeline.&lt;/p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="" srcset="
/ai/attenborough/pipeline_hue860506cb1c1a8ced4983eb02da0f130_122916_d23f71f966a831b9bec743bbcfd330c0.webp 400w,
/ai/attenborough/pipeline_hue860506cb1c1a8ced4983eb02da0f130_122916_7ce085067317ddb28b4edcd7649ccecc.webp 760w,
/ai/attenborough/pipeline_hue860506cb1c1a8ced4983eb02da0f130_122916_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://drsandor.net/ai/attenborough/pipeline_hue860506cb1c1a8ced4983eb02da0f130_122916_d23f71f966a831b9bec743bbcfd330c0.webp"
width="760"
height="428"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;p>Based on 25 input images, I generated 96 clips in 3 hours, taking just below 2 minutes per clip.&lt;/p>
&lt;h2 id="manual-postprocessing">Manual Postprocessing&lt;/h2>
&lt;p>Out of the 96 generated clips, about half were sufficiently good. I then picked the 23 I liked best and performed spatio-temporal upscaling in Topaz AI to 1920p resolution at 30 fps. Finally, I quickly arranged the clips in iMovie, together with an audio track from Sir Attenborough.&lt;/p>
&lt;h2 id="final-result--conclusions">Final Result &amp;amp; Conclusions&lt;/h2>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/-IhoM75vOyI" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;p>The final result is clearly not as good as what can be done with closed source tools (e.g. a recent favorite of mine: &lt;a href="https://x.com/tunguz/status/1832493397694660914" target="_blank" rel="noopener">La Baie Area&lt;/a>). However, it confirmed my opinion about the trajectory of open source video generation models (I have been testing them all&amp;hellip;): they will overtake closed sourced ones sooner than expected!&lt;/p>
&lt;p>I also learned more about the limitations of CogVideoX: I did a similar experiment with humans in the input images, but they did not work out well. While camera trajectories and actions in the background worked very well, consistency of humans was lacking.&lt;/p>
&lt;h3 id="credits">Credits&lt;/h3>
&lt;ul>
&lt;li>Audio: David Attenborough - For All Nature (&lt;a href="https://www.youtube.com/watch?v=-a_dMFnib-s" target="_blank" rel="noopener">https://www.youtube.com/watch?v=-a_dMFnib-s&lt;/a>)&lt;/li>
&lt;li>Wildlife photographs: &lt;a href="https://www.smithsonianmag.com/smart-news/see-25-breathtaking-images-from-the-wildlife-photographer-of-the-year-contest-180983516/" target="_blank" rel="noopener">https://www.smithsonianmag.com/smart-news/see-25-breathtaking-images-from-the-wildlife-photographer-of-the-year-contest-180983516/&lt;/a>&lt;/li>
&lt;li>ComfyUI Workflow: &lt;a href="https://github.com/henrique-galimberti/i2v-workflow/blob/main/CogVideoX-I2V-workflow_v2.json" target="_blank" rel="noopener">https://github.com/henrique-galimberti/i2v-workflow/blob/main/CogVideoX-I2V-workflow_v2.json&lt;/a>&lt;/li>
&lt;li>Models:
&lt;ul>
&lt;li>Florence-2-large&lt;/li>
&lt;li>Lexi-Llama-3-8B-Uncensored_Q4_K_M.gguf&lt;/li>
&lt;li>CogVideoX-5b&lt;/li>
&lt;li>Outpainting: RealVisXL V3.0 Inpainting&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul></description></item></channel></rss>