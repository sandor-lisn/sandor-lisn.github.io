<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Wowchemy 5.6.0 for Hugo"><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media=print onload='this.media="all"'><meta name=author content="Christian Sandor"><meta name=description content="This month, a groundbreaking generative AI video model was released. And no, it's neither Sora nor Veo 2... As a proof-of-concept, we made an AI remake of &#34;The Good, the Bad, and the Ugly&#34; with it."><link rel=alternate hreflang=en-us href=https://drsandor.net/ai/good-bad-ugly/><meta name=theme-color content="#3f51b5"><link rel=stylesheet href=/css/vendor-bundle.min.c7b8d9abd591ba2253ea42747e3ac3f5.css media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css integrity="sha512-W0xM4mr6dEP9nREo7Z9z+9X70wytKvMGeDsj7ps2+xg5QPrEBXC8tAW1IFnzjR6eoJ90JmCnFzerQJTLzIEHjA==" crossorigin=anonymous media=print onload='this.media="all"'><link rel=stylesheet href=/css/wowchemy.ee44e21b938c78856d34562e4ef212d5.css><link rel=stylesheet href=/css/libs/chroma/github-light.min.css title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=/css/libs/chroma/dracula.min.css title=hl-dark media=print onload='this.media="all"' disabled><meta name=google-site-verification content="googleb4bc43fe85d01f37"><script async src="https://www.googletagmanager.com/gtag/js?id=G-8PFRJBZTVW"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}function trackOutboundLink(e,t){gtag("event","click",{event_category:"outbound",event_label:e,transport_type:"beacon",event_callback:function(){t!=="_blank"&&(document.location=e)}}),console.debug("Outbound link clicked: "+e)}function onClickCallback(e){if(e.target.tagName!=="A"||e.target.host===window.location.host)return;trackOutboundLink(e.target,e.target.getAttribute("target"))}gtag("js",new Date),gtag("config","G-8PFRJBZTVW",{}),gtag("set",{cookie_flags:"SameSite=None;Secure"}),document.addEventListener("click",onClickCallback,!1)</script><link rel=manifest href=/manifest.webmanifest><link rel=icon type=image/png href=/media/icon_hu287fcf417612116bcf0d00ac1ba165d7_82622_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/media/icon_hu287fcf417612116bcf0d00ac1ba165d7_82622_180x180_fill_lanczos_center_3.png><link rel=canonical href=https://drsandor.net/ai/good-bad-ugly/><meta property="twitter:card" content="summary_large_image"><meta property="og:site_name" content="Christian Sandor"><meta property="og:url" content="https://drsandor.net/ai/good-bad-ugly/"><meta property="og:title" content="AI Video Generation - The Future is already here | Christian Sandor"><meta property="og:description" content="This month, a groundbreaking generative AI video model was released. And no, it's neither Sora nor Veo 2... As a proof-of-concept, we made an AI remake of &#34;The Good, the Bad, and the Ugly&#34; with it."><meta property="og:image" content="https://drsandor.net/ai/good-bad-ugly/featured.jpg"><meta property="twitter:image" content="https://drsandor.net/ai/good-bad-ugly/featured.jpg"><meta property="og:locale" content="en-us"><meta property="article:published_time" content="2024-12-18T14:00:00+00:00"><meta property="article:modified_time" content="2024-12-18T14:00:00+00:00"><title>AI Video Generation - The Future is already here | Christian Sandor</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper data-wc-page-id=9ff72740d9e4801ea9a6229020f0da3f><script src=/js/wowchemy-init.min.f83d8694902118afb325391a30f9ddfa.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=Search...></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class=page-header><header class=header--fixed><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>Christian Sandor</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>Christian Sandor</a></div><div class="navbar-collapse main-menu-item collapse justify-content-start" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/#about><span>Home</span></a></li><li class=nav-item><a class=nav-link href=/#projects><span>News</span></a></li><li class=nav-item><a class=nav-link href=/#bio><span>Biography</span></a></li><li class=nav-item><a class=nav-link href=/chess><span>Chess</span></a></li><li class=nav-item><a class="nav-link active" href=/ai><span>AI Experiments</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class="nav-item dropdown theme-dropdown"><a href=# class=nav-link data-toggle=dropdown aria-haspopup=true aria-label="Display preferences"><i class="fas fa-moon" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>Light</span></a>
<a href=# class="dropdown-item js-set-theme-dark"><span>Dark</span></a>
<a href=# class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li></ul></div></nav></header></div><div class=page-body><article class=article><div class="article-container pt-3"><h1>AI Video Generation - The Future is already here</h1><div class=article-metadata><span class=article-date>Dec 18, 2024</span></div></div><div class="article-header article-container featured-image-wrapper mt-4 mb-4" style=max-width:720px;max-height:986px><div style=position:relative><img src=/ai/good-bad-ugly/featured_hud47c9a2e59f8fc5c05ccf0771fae044a_2262711_720x2500_fit_q75_h2_lanczos.webp width=720 height=986 alt class=featured-image></div></div><div class=article-container><div class=article-style><p>I concluded my last AI blog post in October with &ldquo;open source video
generation models&mldr; will overtake closed sourced ones sooner than
expected!&rdquo;. <em>Surprise: it has already happened!</em></p><p>In this lengthy blogpost, I want to:</p><ul><li><p>Explain the <em>enormous work</em> that we did over the <em>last 10 days</em>, resulting in a funny AI
movie (jump to the end if you are only here for that)</p></li><li><p>Explain the <em>big picture</em> of generative AI for videos</p></li></ul><details class="toc-inpage d-print-none" open><summary class=font-weight-bold>Table of Contents</summary><nav id=TableOfContents><ul><li><a href=#1-the-ideal-video-model>1. The Ideal Video Model</a><ul><li></li></ul></li><li><a href=#2-current-video-models>2. Current Video Models</a></li><li><a href=#3-case-study-ai-remake-of-the-good-the-bad-and-the-ugly>3. Case Study: AI Remake of &ldquo;The Good, The Bad, and The Ugly&rdquo;</a><ul><li></li></ul></li></ul></nav></details><p>Since my last blog post, a ton of generative AI video models have been
released and I have tested the most important ones. To be upfront with
my conclusion: <a href=https://github.com/Tencent/HunyuanVideo target=_blank rel=noopener>Tencent&rsquo;s Hunyuan Video</a>
is <em>absolutely astonishing</em> and
will have wide impact. The last time I had this feeling was when Stable
Diffusion came out in 2022, which was the foundation for successful
commercial products such as Midjourney and is still being used widely
(e.g. the new Mountain Dew commercials are made with it). I am convinced:
<em>Hunyuan will be for video generation what Stable Diffusion has been
for image generation.</em></p><h2 id=1-the-ideal-video-model>1. The Ideal Video Model</h2><p>Obviously, we want video models to be expressive and generate videos of
high resolution and framerate. Some not-so-obvious requirements include:</p><ul><li><p><em>Open source:</em> model can be downloaded and run locally</p></li><li><p><em>Uncensored:</em> censored models lead to many problems</p></li><li><p>Allow the development of an <em>ecosystem of tools</em> around it</p></li></ul><p>Following the analogy to Stable Diffusion I drew earlier: all of these
were true for it and were key to why it won.</p><h4 id=model-censorship>Model Censorship</h4><p>In general, this is a difficult question. On the one hand, I concur that
some extreme content should simply not exist at all, so models should
not be able to generate it. On the other hand, over-censoring of models
has some unintended side effects.</p><p>Earlier this year, Google&rsquo;s image generation model Gemini was <a href=https://www.nytimes.com/2024/02/22/technology/google-gemini-german-uniforms.html target=_blank rel=noopener>widely ridiculed</a> when a
prompt for &ldquo;Nazi soldiers&rdquo; would generate Black and Asian soldiers. When
OpenAI released DALLE in 2022, I tested it with the prompt &ldquo;My Bavarian
mother unintentionally takes a selfie with her mobile phone&rdquo;, also
returning me a selection of ethnicities in the results, which are unlikely
to match what I actually want. What a bad user experience!</p><p>This year, Stability AI released Stable Diffusion 3, which the community
had been waiting for in great anticipation. It quickly turned out that this
model is quite useless. The training data only contained a very small
amount of lightly-dressed humans, leading to a total incomprehension of
human anatomy. The prompt &ldquo;woman lying on grass&rdquo; quickly became a meme.</p><figure id=figure-woman-lying-on-grass-according-to-stable-diffusion-3httpsstabilityainewsstable-diffusion-3><div class="d-flex justify-content-center"><div class=w-100><img alt="''Woman lying on grass', according to [Stable Diffusion 3](https://stability.ai/news/stable-diffusion-3)" srcset="/ai/good-bad-ugly/1_hu9c3d30bf53313f395dc9309d4b7f68f9_2061541_4736902eab9f0ecd57bf02bf8b4ee839.webp 400w,
/ai/good-bad-ugly/1_hu9c3d30bf53313f395dc9309d4b7f68f9_2061541_5acf54ae7b89121f468fb098ea842425.webp 760w,
/ai/good-bad-ugly/1_hu9c3d30bf53313f395dc9309d4b7f68f9_2061541_1200x1200_fit_q75_h2_lanczos_3.webp 1200w" src=/ai/good-bad-ugly/1_hu9c3d30bf53313f395dc9309d4b7f68f9_2061541_4736902eab9f0ecd57bf02bf8b4ee839.webp width=760 height=430 loading=lazy data-zoomable></div></div><figcaption data-pre=Figure&nbsp; data-post=:&nbsp; class=numbered>&lsquo;&lsquo;Woman lying on grass&rsquo;, according to <a href=https://stability.ai/news/stable-diffusion-3 target=_blank rel=noopener>Stable Diffusion 3</a></figcaption></figure><h4 id=ecosystem>Ecosystem</h4><p>In my last blog post, I showed how I built a pipeline out of several
open source models. This ability is essential for many use cases,
including professional media production and research. It also
supports the creation of an ecosystem of supporting tools (more on this below).</p><p>The models developed by big Silicon Valley companies like Meta, Google,
and OpenAI don&rsquo;t support such use cases and probably don&rsquo;t even want to;
they are happy if users can modify a video they took in a café to show a
Kombucha instead of a Coke or an Avocado Toast instead of a Hamburger.</p><p>An ecosystem of tools around foundation models is critical, because it
allows, among other things, the very precise <em>specification of outputs</em>.
Simply speaking, one of the most useless ways to specify the image or
video that I want to be generated is by text. Much more useful are:</p><ul><li><p><em>Controlnets:</em> can specify output by visual constraints (colors, body
poses, depth information, edge information etc.)</p></li><li><p><em>IPAdapters:</em> can be used to transfer non-tangible information, such
as the style of an image</p></li><li><p><em>LORAs:</em> a more heavyweight approach than IPAdapters that allows much
more control of styles, materials, and humans in the output</p></li></ul><figure id=figure-left-behind-the-scenes-of-a-video-i-posted-on-linkedin-in-022024httpswwwlinkedincompostsdr-christian-sandor-b7240890_i-did-some-dance-training-over-the-holidays-activity-7156976991240355840-28_outm_sourceshareutm_mediummember_desktop-using-a-lora-i-had-trained-on-my-appearance-and-a-controlnet-for-body-pose-right-my-coworker-huyenhttpsar-aiorgauthorhuyen-nguyen-generated--using-an-ipadapter-conditioned-with-4-photos-of-her><div class="d-flex justify-content-center"><div class=w-100><img alt="Left: behind the scenes of a video I [posted on Linkedin in 02/2024](https://www.linkedin.com/posts/dr-christian-sandor-b7240890_i-did-some-dance-training-over-the-holidays-activity-7156976991240355840-28_O?utm_source=share&utm_medium=member_desktop), using a LORA I had trained on my appearance and a controlnet for body pose. Right: my coworker [Huyen](https://ar-ai.org/author/huyen-nguyen/), generated  using an IPAdapter conditioned with 4 photos of her." srcset="/ai/good-bad-ugly/2_hufba5692dd20cec5e2bc1458418e9c891_1409682_c5752d928a33c7648640a778276f4fef.webp 400w,
/ai/good-bad-ugly/2_hufba5692dd20cec5e2bc1458418e9c891_1409682_0ee009d1ae63cc7e8937dc82134f59fa.webp 760w,
/ai/good-bad-ugly/2_hufba5692dd20cec5e2bc1458418e9c891_1409682_1200x1200_fit_q75_h2_lanczos_3.webp 1200w" src=/ai/good-bad-ugly/2_hufba5692dd20cec5e2bc1458418e9c891_1409682_c5752d928a33c7648640a778276f4fef.webp width=760 height=450 loading=lazy data-zoomable></div></div><figcaption data-pre=Figure&nbsp; data-post=:&nbsp; class=numbered>Left: behind the scenes of a video I <a href="https://www.linkedin.com/posts/dr-christian-sandor-b7240890_i-did-some-dance-training-over-the-holidays-activity-7156976991240355840-28_O?utm_source=share&utm_medium=member_desktop" target=_blank rel=noopener>posted on Linkedin in 02/2024</a>, using a LORA I had trained on my appearance and a controlnet for body pose. Right: my coworker <a href=https://ar-ai.org/author/huyen-nguyen/ target=_blank rel=noopener>Huyen</a>, generated using an IPAdapter conditioned with 4 photos of her.</figcaption></figure><h2 id=2-current-video-models>2. Current Video Models</h2><p>Since my last AI blog post, I consider the most important models that
have been released: LTX-Video (1 month ago), Hunyuan (3 weeks ago), and
many additions to CogVideoX. Their main strengths are:</p><ul><li><p><em>LTX:</em> Lightning fast</p></li><li><p><em>CogVideoX:</em> Extreme controllability</p></li><li><p><em>Hunyuan:</em> Best visual quality</p></li></ul><p>First, let me show an interactive session of me with <em>LTX video</em>, captured
in real-time from my screen. The speed is absolutely mind-blowing!</p><video width=720 autoplay loop>
<source src=ltx.mp4 type=video/mp4></video><p>The <em>CogVideoX</em> family stands out by how many things can be controlled
about the videos that are generated, including:</p><ul><li><p>Using images to specify the start & end of the video</p></li><li><p>Define trajectories of objects in the video</p></li><li><p>Exact specification of camera movements</p></li><li><p>Using LORAs, we can already specify styles like &ldquo;Disney Movie&rdquo;, with
more coming out on a daily basis</p></li><li><p>Using a variety of controlnets</p></li></ul><p>Video generation with CogVideoX using controlnet (courtesy of <a href=https://github.com/kijai target=_blank rel=noopener>Kijai</a>):</p><video width=720 autoplay loop>
<source src=kijai.mp4 type=video/mp4></video><p>Finally: <em>Hunyuan</em>. I very quickly got very convinced by it.
Here is the sixth test video that I ever created with it.
The quality is just unbelievable for a quick test video.
As input, I used Huyen&rsquo;s image from Figure 2 piped through an LLM for captioning and motion instructions.</p><video width=720 autoplay loop>
<source src=huyen.mp4 type=video/mp4></video><h2 id=3-case-study-ai-remake-of-the-good-the-bad-and-the-ugly>3. Case Study: AI Remake of &ldquo;The Good, The Bad, and The Ugly&rdquo;</h2><p>The goal of this experiment was to understand Hunyuan capabilities more
deeply. It took 10 intense days from start to finish. It&rsquo;s important to
note that if I had simply wanted to generate the best possible AI video
when I started, I would have used CogVideoX, because of its strong
controllability. And indeed, controllability was the main challenge in
this experiment, as Hunyuan only accepted 2 types of inputs when we
started:</p><ul><li><p>Text prompts</p></li><li><p>Videos for specifying motion vectors</p></li></ul><p>Together with my most artistic PhD student (<a href=https://ar-ai.org/author/david-maruscsak/ target=_blank rel=noopener>Dávid Maruscsák</a>), we set out
to make a short AI movie with Hunyuan. Our plan of attack was:</p><ul><li><p>Model exploration: understand censorship & prompting capabilities</p></li><li><p>Creative exploration: which kind of movie can we make given the
technical constraints?</p></li><li><p>Production</p></li></ul><h4 id=model-exploration>Model Exploration</h4><p>A very weird characteristic of Hunyuan is that to get the best outputs,
you can&rsquo;t really use natural language prompts (same issue as for e.g.
Flux or LTX-Video). Instead, you must use an LLM that transforms what
you want into a prompt that suits the model. Tencent very kindly also
released the LLM that they are using for that, but it&rsquo;s VRAM requirement
is 700GB&mldr; To put this in perspective, you would need 44 of the best
consumer graphics cards available right now. It seemed too heavy.
Instead, we used a quantized version of Llama 3.3 that only needs 35GB
VRAM, so it could fit into our H100 card.</p><p>Without LORAs, the standard trick to achieve consistency between shots
is to use celebrities as actors in your AI movie. So, we started by
testing 120 celebrities .</p><p>The pipeline was fully automatic using Llama and Hunyuan. First, Llama
generated a list of the top-20 celebrities for certain domains (sports,
music, politics, fictional characters, etc.); then, it created a prompt
in the expected format for Hunyuan. We generated four 5-second videos
per celebrity, resulting in 480 clips that Dávid Maruscsák watched and
rated. The whole process took 8 hours on a single H100 GPU.</p><p>Armed with the results, we proceeded to explore different storylines for
our movie.</p><h4 id=creative-exploration>Creative Exploration</h4><p>Our first idea was to remake the arthouse movie <a href=https://en.wikipedia.org/wiki/Coffee_and_Cigarettes target=_blank rel=noopener>Coffee and
Cigarettes</a>. Initial results were very encouraging:</p><figure id=figure-input-screencap-of-coffee-and-cigarettes-below-sample-outputs-of-automated-pipeline><div class="d-flex justify-content-center"><div class=w-100><img alt="Input screencap of Coffee and Cigarettes. Below: sample outputs of automated pipeline." srcset="/ai/good-bad-ugly/cc_hu3ebab9955c4ef13d01d84409b44410e3_4088521_1d5f95cc72399937db7f510830518c1d.webp 400w,
/ai/good-bad-ugly/cc_hu3ebab9955c4ef13d01d84409b44410e3_4088521_8e69f39dfdc54929eb96796b2e10d5e7.webp 760w,
/ai/good-bad-ugly/cc_hu3ebab9955c4ef13d01d84409b44410e3_4088521_1200x1200_fit_q75_h2_lanczos_3.webp 1200w" src=/ai/good-bad-ugly/cc_hu3ebab9955c4ef13d01d84409b44410e3_4088521_1d5f95cc72399937db7f510830518c1d.webp width=760 height=402 loading=lazy data-zoomable></div></div><figcaption data-pre=Figure&nbsp; data-post=:&nbsp; class=numbered>Input screencap of Coffee and Cigarettes. Below: sample outputs of automated pipeline.</figcaption></figure><video width=720 autoplay loop>
<source src=cc1.mp4 type=video/mp4></video>
<video width=720 autoplay loop>
<source src=cc2.mp4 type=video/mp4></video><p>However, we quickly realized that with the currently incomplete
ecosystem around Hunyuan, we can only depict 1 celebrity per shot
reliably. In general, the success rate when having 2 goes down dramatically (90% =>
1%).</p><p>So, we had to find a movie genre that mainly shows single actors in
shots, which led us to &ldquo;The Good, The Bad, and The Ugly&rdquo;.</p><p>Based on our test, the celebrity that works by far the best among all is
Donald Trump. So, he had to be one of them. I remembered that Donald
Trump had some beef with Taylor Swift (&ldquo;cat lady&rdquo;), so we considered it
to be a good fit to make Taylor &ldquo;The Good&rdquo;, as she kills &ldquo;The Bad&rdquo; at
the end of the movie. We were only left with determining &ldquo;The Ugly&rdquo;.
While Vladimir Putin works very well in Hunyuan and could be a good fit,
we were not too keen to get to experience Novichok agent. So, we settled
for Elon Musk (If you read this Elon: Please don&rsquo;t disable the breaks
while I am driving my car. It is only a joke! Thank you!).</p><h4 id=production>Production</h4><p>To be honest, it was a painful process. We had about 20 short shots that
we had to generate. Only very few of them worked through a completely
automated pipeline. For the others, it was more like:</p><ol><li><p>Generate 16 clips with automated pipeline (~5 minutes)</p></li><li><p>Look at clips, pick best one, refine prompt</p></li><li><p>Goto 1.</p></li></ol><p>But, oh boy! When it worked, it worked astonishingly well:</p><video width=720 autoplay loop>
<source src=elon.mp4 type=video/mp4></video><p>A few clips were completely impossible to generate, as there is a
strange dependency between:</p><ul><li><p>Celebrity in the shot</p></li><li><p>Activity that the celebrity is doing</p></li><li><p>Overall scene</p></li></ul><p>I still don&rsquo;t fully understand this point. Changing a single word in the
prompt can get you from &ldquo;completely unusable&rdquo; to &ldquo;perfect&rdquo;.</p><p>It took me about 10 hours of work to generate the 20 shots. Finally,
David used his great video editing skills to put everything together.
Enjoy!</p><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube.com/embed/FpGGfpizIi4 style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div><p><strong>Conclusions</strong> Hunyuan video demonstrated great potential in this
project. I also had to dive into LLMs (something that I religiously
avoided before that), which was a very interesting experience! I can&rsquo;t
wait for Tencent to release further capabilities for their model (most
importantly: image2video), as well as the open source community creating
more and especially better LORAs (I found the ones that are currently
available to not work very well).</p><p><strong>Acknowledgements</strong></p><ul><li><p><a href=https://ar-ai.org/author/david-maruscsak/ target=_blank rel=noopener>Dávid Maruscsák</a>: Creative input, video editing</p></li><li><p>Banadoco discord: Technical support and joint explorations of the model</p></li><li><p><a href=https://github.com/kijai target=_blank rel=noopener>Kijai</a>: advanced technical support</p></li></ul></div><div class=share-box><ul class=share><li><a href="https://twitter.com/intent/tweet?url=https://drsandor.net/ai/good-bad-ugly/&text=AI%20Video%20Generation%20-%20The%20Future%20is%20already%20here" target=_blank rel=noopener class=share-btn-twitter aria-label=twitter><i class="fab fa-twitter"></i></a></li><li><a href="https://www.facebook.com/sharer.php?u=https://drsandor.net/ai/good-bad-ugly/&t=AI%20Video%20Generation%20-%20The%20Future%20is%20already%20here" target=_blank rel=noopener class=share-btn-facebook aria-label=facebook><i class="fab fa-facebook"></i></a></li><li><a href="mailto:?subject=AI%20Video%20Generation%20-%20The%20Future%20is%20already%20here&body=https://drsandor.net/ai/good-bad-ugly/" target=_blank rel=noopener class=share-btn-email aria-label=envelope><i class="fas fa-envelope"></i></a></li><li><a href="https://www.linkedin.com/shareArticle?url=https://drsandor.net/ai/good-bad-ugly/&title=AI%20Video%20Generation%20-%20The%20Future%20is%20already%20here" target=_blank rel=noopener class=share-btn-linkedin aria-label=linkedin-in><i class="fab fa-linkedin-in"></i></a></li><li><a href="whatsapp://send?text=AI%20Video%20Generation%20-%20The%20Future%20is%20already%20here%20https://drsandor.net/ai/good-bad-ugly/" target=_blank rel=noopener class=share-btn-whatsapp aria-label=whatsapp><i class="fab fa-whatsapp"></i></a></li><li><a href="https://service.weibo.com/share/share.php?url=https://drsandor.net/ai/good-bad-ugly/&title=AI%20Video%20Generation%20-%20The%20Future%20is%20already%20here" target=_blank rel=noopener class=share-btn-weibo aria-label=weibo><i class="fab fa-weibo"></i></a></li></ul></div><div class="media author-card content-widget-hr"><a href=https://drsandor.net><img class="avatar mr-3 avatar-circle" src=/authors/christian-sandor/aavatar_hu188cb96a342c5622107b7738ee80b4ce_62574_270x270_fill_q75_lanczos_center.jpg alt="Christian Sandor"></a><div class=media-body><h5 class=card-title><a href=https://drsandor.net>Christian Sandor</a></h5><h6 class=card-subtitle>Professor</h6><p class=card-text>Interested in Augmented Reality, Artificial Intelligence, and Human Perception</p><ul class=network-icon aria-hidden=true><li><a href=mailto:christian@sandor.com><i class="fas fa-envelope"></i></a></li><li><a href=https://www.linkedin.com/in/dr-christian-sandor-b7240890/ target=_blank rel=noopener><i class="fab fa-linkedin"></i></a></li><li><a href="https://scholar.google.com/citations?user=88vBLhsAAAAJ&hl=en" target=_blank rel=noopener><i class="ai ai-google-scholar"></i></a></li><li><a href=/uploads/resume.pdf><i class="ai ai-cv"></i></a></li><li><a href="https://www.mathgenealogy.org/id.php?id=135070" target=_blank rel=noopener><i class="fas fa-tree"></i></a></li><li><a href=https://x.com/xanda55555 target=_blank rel=noopener><i class="fab fa-twitter"></i></a></li></ul></div></div></div></article></div><div class=page-footer><div class=container><footer class=site-footer><p class="powered-by copyright-license-text">© 2025 Christian Sandor</p><p class=powered-by></p></footer></div></div><script src=/js/vendor-bundle.min.46271ef31da3f018e9cd1b59300aa265.js></script>
<script id=search-hit-fuse-template type=text/x-template>
    <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
    </div>
  </script><script src=https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin=anonymous></script>
<script id=page-data type=application/json>{"use_headroom":true}</script><script src=/js/wowchemy-headroom.c251366b4128fd5e6b046d4c97a62a51.js type=module></script>
<script src=/en/js/wowchemy.min.a6238d5886fa4a2f7cf92df25709326f.js></script><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> Copy</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> Download</a><div id=modal-error></div></div></div></div></div><script src=/js/wowchemy-publication.68f8d7090562ca65fc6d3cb3f8f2d2cb.js type=module></script></body></html>